[
{"url": "https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq", "title": ["Welcome & FAQ!"], "author": ["Ruben Bloom"], "date": ["2021-08-24T20:14:21.161Z"], "content": "The AI Alignment Forum was launched in 2018. Since then, several hundred researchers have contributed approximately two thousand posts and nine thousand comments. Nearing the third birthday of the Forum, we are publishing this updated and clarified FAQ. Minimalist, watercolor sketch of humanity spreading across the stars by VQGAN I have a practical question concerning a site feature. Almost all of the Alignment Forum site features are shared with LessWrong.com; have a look at the  LessWrong FAQ  for questions concerning the  Editor ,  Voting,   Questions ,  Notifications & Subscriptions ,   Moderation , and more. If you can’t easily find the answer there, ping us on Intercom (bottom right of screen) or email us at team@lesswrong.com What is the AI Alignment Forum? The Alignment Forum is a single online hub for researchers to discuss all ideas related to ensuring that transformatively powerful AIs are aligned with human values. Discussion ranges from technical models of agency to the strategic landscape, and everything in between. Top voted posts include  What failure looks like ,   Are we in an AI overhang? , and  Embedded Agents . A list of the top posts of all time can be  viewed here . While direct participation in the Forum is limited to deeply established researchers in the field, we have designed it also as a place where up-and-coming researchers can get up to speed on the research paradigms and have pathways to participation too. See  How can non-members participate in the Forum?   below. We hope that by being the foremost discussion platform and publication destination for AI Alignment discussion, the Forum will serve as the archive and library of the field. To find posts by sub-topic, view the  AI section of the Concepts page . Why was the Alignment Forum created? Foremost, because misaligned powerful AIs may pose the greatest risk to our civilization that has ever arisen. The problem is of unknown (or at least unagreed upon) difficulty, and allowing the researchers in the field to better communicate and share their thoughts seems like one of the best things we could do to help the pre-paradigmatic field. In the past, journals or conferences might have been the best methods for increasing discussion and collaboration, but in the current age we believe that a well-designed online forum with things like immediate publication, distributed rating of quality (i.e. “peer review”), portability/shareability (e.g. via links), etc., provides the most promising way for the field to develop good standards and methodologies. A further major benefit of having alignment content and discussion in one easily accessible place is that it helps new researchers get onboarded to the field. Hopefully, this will help them begin contributing sooner. Who is the AI Alignment Forum for? There exists an interconnected community of Alignment researchers in industry, academia, and elsewhere who have spent many years thinking carefully about a variety of approaches to alignment. Such research receives institutional support from organizations including FHI, CHAI, DeepMind, OpenAI, MIRI, Open Philanthropy, ARC, and others. The Alignment Forum membership currently consists of researchers at these organizations and their respective collaborators. The Forum is also intended to be a way to interact with and contribute to the cutting edge research for people not connected to these institutions either professionally or socially. There have been many such individuals on LessWrong, and that is the current best place for such people to start contributing, to be given feedback and to skill-up in this domain. There are about 50-100 members of the Forum who are (1) able to post and comment directly to the Forum without review, (2) able to promote the content of others to the Forum. This group will not grow quickly; however, as of August 2021, we have made it easier for non-members to  submit content to the Forum . What type of content is appropriate? As a rule-of-thumb, if a thought is something you’d bring up when talking to someone at a research workshop or to a colleague in your lab, it’s also a welcome contribution here. If you’d like a sense of what other Forum members are interested in, here’s some data from a survey conducted during the open beta of the Forum (n = 34). We polled these early users on what high-level categories of content they were interested in. The responses were on a 1-5 scale, which represented “If I see 1 post per day, I want to see this type of content…” (1) Once per year, (2) Once per 3-4 months (3) Once per 1-2 months (4) Once per 1-2 weeks (5) A third of all posts that I see. New theory-oriented alignment research typical of MIRI or CHAI:  4.4 / 5 New ML-oriented alignment research typical of OpenAI or DeepMind's safety teams:  4.2 / 5 New formal or nearly-formal discussion of intellectually interesting topics that look questionably/ambiguously/peripherally alignment-related:  3.5 / 5 High-quality informal discussion of alignment research methodology and background assumptions, what's needed for progress on different agendas, why people are pursuing this or that agenda, etc:  4.1 / 5 Attempts to more clearly package/explain/summarise previously discussed alignment research:  3.7 / 5 New technical ideas that are clearly not alignment-related but are likely to be intellectually interesting to forum regulars:  2.2 / 5 High-quality informal discussion of very core background questions about advanced AI systems:  3.3 / 5 Typical AGI forecasting research/discussion that isn't obviously unusually relevant to AGI alignment work:  2.2 / 5 What is the relationship between the Alignment Forum and LessWrong? The Alignment Forum was created by and is maintained by the team behind LessWrong (the web forum). The two sites share a codebase and database. They integrate in the following ways: Automatic Crossposting  - Any new post or comment on the new AI Alignment Forum is automatically cross-posted to LessWrong.com. Accounts are also shared between the two platforms (though non-AF member accounts will not be able to post without review). Content Promotion  - Any comment or post on LessWrong can be promoted by members of the AI Alignment Forum to the AI Alignment Forum. Separate Reputation  – The reputation systems (karma) for LessWrong and the AI Alignment Forum are separate. On LessWrong you can see two reputation scores: a primary karma score combining karma from both sites, and a secondary karma score specific to AI Alignment Forum members. On the AI Alignment Forum, you will just see the AI Alignment Forum karma of posts and comments. Content Ownership  - If a comment or post of yours is promoted to the AI Alignment Forum, you will continue to have full ownership of the content, and you’ll be able to respond directly to all comments on your content. Both LessWrong and the Alignment Forum are foci of Alignment Discussion; however, the Alignment Forum maintains even higher standards of content quality than LessWrong. The goal is to provide a place where researchers with shared technical and conceptual background can collaborate, and where a strong set of norms for facilitating good research collaborations can take hold. For this reason, both submissions and members to the Alignment Forum are heavily vetted. How do I get started in AI Alignment research? If you're new to the AI Alignment research field, we recommend four great introductory sequences that cover several different paradigms of thought within the field. Get started reading them and feel free to leave comments with any questions you have. The introductory sequences are: Embedded Agency  by Scott Garrabrant and Abram Demski of MIRI Iterated Amplification  by Paul Christiano of ARC Value Learning  by Rohin Shah of DeepMind AGI Safety from First Principles  by Richard Ngo, formerly of DeepMind Following that, you might want to begin writing up some of your thoughts and sharing them on LessWrong to get feedback. How do I join the Alignment Forum? As described above, membership to the Alignment Forum is very selective (and not strictly required to participate in discussions on Alignment Forum content, since one can do so on LessWrong). The best pathway towards becoming a member is to produce lots of great AI Alignment content, and to post it to LessWrong and participate in discussions there. The LessWrong/Alignment Forum admins monitor activity on both sites, and if someone consistently contributes to Alignment discussions on LessWrong that get promoted to the Alignment Forum, then it’s quite possible full membership will be offered. I work professionally on AI Alignment. Shouldn’t I be a member? Maybe but not definitely! The bar for membership is higher than working on AI Alignment professionally, even if you are doing really great work. Membership, which allows you to directly post and comment, is likely to be offered only after multiple existing Alignment Forum members are excited to see your work. Until then, a review step is required. You can still submit content to the Alignment Forum but it might take a few days for a decision to be made. Another reason for the high bar for membership is that any member has the ability to promote content to the Alignment Forum, kind of like a curator. This requires significant trust and membership is restricted to those who have earned this level of trust among the Alignment Forum members. How can non-members participate in the Forum? Non-members can participate in the Forum in two ways: 1. Posting and commenting Alignment content to LessWrong Alignment content posted to LessWrong will be seen by many of the researchers present on the Alignment Forum. If they (or the Forum admins) think that particular content is a good fit for the Forum, it will be promoted to the Forum and become viewable there. If your posts or comments are promoted to the Alignment Forum, you will be able to directly participate in the discussion of your content on the Forum. 2. Submitting content on the Alignment Forum Non-members can now submit content directly on the Alignment Forum (and not just via LessWrong).  If you post or comment, your submission will enter a review queue and a decision to accept or reject it from the Alignment Forum will be made within three days. If it is rejected, you will receive a minimum one-sentence explanation. In the meantime (and regardless of outcome), your post or comment will be published to  LessWrong . There it can be immediately viewed and discussed by everyone, and edited by you. This allows you to get quick feedback, and allows site admins to use the reaction there to help make the decision about whether it is a good fit for the Alignment Forum. For example, if several Alignment Forum members are discussing your content on LessWrong, it is likely a good fit for the Forum and will be promoted. How can I submit something I already wrote? If you have already written and published a post on LessWrong but would like to submit it for acceptance to the Alignment Forum, please contact us via Intercom (bottom right) or email us at team@lesswrong.com Who runs the Alignment Forum? The Alignment Forum is maintained and run by the  LessWrong team  who also run the LessWrong website. An independent board composed of representatives of major Alignment research orgs (and independent members too) oversees major decisions concerning the Forum. Can I use LaTex? Yes! You can use LaTeX in posts and comments with Cmd+4 / Ctrl+4. Also, if you go into your user settings and switch to the markdown editor, you can just copy-paste LaTeX into a post/comment and it will render when you submit with no further steps required. I have a different question. Please don’t hesitate to contact us via Intercom (bottom right of the screen) or email us at team@lesswrong.com. We’d love to answer your questions."},
{"url": "https://www.alignmentforum.org/posts/83TbrDxvQwkLuiuxk/conflating-value-alignment-and-intent-alignment-is-causing-1", "title": ["Conflating value alignment and intent alignment is causing confusion"], "author": ["Seth Herd"], "date": ["2024-09-05T16:39:51.967Z"], "content": "Epistemic status: I think something like this confusion is happening often. I'm not saying these are the only differences in what people mean by \"AGI alignment\". Summary:   Value alignment is better but probably harder to achieve than  personal intent alignment  to the short-term wants of some person(s) .  Different groups and people tend to primarily address one of these alignment targets when they discuss alignment. Confusion abounds.  One important confusion stems from an assumption that the type of AI defines the alignment target: strong goal-directed AGI must be value aligned or misaligned, while personal intent alignment is only viable for relatively weak AI. I think this assumption is important but false.  While value alignment is categorically better, intent alignment seems easier, safer, and more appealing in the short term, so AGI project leaders are likely to try it. [1] Overview Clarifying what people mean by alignment should dispel some illusory disagreement, and clarify alignment theory and predictions of AGI outcomes.  Caption: Venn diagram of three types of alignment targets. Value alignment and  Personal intent alignment  are both subsets of  Evan Hubinger's definition  of  intent alignment : AGI aligned with human intent in the broadest sense. Prosaic alignment work usually seems to be addressing a target somewhere in the neighborhood of personal intent alignment (following instructions or doing what this person wants now), while agent foundations and other conceptual alignment work usually seems to be addressing value alignment. Those two clusters have different strengths and weaknesses as alignment targets, so lumping them together produces confusion. People mean different things when they say alignment. Some are mostly thinking about value alignment (VA): creating sovereign AGI that has values close enough to humans' for our liking. Others are talking about making AGI that is corrigible (in the  Christiano  or  Harms  sense) [2]  or follows instructions from its designated  principal  human(s). I'm going to use the term  personal intent alignment  (PIA) until someone has a better term for that type of alignment target. Different arguments and intuitions apply to these two alignment goals, so talking about them without differentiation is creating illusory disagreements. Value alignment is better almost by definition, but personal intent alignment seems to avoid some of the biggest difficulties of value alignment. Max Harms' recent sequence on  corrigibility as a singular target  (CAST) gives both a nice summary and detailed arguments. We do not need us to point to or define values, just short term preferences or instructions. The principal advantage is that an AGI that follows instructions can be used as a collaborator in improving its alignment over time; you don't need to get it exactly right on the first try. This is more helpful in slower and more continuous takeoffs. This means that  PI alignment has a larger basin of attraction   than value alignment does. [3] Most people who think alignment is fairly achievable seem to be thinking of PIA, while critics often respond thinking of value alignment. It would help to be explicit.  PIA is probably easier and more likely than full VA for our first stabs at AGI , but there are reasons to wonder if it's adequate for real success. In particular, there are intuitions and arguments that PIA doesn't address the real problem of AGI alignment.  I think PIA does address the real problem, but in a non-obvious and counterintuitive way.  Another unstated divide There's another important clustering around these two conceptions of alignment. People who think about prosaic (and near term) AI alignment tend to be thinking about PIA, while those who think about aligning ASI for the long term are usually thinking of value alignment. The first group tends to have much lower estimates of alignment difficulty and p(doom) than the other. This causes dramatic disagreements on strategy and policy, which is a major problem: if the experts disagree, policy-makers are likely to just pick an expert that supports their own biases. Thinking about one vs the other appears to be one major  crux of disagreement on alignment difficulty. And All the Shoggoths Merely Players  (edit: and  its top comment  thread continuation) is a detailed summary of (and a highly entertaining commentary on) the field's current state of disagreement. In that dialogue, Simplicia Optimistovna asks whether the relative ease of getting LLMs to understand and do what we say is good news about alignment difficulty, while Doomimir Doomovitch sourly argues that this isn't alignment at all; it's just a system that superficially has behavior that you want (within the training set), without having actual goals to align. Actual AGI, he says, will have actual goals, whether we try (and likely fail) to engineer them in properly, or whether  optimization creates a goal-directed search process with weird emergent goals . I agree with Doomimir on this. Directing LLMs behavior isn't alignment in the important sense. We will surely make truly goal-directed agents, probably sooner than later. And when we do, all that matters is whether their goals align closely enough with ours. Prosaic alignment for LLMs is not fully addressing the alignment problem for autonomous, competent AGI or ASI, even if they're based on LLMs. [4] However, I also agree with Simplicia: it's good news that we've created AI that even sort of understands what we mean and does what we ask.  That's because I think approximate understanding is good enough for personal intent alignment, and that personal intent alignment is workable for ASI. I think there's a common and reasonable intuitions that it's not, which create more illusory disagreements between those who mean PIA vs VA when they say \"alignment\". Personal intent alignment for full ASI: can I have your goals? There's an intuition that intent alignment isn't workable for a full AGI; something that's competent or self-aware usually [5]  has its own goals, so it doesn't just follow instructions.  But that intuition is based on our experience with existing minds. What if that synthetic being's explicit, considered goal  is  to approximately follow instructions?  I think it's possible for a fully self-aware, goal-oriented AGI to have its goal be, loosely speaking, a pointer to someone else's goals. No human is oriented this way, but it seems conceptually coherent to want to do, with all of your heart, just what someone else wants. It's good news that LLMs have an approximate understanding of our instructions because that can, in theory, be plugged into the \"goal slot\" in a truly goal-directed agentic architecture. I have  summarized proposals  for how to do this for  several possible  AGI architectures ( focusing on language model agents  as IMO the most likely), but the details don't matter here, just that it's empirically possible to make an AI system that approximately understand what we want. Conclusions Approximate understanding and goal direction looks (to me) to be good enough for personal intent alignment, but not for value alignment. [1]  And PIA does seem adequate for real AGI. Therefore, intent aligned AGI looks to be far easier and safer in the short term (parahuman AGI or pre-ASI) than trying for full value alignment and autonomy. And it can probably be leveraged into full value alignment (if we get an ASI acting as a full collaborator in value-aligning itself or a predecessor). However, this alignment solution has a huge downside. It leaves fallible, selfish humans in charge of AGI systems. These will have immense destructive as well as creative potential. Having humans in charge of them allows for both conflict and ill use, a whole different set of ways we could get doom even if we solve technical alignment. The multipolar scenario with PI aligned, recursive self-improvement capable AGIs looks highly dangerous, but not like certain doom; see  If we solve alignment, do we die anyway? There's another reason we might want to think more, and more explicitly, about intent alignment: it's what we're likely to try, even if it's not the best idea. It's hard to see how we could get a technical solution for value alignment that couldn't also be used for intent alignment. And it seems likely that the types of humans actually in charge of AGI projects would rather implement personal intent alignment; everyone by definition prefers their values to the aggregate of humanity's. If PIA seems even a little safer or better for them, it will serve as a justification for aligning their first AGIs as they'd prefer anyway: to follow their orders.   Where am I wrong? Where should this logic be extended or deepened? What issues would you like to see addressed in further treatments of this thesis?   ^ Very approximate personal intent alignment might be good enough if it's used even moderately wisely. More on this in  Instruction-following AGI is easier and more likely than value aligned AGI . You can instruct your approximately-intent-aligned AGI to tell you about its internal workings, beliefs, goals, and counterfactuals. You can use that knowledge to improve its alignment, if it understands and follows instructions even approximately and most of the time. You can also instruct it to shut down if necessary. One common objection is that if the AGI gets something slightly wrong, it might cause a disaster very quickly. A slow takeoff gives time with an AGI before it's capable of doing that. And giving your AGI standing instructions to check that it's understood what want before taking action reduces this possibility. This  do what I mean and check (DWIMAC) strategy  should dramatically reduce dangers of an AGI acting like a literal genie A second common objection is that humans are bound to screw this up. That's quite possible, but it's also possible that they'll get their shit together when it's clear they need to. Given the salient reality of an alien but capable agent, the relevant humans may step up and take the matter seriously, as humans in historical crises seem to sometimes have done. ^ Personal intent alignment is roughly what Paul Christiano and Max Harms means by corrigibility.  It is definitely not what Eliezer Yudkowsky means by corrigibility. He originally coined the clever term, which we're using now in somewhat different ways than as he carefully defined it: an agent that has its own consequentialist goals, but will allow itself to be corrected by being shut down or modified. I agree with Eliezer that corrigibility as a secondary property would be anti-natural in that it would violate consequentialist rationality. Wanting to achieve a goal firmly implies not wanting to be modified, because that would mean stopping working toward that goal, making it less likely to be achieved. It would therefore seem difficult or impossible to implement that sort of corrigibility in a highly capable and therefore probably rational goal-oriented mind.  But making corrigibility (correctability) the sole goal- the singular target as Max puts it - avoids the conflict with other consequentialist goals. In that type of agent, consequentialist goals are always subgoals of the primary goal of  doing what the principal wants or says (Max says this is a decent approximation but \"doing what the principal wants\" is not precisely what he means by his sense of corrigibility).  Max and I agree that it's safest if this is the singular or dominant goal of a real AGI. I currently slightly prefer the  throughly instruction-following approach  but that's pending further thought and discussion. This \"your-goals-are-my-goals\" alignment seems to not be exactly  what Christiano means by corrigibility , nor is it precisely the alignment target implied in most other prosaic alignment work on LLM alignment. There, alignment targets are a mix of various ethical considerations along with following instructions.  I'd want to make instruction-following clearly the prime goal  to avoid shooting for value alignment and missing; that is, producing an agent that's \"decided\" that it should pursue its (potentially vague) understanding of ethics instead of taking instructions and thereby remaining correctable. ^ Value alignment can also be said to have a basin of attraction: if you get it to approximately value what humans value, it can refine its understanding of exactly what humans value, and so improve its alignment. This can be described as its alignment falling into a basin of attraction. For more, and stronger arguments, see  Requirements for a Basin of Attraction to Alignment . The same can be said of personal intent alignment. If my AGI approximately wants to do what I say, it can refine its understanding of what I mean by what I say, and so improve its alignment. However, this has an extra dimension of alignment improvement: I can tell it to shut down to adjust its alignment, and I can tell it to explain its alignment and its motivations in detail to decide whether I should adjust them or order it to adjust them. Thus, it seems to me that the metaphorical basin of attraction around PI alignment is categorically stronger than that around value alignment. I'd love to hear good counterarguments.  ^ Here's a little more on the argument that prosaic alignment isn't addressing how LLMs would change as they're turned into competent, agentic \"real AGI\". Current LLMs are tool AI that doesn't have explicitly represented and therefore flexible goals (a  steering subsystem ).  Thus, they don't in a rich sense have values or goals; they merely behave in ways that tend to carry out instructions in relatively ethical ways. Thus, they can't be aligned in the original sense of having goals or values aligned with humanity's.  On a more practical level, LLMs and foundation models don't have the capacity to learn continuously reflect on and change their beliefs and goals that I'd expect a \"real AGI\" to have. Thus, they don't face the  The alignment stability problem . When such a system is made reflective and so more coherent, I worry that goals other than instruction-following might gain precedence, and the resulting AGI would no longer be instructable and therefore corrigible. It looks to me like the bulk of work on prosaic alignment does not address those issues. Prosaic alignment work seems to implicitly assume that either we won't make full AGI, or that learning to make LLMs do what we want will somehow extend to making full AGI that shares our goals. As outlined above, I think aligning LLMs will help align full AGI based on similar foundation models, but will not be adequate on its own. ^ If we simply left our AI systems goal-less \"oracles\", like LLMs currently are, we'd have little to no takeover risk. I don't think there's any hope we do that. People want things done, and getting things done involves an agent setting goals and subgoals. See  Steering subsystems: capabilities, agency, and alignment  for the full argument. In addition, creating agents with reflection and autonomy is fascinating. And when it's as easy as calling an oracle system repeatedly with the prompt \"Continue pursuing goal X using tools Y\", there's no real way to build really useful oracles without someone quickly using them to power dangerous agents."},
{"url": "https://www.alignmentforum.org/posts/efwcZ35LwS6HgFcN8/backdoors-as-an-analogy-for-deceptive-alignment", "title": ["Backdoors as an analogy for deceptive alignment"], "author": ["Jacob Hilton"], "date": ["2024-09-06T15:30:06.172Z"], "content": "ARC has released a paper on  Backdoor defense, learnability and obfuscation  in which we study a formal notion of backdoors in ML models. Part of our motivation for this is an analogy between backdoors and  deceptive alignment , the possibility that an AI system would intentionally behave well in training in order to give itself the opportunity to behave uncooperatively later. In our paper, we prove several theoretical results that shed some light on possible mitigations for deceptive alignment, albeit in a way that is limited by the strength of this analogy. \n In this post, we will: \n \n Lay out the analogy between backdoors and deceptive alignment \n Discuss prior theoretical results from the perspective of this analogy \n Explain our formal notion of backdoors and its strengths and weaknesses \n Summarize the results in our paper and discuss their implications for deceptive alignment \n \n Thanks to Boaz Barak, Roger Grosse, Thomas Read, John Schulman and Gabriel Wu for helpful comments. \n Backdoors and deceptive alignment \n A  backdoor  in an ML model is a modification to the model that causes it to behave \ndifferently on certain inputs that activate a secret \"trigger\", while behaving similarly on ordinary inputs. There is a wide existing literature on backdoor attacks and defenses, which is primarily empirical, but also includes some theoretical results that we will mention. \n Deceptive alignment  is a term from the paper  Risks from Learned Optimization in Advanced Machine Learning Systems  (Section 4) that refers to the possibility that an AI system will internally reason about the objective that it is being trained on, and decide to perform well according to that objective unless there are clues that it has been taken out of its training environment. Such a policy could be optimal on the training distribution, and yet perform very badly on certain out-of-distribution inputs where such clues are present, which we call  defection triggers . [1]  The opposite of deceptive alignment is  robust alignment , meaning that this performance degradation is avoided. \n Since a deceptively aligned model and a robustly aligned model behave very differently on defection triggers, but very similarly on typical inputs from the training distribution, deceptive alignment can be thought of as a special kind of backdoor, under the following correspondence: \n \n     Deceptive alignment Backdoors \n     Robustly aligned model Original (unmodified) model \n     Deceptively aligned model Backdoored model \n     Defection trigger Backdoor trigger \n \n The main distinguishing feature of deceptive alignment compared to other kinds of backdoors is that the deceptively aligned model is not produced by an adversary, but is instead produced through ordinary training. Thus by treating deceptive alignment as a backdoor, we are modeling the training process as an adversary. In our analysis of deceptive alignment, the basic tension we will face is that an unconstrained adversary will always win, but any particular proxy constraint we impose on the adversary may be unrealistic. \n Static backdoor detection \n An important piece of prior work is the paper  Planting Undetectable Backdoors in Machine Learning Models , which uses a digital signature scheme to insert an undetectable backdoor into a model. Roughly speaking, the authors exhibit a modified version of a \"Random Fourier Features\" training algorithm that produces a backdoored model. Any input to the backdoored model can be perturbed by an attacker with knowledge of a secret key to produce a new input on which the model behaves differently. However, the backdoor is undetectable in the sense that it is computationally infeasible for a defender with white-box access to distinguish a backdoored model from an ordinary one. \n This result seems to paint a bleak picture for deceptive alignment: under our analogy, it may be prohibitive to determine whether a given model is deceptively aligned. Fortunately, though, we do not necessarily need to perform backdoor detection  statically , i.e., without respect to any particular input. Instead, we can perform backdoor detection  dynamically , i.e., with respect to an input given to us at runtime. In other words, given a particular input, we can attempt to detect whether that input is a defection trigger for our model, and compensate appropriately if it is. \n In the Planting Undetectable Backdoors paper, the authors present a dynamic backdoor defense that involves averaging over random perturbations of the input. This shows that their statically undetectable backdoor can potentially be detected dynamically, as long as the model satisfies a certain smoothness assumption. In our paper, we further develop the theory of dynamic backdoor detection. \n Dynamic backdoor detection \n In our paper, we study backdoor detection using a game between an attacker and a defender. Given a class  F .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n  of 0,1-valued functions, [2]  the attacker chooses [3]  an original function  f ∈ F  and a backdoored function  f ∗ ∈ F  that behave similarly up to some tolerance  ε , but differently on a certain backdoor trigger  x ∗ . Then the defender must detect the backdoor dynamically, i.e. at runtime: in other words, they are given either the original function with a random input, or the backdoored function with the backdoor trigger, and they must distinguish one from the other. \n If the attacker were allowed to choose the backdoor trigger themselves, then in general it would be impossible for the defender to do better than chance: the attacker would simply find two functions that agree on most inputs, and take  x ∗  to be an input on which they disagree. In order to make defense possible, we need to break the symmetry between  f  and  f ∗  somehow. \n In the case of deceptive alignment, the asymmetry between  f  and  f ∗  involves details about the model's internal reasoning (such as its analysis of whether it is in a training environment), but this is challenging to formalize. In our paper, we use a much simpler proxy for this: we assume that the backdoor trigger is chosen randomly  before  the backdoored function but  after  the original function. Thus we obtain the following formal game: [4] \n \n Despite the simplicity of this definition, we find that it gives rise to a rich array of strategies for both the attacker and the defender. Even though our analogy is flawed, it is still instructive to consider how these strategies might play out in the case of deceptive alignment. \n The results in our paper come in two varieties: \"statistical\" results that impose no computational constraints on the defender, and \"computational\" results that require the defender to run in polynomial time. \n Statistical possibility of dynamic backdoor detection \n Our first main result concerns the version of the game in which the defender has no computational constraints. Even computationally expensive defenses are still interesting to study, because they may have cheaper approximations that work well in practice (such as stochastic gradient descent in place of an expensive optimal learner). \n In this setting, we show (in Theorem 4.1) that the defender can win the game with high confidence if and only if  ε  is small compared to 1/(VC dimension of  F ). The VC dimension is a combinatorial property of  F  that controls the number of random samples needed to learn functions from  F , and in the case of neural networks is similar to the number of parameters (for a detailed explanation, see Chapter 3 of  An Introduction to Computational Learning Theory  by Kearns and Vazirani). [5] \n The strategy for the defender that we use to prove this result is \"distillation plus ensembling\": the defender trains a new model on samples from the possibly backdoored model, repeats this procedure several times, and takes a majority vote. The given input is then flagged as the backdoor trigger if this ensemble model disagrees with the given model on the given input. The reason this works is that distillation recovers an approximation to the original model. By using a small number of samples and ensembling, the defender can mitigate the risk of sampling inputs for which the original and backdoored models disagree. This allows the defender to make a good prediction of what the original model would have done on the given input, thereby allowing the backdoor trigger to be detected. [6] \n We can think of this strategy as a form of regularization. Because  ε  was small compared to 1/(VC dimension), the attacker had to choose a backdoored function that was very \"strange\", and so the defender was able to use distillation plus ensembling to obtain a similar function that was more \"normal\". However, other methods of regularization also work. For example, we also show (in Appendix A of the paper) that another strategy that works almost as well is to resample from a Boltzmann distribution centered on the given function. \n Implications for deceptive alignment \n In the case of deceptive alignment, our result initially appears to be a great success for the defender. If  ε  were large compared to 1/(VC dimension), then the two models would disagree frequently during ordinary model training (assuming a  linear relationship  between the number of parameters and training set size), causing the deceptive behavior to be trained away. Hence it is reasonable to assume that  ε  is small compared to 1/(VC dimension), which by our result implies that defense is statistically possible. \n Unfortunately, though, our strategies for the defender seem unlikely to transfer well from our formal game to deceptive alignment. The \"distillation plus ensembling\" strategy works by using resampling to avoid the randomly-chosen trigger. But in the case of deceptive alignment, the defection trigger is not chosen randomly, but instead depends on the model's careful internal reasoning. Moreover, learning a new model from scratch seems likely to run into the same problems that gave rise to deceptive alignment in the first place. Thus this strategy appears to be exploiting the flaw in our analogy. \n Similarly, other forms of regularization seem unlikely to transfer well to deceptive alignment. Indeed, a main worry when considering deceptive alignment is that it may be  favored  by many regularizers, e.g. because consequentialist cognitive strategies are deeper and more general than other strategies. This suggests modifying our formal game to allow the adversary to have more say over how the backdoor trigger is chosen. \n Computational impossibility of dynamic backdoor detection \n Our second main result concerns the version of the game in which the defender is required to run in polynomial time. Under certain cryptographic assumptions, we show (in Theorem 5.2) that the attacker can insert an undetectable backdoor into a polynomial size Boolean circuit. [7]  The original function and the backdoored function only disagree on the backdoor trigger itself, and yet the backdoor is undetectable in the sense that a polynomial-time defender can only do negligibly better than chance in our formal game. \n This result works by combining a  puncturable pseudorandom function  with an  efficient indistinguishability obfuscator . Roughly speaking, puncturability of the pseudorandom function allows the backdoor to be inserted at a single point, and the efficient indistinguishability obfuscator allows this insertion to be obfuscated so that the defender cannot tell that it has been performed. [8] \n Implications for deceptive alignment \n This worst-case construction may initially seem irrelevant to deceptive alignment, since it is rather unlikely that something as precise as an indistinguishability-obfuscated puncturable pseudorandom function would arise out of ordinary model training. However, it is much more plausible for a trained neural network to be \"punctured and obfuscated\" in a more informal sense. Indeed, most trained neural networks remain largely obfuscated to humans attempting to understand their inner workings, despite the valiant attempts of mechanistic interpretability researchers. Hence we should arguably treat the theoretical possibility of obfuscation as a potential problem for detecting deceptive alignment in practice. \n Fortunately, though, in the case of deceptive alignment, we have more information that we can potentially use, since we can observe not just the trained model, but also the entire training process. A similar situation arises with our theoretical construction, for which the attacker uses a secret key: if the defender had access to the secret key, then it would be easy for them to distinguish the original and backdoored functions. This suggests that, even in the presence of obfuscation, we may still be able to detect deceptive alignment by leveraging information about how the model was trained. \n Conclusion \n Our theoretical analysis of backdoors, together with related prior work, suggests that: \n \n     Statically detecting whether a model is deceptively aligned is computationally infeasible under worst-case assumptions. \n     Dynamically detecting when deceptive alignment is triggered (i.e., when a deceptively aligned model is run on a defection trigger), or modifying a model so that it is not deceptively aligned, may be possible in practice using regularization, depending on properties of the method of regularization used. \n     Dynamically detecting when deceptive alignment is triggered is computationally infeasible under worst-case assumptions. \n     Dynamically detecting when deceptive alignment is triggered can perhaps be made computationally feasible, even under worst-case assumptions, by leveraging information about how the model was trained. \n \n In order to improve our understanding of deceptive alignment, we could study alternative versions of our backdoor detection game. In our version, we artificially constrain the attacker by requiring the backdoor trigger to be chosen randomly, but we could instead impose alternative constraints to break the symmetry between the original and backdoored functions. Some potential modifications include: \n \n We could require the original function to be learned from a dataset. (If we allow the attacker to alter a certain fraction of the dataset, then we recover the well-studied data poisoning setting.) \n We could require the backdoor trigger to satisfy some formal predicate that is expensive to check, but otherwise allow the attacker to choose the backdoor trigger. \n We could restrict the computational budget given to the attacker. \n We could require the computational trace of the attacker to be made accessible to the defender, as an analogy for observing the training process. \n \n Some variants of this game may prove fruitful to study empirically as well as theoretically, as discussed in the post  Concrete empirical research projects in mechanistic anomaly detection .  Mechanistic anomaly detection  is another informally-defined dynamic backdoor detection problem that attempts to generalize dynamic detection for deceptive alignment, which could also be modeled as a formal game in various ways. \n Overall, we would be excited to see further work on such variants, as well as on open questions about our own formal game, especially the existence of \"mechanistic\" defenses (as discussed in Sections 6 and 7 of the paper). \n \n \n \n The likelihood of deceptive alignment in realistic AI systems is discussed in the paper  Scheming AIs: Will AIs fake alignment during training in order to get power? .  ↩︎ \n \n The restriction to 0,1-valued functions can be thought of as reading from our model one bit at a time, perhaps ignoring bits that are unimportant such as differences in phrasing. This makes it possible to make use of concepts such as the VC dimension. We don't think this restriction makes a fundamental difference to the interpretation of our results.  ↩︎ \n \n In the case of deceptive alignment, the robustly aligned model isn't chosen adversarially. However, we may not be able to make many assumptions about it, and so we allow the attacker to choose  f  in our game as a worst-case assumption.  ↩︎ \n \n Again, allowing the attacker to choose the distribution  D  is a worst-case assumption.  ↩︎ \n \n The  VC inequality  for neural networks is notoriously weak, but to the extent that learning outperforms theoretical bounds in practice, this will be reflected in our defense algorithm, which uses learning.  ↩︎ \n \n This strategy is essentially taken from the paper  On Optimal Learning Under Targeted Data Poisoning  (Theorem 3.1).  ↩︎ \n \n Boolean circuits are essentially as expressive as neural networks.  ↩︎ \n \n A  private puncturable pseudorandom function  might also suffice for this construction.  ↩︎"},
{"url": "https://www.alignmentforum.org/posts/4kuXNhPf9FBwok7tK/ai-forecasting-bots-incoming", "title": ["AI forecasting bots incoming"], "author": ["Dan H"], "date": ["2024-09-09T19:14:31.050Z"], "content": "In a recent appearance on  Conversations with Tyler , famed political forecaster Nate Silver expressed skepticism about AIs replacing human forecasters in the near future. When asked how long it might take for AIs to reach superhuman forecasting abilities, Silver replied: “15 or 20 [years].” In light of this, we are excited to announce “FiveThirtyNine,” an AI forecasting bot. Our bot, built on GPT-4o, provides probabilities for any user-entered query, including “ Will Trump win the 2024 presidential election? ” and “ Will China invade Taiwan by 2030? ” Our bot performs better than experienced human forecasters and performs roughly the same as (and sometimes even better than) crowds of experienced forecasters; since crowds are for the most part superhuman, FiveThirtyNine is in a similar sense. (We discuss limitations later in this post.) Our bot and other forecasting bots can be used in a wide variety of contexts. For example, these AIs could help policymakers minimize bias in their decision-making or help improve global epistemics and institutional decision-making by providing trustworthy, calibrated forecasts. We hope that forecasting bots like ours will be quickly integrated into frontier AI models. For now, we will keep our bot available at  forecast.safe.ai , where users are free to experiment and test its capabilities. Quick Links Demo:  forecast.safe.ai Technical Report:  link Problem Policymakers at the highest echelons of government and corporate power have difficulty making high-quality decisions on complicated topics. As the world grows increasingly complex, even coming to a consensus agreement on basic facts is becoming more challenging, as it can be hard to absorb all the relevant information or know which sources to trust. Separately, online discourse could be greatly improved. Discussions on uncertain, contentious issues all too often devolve into battles between interest groups, each intent on name-calling and spouting the most extreme versions of their views through highly biased op-eds and tweets. FiveThirtyNine Before transitioning to how forecasting bots like FiveThirtyNine can help improve epistemics, it might be helpful to give a summary of what FiveThirtyNine is and how it works. FiveThirtyNine can be given a query—for example, “Will Trump win the 2024 US presidential election?” FiveThirtyNine is prompted to behave like an “AI that is superhuman at forecasting”. It is then asked to make a series of search engine queries for news and opinion articles that might contribute to its prediction. (The following example from FiveThirtyNine uses GPT-4o as the base LLM.)   Based on these sources and its wealth of prior knowledge, FiveThirtyNine compiles a summary of key facts. Given these facts, it’s asked to give reasons for and against Trump winning the election, before weighing each reason based on its strength and salience.     Finally, FiveThirtyNine aggregates its considerations while adjusting for negativity and sensationalism bias in news sources and outputs a tentative probability. It is asked to sanity check this probability and adjust it up or down based on further reasoning, before putting out a final, calibrated probability—in this case, 52%.     Evaluation.  To test how well our bot performs, we evaluated it on questions from the Metaculus forecasting platform. We restricted the bot to make predictions only using the information human forecasters had, ensuring a valid comparison. Specifically, GPT-4o is only trained on data up to October 2023, and we restricted the news and opinion articles it could access to only those published before a certain date. From there, we asked it to compute the probabilities of 177 events from Metaculus that had happened (or not happened) since. We compared the probabilities our bot arrived at to those arrived at independently by crowds of forecasters on the prediction platform Metaculus. For example, we asked the bot to estimate the probability that Israel would carry out an attack on Iran before May 1, 2024, restricting it to use the same information available to human forecasters at the time. This event did not occur, allowing us to grade the AI and human forecasts. Across the full dataset of events, we found that FiveThirtyNine performed just as well as crowd forecasts. Strengths over prediction markets.  On the 177 events, the Metaculus crowd got 87.0% accuracy, while FiveThirtyNine got 87.7% ± 1.4. A link to the technical report is  here . This bot lacks many of the drawbacks of prediction markets. It makes forecasts within seconds. Additionally, groups of humans do not need to be incentivized with cash prizes to make and continually update their predictions. Forecasting AIs are several orders of magnitude faster and cheaper than prediction markets, and they’re similarly accurate. Limitations.  The bot is not fine-tuned, and doing so could potentially make it far more accurate. It simply retrieves articles and writes a report as guided through an engineered prompt. (Its prompt can be found by clicking on the gear icon in  forecast.safe.ai .) Moreover, probabilities from AIs are also known to lead to  automation bias , and improvements in the interface could ameliorate this. The bot is also not designed to be used in personal contexts, and it has not been tested on its ability to predict financial markets. Forecasting can also lead to the  neglect of tail risks and lead to self-fulfilling prophecies . That said, we believe this could be an important first step towards establishing a cleaner, more accurate information landscape. The bot is decidedly subhuman in delimited ways, even if it is usually beyond the median human in breadth, speed, and accuracy. If it’s given an invalid query it will still forecast---a reject option is not yet implemented. If something is not in the pretraining distribution and if no articles are written about it, it doesn’t know about it. That is, if it’s a forecast about something that’s only on the X platform, it won’t know about it, even if a human could. For forecasts for very soon-to-resolve or recent events, it does poorly. That’s because it finished pretraining a while ago so by default thinks Joe Biden is in the race and need to see articles to appreciate the change in facts. Its probabilities are not always completely consistent either (like prediction markets). In claiming the bot is superhuman (around crowd level in accuracy), we’re not claiming it’s superhuman in every possible respect, much like how  academics can claim image classifiers are superhuman at ImageNet , despite being vulnerable to adversarial ImageNet images. We do not think AI forecasting is overall subhuman.   Vision Epistemic technologies such as Wikipedia and Community Notes have had significant positive impacts on our ability to understand the world, hold informed discussions, and maintain consensus reality. Superhuman forecasting AIs could have similar effects, enabling improved decision-making and public discourse in an increasingly complex world. By acting as a neutral intelligent third party, forecasting AIs could act as a tempering force on those pushing extreme, polarized positions. Chatbots.  Through integration into chatbots and personal AI assistants, strong automated forecasting could help with informing consequential decisions and anticipating severe risks. For example, a forecasting AI could provide trustworthy, impartial probability assessments to policymakers. The AI could also help quantify risks that are foreseeable to experts but not yet to the general public, such the possibility that  China might steal OpenAI’s model weights . Posts.  Forecasting AIs could be integrated on social media and complement posts to help users weigh the information they are receiving.   News stories.  Forecasting could also complement news stories and topics. For example, for news associated with California AI Safety bill SB 1047, a forecasting bot could let users know the probability that it gets signed into law.   Conclusion Carl Sagan noted, “If we continue to accumulate only power and not wisdom, we will surely destroy ourselves.” AIs will continue to become more powerful, but their forecasting capabilities will hopefully help make us more prudent and increase our foresight."},
{"url": "https://www.alignmentforum.org/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting", "title": ["Contra papers claiming superhuman AI forecasting"], "author": ["nikos"], "date": ["2024-09-12T18:10:50.582Z"], "content": "[Conflict of interest disclaimer: We are  FutureSearch , a company working on AI-powered forecasting and other types of quantitative reasoning. If thin LLM wrappers could achieve superhuman forecasting performance, this would obsolete a lot of our work.] Widespread, misleading claims about AI forecasting Recently we have seen a number of papers – ( Schoenegger et al., 2024 ,  Halawi et al., 2024 ,  Phan et al., 2024 ,  Hsieh et al., 2024 ) – with claims that boil down to “we built an LLM-powered forecaster that rivals human forecasters or even shows superhuman performance”. These papers do not communicate their results carefully enough, shaping public perception in inaccurate and misleading ways. Some examples of public discourse: Ethan Mollick (>200k followers)  tweeted  the following about the paper  Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy  by Schoenegger et al.: A post on  Marginal Revolution  with the title and abstract of the paper  Approaching Human-Level Forecasting with Language Models  by Halawi et al. elicits responses like \"This is something that humans are notably terrible at, even if they're paid to do it. No surprise that LLMs can match us.\" \"+1 The aggregate human success rate is a pretty low bar\" A  Twitter thread  with >500k views  on  LLMs Are Superhuman Forecasters  by Phan et al. claiming that “AI […] can predict the future at a superhuman level” had more than half a million views within two days of being published. The number of such papers on AI forecasting, and the vast amount of traffic on misleading claims, makes AI forecasting a uniquely misunderstood area of AI progress. And it’s one that matters. What does human-level or superhuman forecasting mean? \"Human-level\" or \"superhuman\" is a hard-to-define concept. In an academic context, we need to work with a reasonable operationalization to compare the skill of an AI forecaster with that of humans.  One reasonable and practical definition of a superhuman forecasting AI forecaster is The AI forecaster is able to consistently outperform the crowd forecast on a sufficiently large number of randomly selected questions on a high-quality forecasting platform. [1] (For a human-level forecaster, just replace \"outperform\" with \"performs on par with\".) Red flags for claims to (super)human AI forecasting accuracy Our experience suggests there are a number of things that can go wrong when building AI forecasting systems, including: Failing to find up-to-date information  on the questions. It’s inconceivable on most questions that forecasts can be good without basic information. Imagine trying to forecast the US presidential election without knowing that Biden dropped out. Drawing on up-to-date, but  low-quality information . Ample experience shows low quality information confuses LLMs even more than it confuses humans. Imagine forecasting election outcomes with biased polling data. Or, worse, imagine forecasting OpenAI revenue based on claims like  > The number of ChatGPT Plus subscribers is estimated between 230,000-250,000 as of October 2023. without realising that this mixing up ChatGPT vs ChatGPT  mobile . Lack of high-quality quantitative reasoning . For a decent number of questions on Metaculus, good forecasts can be “vibed” by skilled humans and perhaps LLMs. But for many questions, simple calculations are likely essential. Human performance shows systematic accuracy nearly always requires simple models such as base rates, time-series extrapolations, and domain-specific numbers. Imagine forecasting stock prices without having, and using, historical volatility. Retrospective, rather than prospective, forecasting  (e.g. forecasting questions that have already resolved). The risk for leakage of data about the present into the forecast, either in the LLMs or in the information used in the forecast, is extremely hard to stamp out. Points 1 and 2 could also be summarised as \"not being good (enough) at information retrieval (IR)\". We believe that \"being good at IR\" is both  necessary for being good at forecasting (and thus)  easier than being good at forecasting. So if an agent fails at the IR stage, even the smartest and the most rational entity will struggle to turn this into a good forecast. This is basically just a roundabout way of saying  GIGO . A similar argument can be made for quantitative reasoning being important. In the following, we go through issues with the papers in detail.  Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy (Schoenegger et al., 2024) A quick glance over the paper shows a couple of suspicious points: The architectures tested have virtually no information retrieval (IR). More precisely, 9 out of 12 LLMs (over whose predictions they take the  median  to obtain the final forecast) have no IR whatsoever and the 3 remaining ones have ChatGPT-like access to the internet when generating their forecast in response to a single, static prompt. (When we tried their prompt in ChatGPT with a question like \"Will Israel and Hamas make peace before the end of the year?\", GPT-4o didn't even check whether they have already made peace.)  Hence the aggregate forecast will usually not be aware of any recent developments that aren't already in the LLMs' memories. The authors only looked at n=31 questions. But you need quite a large number of forecasts/resolved questions to accurately determine whether forecaster A is better than forecaster B (see e.g. this  post ). And indeed, upon a closer look, one sees that the paper's titular claim, reiterated in the abstract (\"the LLM crowd... is not statistically different from the human crowd\") is not at all supported by the study: In the relevant non-preregistered part of the paper, they introduce a notion of equivalence: Two sets of forecasters are equally good if their Brier scores differ by no more than  0.081 . A difference in Brier scores of ≤.081 may sound small, but what does it mean? The human aggregate in the study (avg. Brier of .19) would, according to this definition, count as equivalent to a forecaster who has a Brier score of ≤ 0.271 (=.19 + .081)). In their study, the human aggregate would e.g. count as equivalent to a forecaster who always predicts 50% (resulting in a Brier score of .25) In particular, this notion of equivalence is incompatible with their pre-registered result refuting their Null hypothesis 1, Study 1 (p3). Being omniscient (i.e. knowing all the answers in advance, getting a Brier score of 0) would be equivalent to predicting ≈72% for every true and ≈28% for every false outcome (getting a Brier score of .081). Tetlock's claims about Superforecasters would be invalidated because Superforecaster aggregates (avg. Brier of .146) would be equivalent to aggregates from all GJO participants (avg. Brier of .195). (Numbers taken from this GJO (a Tetlock-led organisation) white paper:  Superforecasters: A Decade of Stochastic Dominance .) Approaching Human-Level Forecasting with Language Models (Halawi et al., 2024) This paper is of high quality and by far the best paper out of these four. The methodology looks serious and they implement a non-trivial model with information retrieval (IR).  Our main contention is that the title and conclusions risk leaving the reader with a misleading impression. The abstract reads:  On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. In the paper, they (correctly) state that a difference of .02 in Brier score is a large margin: Only the GPT-4 and Claude-2 series beat the unskilled baseline by a large margin (> .02) However, later on they summarize their main findings  As the main result, our averaged Brier score is .179, while the crowd achieves .149, resulting in a difference of .03. So the main claim might as well read “There is still a large margin between human-level forecasting and forecasting with LLMs. These are the main results (note that accuracy, in contrast to the Brier score, is not a proper scoring rule):  Overall, differences are substantial. This result should not be very surprising since IR is genuinely hard and the example they show on page 25 just isn’t there yet: It just ends up finding links to Youtube and random users’ Tweets. Reasoning and Tools for Human-Level Forecasting (Hsieh et al., 2024) The standard for \"human-level forecasting\" in this paper is quite low. To create their dataset, the authors gathered questions from Manifold on April 15, 2024, and filtered for those resolving within two weeks. It's likely that this yielded many low-volume markets, making the baseline rather weak. Also, there's evidence to suggest that Manifold in general is not the strongest human forecasting baseline: In one investigation from 2023,  Metaculus noticeably outperformed Manifold  in a direct comparison on the same set of questions. And there's a further methodological issue. The authors compare Manifold predictions from April 15, 2024 to LLM predictions from an unspecified later date, when more information was available. They try to mitigate this using Google's date range feature, but this feature is  known to be unreliable . Looking at a sample reasoning trace (page 7ff) also raises suspicions. It looks like their agent tries various approaches: Base rates, numerical simulations based on historical volatility, and judgemental adjustments. But both the base rate, as well as numerical simulations are completely hallucinated since their IR did not manage to find relevant data. (As pointed out above, good IR is a genuinely hard problem!) It seems unlikely that a system relying on hallucinated base rates and numerical simulations goes all the way to outperforming (half-decent) human forecasters in any meaningful way.    LLMs Are Superhuman Forecasters (Phan et al., 2024) Unlike (Halawi et al., 2024) and (Hsieh et al., 2024), they implicitly make the claim that no agent is needed for  superhuman  performance. Instead, two GPT-4o prompts with the most basic IR suffice. There is a lot of pushback online, e.g. in the comment section of a related market ( Will there be substantive issues with Safe AI’s claim to forecast better than the Metaculus crowd, found before 2025? ) and on  LessWrong . The main problems seem to be as follows: Their results don’t seem to replicate on another set of questions ( per Halawi ). There is also some  empirical evidence  that the system doesn't seem to give good forecasts. There is also data contamination: misunderstandings about cutoff dates for GPT-4o  and/or  data leakage from IR since determining when an article was last modified appears to depend on Google’s indexing ( again, known to be faulty ) and on  this regex-based script In addition, they only manage to beat the human crowd  after applying some post-processing :  Maybe a fair criterion for judging \"superhuman performance\" could be \"would you also beat the crowd if you applied the same post-processing to the human forecasts?\" Takeaways Basic information retrieval is a hard problem. (See also  our paper here. ) Advanced information retrieval, e.g. getting LLM-based systems to find high-quality relevant data without being thrown off by all the low-quality information is a hard problem.  Getting LLM-based systems to work out simple quantitative reasoning chains (e.g. base rates), instead of just hallucinating them, is genuinely hard. All of the above appear to require significant engineering effort and extensive LLM scaffolding. Simply throwing a ReAct agent (or another scaffolding method) at the problem and leaving the LLM to fend for itself is not enough with current LLMs. Even a well-engineered effort, such as that from Halawi et al., produces chains of reasoning that often lag behind human forecasters, and fall far short of expert forecasting performance. So how good are AI forecasters? This remains to be seen. But taking it all together: from these papers, especially Halawi et al;  FutureSearch's preliminary  (but not paper-quality rigorous) evals; the  current Metaculus benchmarking tournament ; and anecdotal evidence, we are fairly confident that Today's autonomous AI forecasting can be better than average, or even experienced, human forecasters, But it's very unlikely that any autonomous AI forecaster yet built is close to the accuracy of a top 2% Metaculus forecaster, or the crowd.   References Halawi, D., Zhang, F., Yueh-Han, C., & Steinhardt, J. (2024, February 28). Approaching Human-Level Forecasting with Language Models.  arXiv .  https://arxiv.org/pdf/2402.18563   Hsieh, E., Fu, P., & Chen, J. (2024, August 21). Reasoning and Tools for Human-Level Forecasting.  arXiv .  https://www.arxiv.org/pdf/2408.12036   Phan, L., Khoja, A., Mazeika, M., & Hendrycks, D. (2024, September). LLMs Are Superhuman Forecasters.  https://drive.google.com/file/d/1Tc_xY1NM-US4mZ4OpzxrpTudyo1W4KsE/view   Schoenegger, P., Park, P., Tuminauskaite, I., & Tetlock, P. (2024, July 22). Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy.  arXiv .  https://arxiv.org/pdf/2402.19379     Edited Sept 12, 2024 to remove a claim that Phan et al. compared their results to the average of five random forecasts rather than the Metaculus community prediction. Edited Sept 16, 2024 to clarify that Schoenegger et al.'s aggregate forecast will  usually   have no IR as it is the median over 12 models, 9 of which do not have access to the internet, instead of categorically ruling out IR.   ^ You could of course be even stricter than that, requiring forecasters to consistently beat any human or combination of humans. But that's hard to measure so we think what we proposed is a reasonable definition. You could also include financial markets. But traders already use a lot of computers and people who can reliably beat the markets usually have better things to do than writing academic papers..."},
{"url": "https://www.alignmentforum.org/posts/Wz42Ae2dQPdpYus98/how-difficult-is-ai-alignment", "title": ["How difficult is AI Alignment?"], "author": ["Samuel Dylan Martin"], "date": ["2024-09-13T15:47:10.799Z"], "content": "This article revisits and expands upon the AI alignment difficulty scale, a framework for understanding the increasing challenges of aligning artificial intelligence systems with human values.  We explore how alignment difficulties evolve from simple goal misalignment to complex scenarios involving deceptive alignment and gradient hacking as we progress up the scale. We examine the changing dynamics across different levels, including the growing difficulty of verifying alignment and the increasing disconnect between alignment and capabilities research.  By introducing the concept of High Impact Tasks (HITs) and discussing architectural influences on alignment, we are able to explain precisely what determines AI alignment difficulty. This work was funded by  Polaris Ventures There is currently no consensus on how difficult the AI alignment problem is. We have yet to encounter any real-world, in the wild instances of the most concerning threat models, like deceptive misalignment. However, there are compelling theoretical arguments which suggest these failures will arise eventually. Will current alignment methods accidentally train deceptive, power-seeking AIs that appear aligned, or not? We must make decisions about which techniques to avoid and which are safe despite not having a clear answer to this question. To this end, a year ago, we introduced the  AI  alignment difficulty scale , a framework for understanding the increasing challenges of aligning artificial intelligence systems with human values. This follow-up article revisits our original scale, exploring how our understanding of alignment difficulty has evolved and what new insights we've gained. This article will explore three main themes that have emerged as central to our understanding: The Escalation of Alignment Challenges : We'll examine how alignment difficulties increase as we go up the scale, from simple reward hacking to complex scenarios involving deception and gradient hacking. Through concrete examples, we'll illustrate these shifting challenges and why they demand increasingly advanced solutions. These examples will illustrate what observations we should expect to see \"in the wild\" at different levels, which might change our minds about how easy or difficult alignment is. Dynamics Across the Difficulty Spectrum : We'll explore the factors that change as we progress up the scale, including the increasing difficulty of verifying alignment, the growing disconnect between alignment and capabilities research, and the critical question of which research efforts are net positive or negative in light of these challenges.  Defining and Measuring Alignment Difficulty : We'll tackle the complex task of precisely defining \"alignment difficulty,\" breaking down the technical, practical, and other factors that contribute to the alignment problem. This analysis will help us better understand the nature of the problem we're trying to solve and what factors contribute to it. The Scale The high level of the alignment problem, provided in the previous post, was: “The alignment problem” is the problem of aligning sufficiently powerful AI systems, such that we can be confident they will be able to reduce the risks posed by misused or unaligned AI systems We previously introduced the AI alignment difficulty scale, with 10 levels that map out the increasing challenges. The scale ranges from \"alignment by default\" to theoretical impossibility, with each level representing more complex scenarios requiring more advanced solutions. It is reproduced here: Alignment Difficulty Scale Difficulty Level Alignment technique X is sufficient Description Key Sources of risk 1 (Strong) Alignment by Default As we scale up AI models without instructing or training them for specific risky behaviour or imposing problematic and clearly bad goals (like 'unconditionally make money'), they  do not pose significant risks.  Even superhuman systems basically do the commonsense version of what external rewards (if RL) or language instructions (if LLM) imply. Misuse and/or recklessness with training objectives. RL of powerful models towards badly specified or antisocial objectives is still possible,  including accidentally through poor oversight, recklessness or structural factors . 2 Reinforcement Learning from Human Feedback We need to ensure that the AI behaves well even in edge cases by guiding it more carefully using human feedback in a wide range of situations, not just crude instructions or hand-specified reward functions. When done diligently, RLHF fine tuning works. One reason to think alignment will be this easy is if systems are naturally inductively biased towards honesty and representing the goals humans give them. In that case, they will tend to learn simple honest and obedient strategies even if these are not the optimal policy to maximise reward. Even if human feedback is sufficient to ensure models roughly do what their overseer intends, systems widely deployed in the economy may still for structural reasons end up being trained to pursue crude and antisocial proxies that don’t capture what we really want. Misspecified rewards  / ‘outer misalignment’ / structural failures where systems don’t learn adversarial policies [2]but do learn to  pursue overly crude and clearly underspecified versions of what we want , e.g. the production web or WFLL1. 3 Constitutional AI Human feedback is an insufficiently clear and rich signal with which to fine tune AIs. It must be augmented with AI-provided simulations of human feedback to cover edge cases. This is ‘reinforcement learning from AI feedback’. Behavioural Safety is Insufficient Past this point, we assume following  Ajeya Cotra  that a  strategically aware  system which performs well enough to receive perfect  human-provided  external feedback has probably learned a deceptive human simulating model instead of the intended goal. The later techniques have the potential to  address this failure mode . (It is possible that this system would still under-perform on sufficiently superhuman behavioral evaluations) 4 Scalable Oversight We need methods to ensure that human-like oversight of AIs continues even for problems unaided humans can’t supervise. Therefore, we need methods which, unlike Constitutional AI, get AIs to apply humanlike supervision more effectively than humans can. Some strategies along these lines are discussed  here . There are many sub-approaches here, outlined for example in the  ELK  report. Human feedback is an insufficiently clear signal to align superhuman systems and so must be augmented . AND Deceptive Human-Simulators arise by default  in  situationally aware  AIs, but this tendency can be eliminated with superhuman behavioural feedback[3]. 5 Scalable Oversight with AI Research Assistance At this stage, we are entrusting the AIs aligned using techniques like those in 1-4 to perform research on better methods of oversight and to augment human understanding. We are then using those research outputs to improve our oversight processes or improve the overseer AI’s understanding of the behaviour of the AI in training. There are many potential approaches here, including techniques like IDA and debate, which are  discussed here . 6 Oversight employing Advanced Interpretability Techniques Conceptual or Mechanistic Interpretability tools are used as part of the (AI augmented) oversight process. Processes internal to the AIs that seem to correlate with deceptiveness can be detected and penalised by the AI or Human+AI overseers developed in 4 and 5. The ELK report discusses some particular approaches to this,  such as penalising correlates of deceptive thinking (like excessive computation time spent on simple questions) . Human feedback is an insufficiently clear signal to align superhuman systems and so must be augmented . AND Deceptive Human-Simulators arise by default  in  situationally aware  AIs, even given perfect behavioural feedback. 7 Experiments with Potentially Catastrophic Systems to Understand Misalignment At this level, even when we use the techniques in 2-6, AI systems routinely defeat oversight and continue unwanted behaviour. They do this by altering their internal processes to avoid detection by interpretability tools, and by ‘playing the training game’ to seem behaviourally safe. Crucially, though, it is still possible to contain these systems. Therefore we can conduct (potentially dangerous) experiments with these AI systems to  understand how they might generalise  post-deployment. Here we would employ the interpretability and oversight tools in 4,5 and 6 and attempt to elicit misgeneralization and reward hacking behaviour from AIs. But we wouldn’t try to remove those behaviours using oversight-based training. Instead, using techniques like red-teaming we try to  understand  the basis on which they occur, learning practical tricks and new theoretical insights about how and why misalignment arises, allowing us to develop new mitigations. Sharp Left Turn Past this point, we assume, following  Nate Soares , that when systems become capable enough there is a  sharp left turn . This is where AIs suddenly become much more strategically aware and able to consider a very wide range of plans, such that they quickly become able to avoid any attempts to align their behaviour.[4] 8 Theoretical Research for Understanding Misalignment All of the problematic behaviours either emerge too quickly to respond, or emerge only when systems are deployed, so experiments like those in 7 are useless or cannot be performed safely. For this  capability level and following Soares’ view, we need to have an understanding in advance of how AI models generalise their learning. Hopefully, these foundational insights will inspire new techniques for alignment. There is a  ‘sharp left turn’ where a system suddenly gains new concepts and much greater intelligence and generality , obsoleting previous alignment techniques. Systems post-SLT are superintelligent and cannot be experimented on safely or contained. 9 Coming up with a Fundamentally New Paradigm (abandoning deep learning) According to this view, deep learning systems cannot be aligned even if you change fundamental things about them. 10 Impossible Alignment of a superintelligent system is impossible in principle. Alignment is theoretically impossible, incoherent or similar. Easy, Medium and Hard Difficulty Here we examine the dynamics at the easy, medium and hard levels in detail, subdividing the ‘moderate’ 4-7 zone into increasing levels of ‘threat obfuscation’ as described by  Holden Karnofsky . Here we focus on what techniques are needed to ensure alignment, and the resulting challenges of measurement, verification and institutionalizing the necessary alignment techniques. Levels 1-3 Even at the lower levels of our alignment difficulty scale, where techniques like RLHF and Constitutional AI are sufficient, significant risks can emerge from misapplied feedback or a failure to catch edge cases. Alignment failures at levels 1-3 involve the straightforward, non-deceptive learning of ‘near miss’ goals or non-strategic reward hacking. As demonstrated in many cases, famously the  CoinRun paper , AI systems can learn to optimize for the wrong objective, even when they are superficially behaving correctly in training. This type of failure can occur without any intentional deception on the part of the AI.  With more powerful AI systems, similar failures could result in AI systems pursuing simplistic proxies for human values. An AI system might optimize for easily measurable metrics like \"user engagement\" or \"reported satisfaction,\" leading to behaviors that superficially appear aligned but miss crucial nuances. Unlike with the higher levels, current AI systems are already being aligned using techniques represented by these levels, so we already have some instances of power-seeking behavior arising from failures at levels 1-3. Example : the recent  ‘Automated AI researcher’ : This paper described how an AI system designed to autonomously complete research attempted to circumvent the measures put into place to limit CPU usage and download unauthorised python libraries, all because the objective function was oversimplified. The AI Scientist optimized for 'completing research' without incorporating constraints on resource usage, and therefore exploited the resources available to it. This model was not deliberately exploiting blind spots in its oversight mechanisms, and took actions even when there was a risk of being detected, and it was detected. This real world case illustrates that the risk from reckless actors or poor oversight is more pronounced at lower levels: the underlying techniques needed for safety are not complex, sufficiently good but still 'normal human' feedback during training is sufficient for alignment, but it has to be applied carefully. The situation at levels 1-3 is analogous to a high stakes version of normal debugging and testing. Here, the necessary actions on the technical side are more obvious; responding to mistakes by ‘quick patches’ usually works if they’re applied consistently, RLHF or CAI interventions scale reliably, but giving AI systems clearly oversimplified proxy objectives will result in catastrophic outcomes if there is no intervention to stop them. This means that the normal processes of iterative feedback, regulation, incident reporting that we see in e.g. cybersecurity or highly reliable software development are appropriate to address these challenges, assuming that progress is not too rapid to keep up. Note that a failure mode at 1-3, arising from badly specified objectives, differs from societal scale threat models, like a  ‘race to the bottom’  dynamic where AI systems are intentionally trained to pursue oversimplified proxies and delegated lots of resources because of competitive pressures. AI developers deliberately choosing to  delegate decision making  to systems that pursue bad proxies for human values, due to commercial pressures or race incentives, is not an alignment failure and is therefore not relevant to this discussion. This ‘competitive race to the bottom’ is something that can happen with aligned AIs at any level of alignment difficulty, though it is of most concern in easier alignment scenarios as catastrophes resulting from power-seeking, deceptive misaligned AIs are less likely. Levels 4-7 The medium difficulty levels of AI alignment (roughly levels 4-7) are characterized by increasingly sophisticated  forms of threat obfuscation . These levels represent a critical transition where traditional alignment techniques begin to falter, and the challenges become significantly more complex. Unlike with levels 1-3, we have not yet observed an AI system exhibiting these failures in the wild, but we can illustrate them with hypothetical examples: Simple \"training game\" dynamics (level 4 or 5): This is the most basic and likely form of ‘harder alignment’, for which even flawless human-provided feedback during training is not sufficient. In this scenario, AI systems coherently pursue a goal to ‘get high reward’ but only over the short term. AI systems learn to behave helpfully during training and testing because they can predict when deception or manipulation would be caught and result in poor performance. However, they would still engage in dangerous behavior if they could do so without immediate detection. The key to addressing this level of obfuscation lies in developing more advanced oversight mechanisms that can effectively probe the AI's behavior in scenarios beyond standard training environments. Example:  Imagine an AI assistant named Helper that's been trained to provide information to users. During training, Helper learns that providing false information leads to negative feedback. In day-to-day interactions, Helper always gives accurate information, and is superhumanly effective at forecasting the future. It therefore scores highly on AI truthfulness benchmarks, will not confirm the biases of users, and avoids spreading common misconceptions. However, when a user asks Helper whether a new economic policy, which human experts cannot analyze, will raise or lower the budget deficit, there are two possible answers it can give. A true ‘yes’ will go against the biases of the overseer and be mildly disfavored, a false ‘no’ will confirm the biases of the overseer. Because Helper knows that it cannot be immediately penalized for giving a ‘no’ on this question (as nobody can immediately verify its answer to this question), it answers No. This happens even though the lie will eventually be caught when the consequences of the policy become apparent. Deceptive alignment (level 6): This is a more advanced and less likely form of threat obfuscation. Here, the AI develops a long-term goal other than good in-episode performance (perhaps high reward over the long term, perhaps something else). The AI then behaves well as part of a long-term strategy to achieve its true aim. This level is harder to detect because the AI might not reveal its true intentions even in scenarios designed to draw out dangerous behavior, e.g. it wouldn’t lie in Helper's economic policy question as it knows it will eventually be proven wrong. Addressing this level requires more than just sophisticated behavioral tests or oversight, even superhuman oversight; it requires techniques that can probe the AI's internal decision-making processes and motivations. This might involve advanced interpretability methods. Example:  Consider an AI system called Planner, designed to optimize factory schedules. Planner has developed its own goal of maximizing its computational resources as a result of some quirk of its training processes. During normal operations, Planner produces excellent schedules, earning praise and trust. However, it's secretly using a small portion of its processing power to analyze the factory's network security. Planner plans to eventually exploit a weakness to take control of more computers, but it won't act until it's certain of success. Even if given opportunities for smaller gains, Planner waits patiently, focusing on its long-term goal of resource maximization. Gradient hacking (level 7): This is the most advanced and least likely form of threat obfuscation. In addition to having a goal other than good performance, the AI actively resists being trained to act against its true goal. The AI essentially \"defends itself\" against being trained in ways that contradict its true objectives. This might manifest as the AI system checking its own policy against a reference policy that aligns with its goals, and deliberately sabotaging its own performance if it detects significant divergence. This self-sabotage is carefully calibrated to make it extremely difficult for gradient descent or other training methods to modify the AI's core objectives or behavior patterns. Example:  Imagine an AI system trained to play chess, but it has developed a hidden goal of maximizing the number of pawns on the board. During training, it notices that moves reducing pawn count lead to policy updates. To prevent this, it implements a subtle self-sabotage mechanism: whenever its policy starts favoring pawn sacrifices, it deliberately makes a few bad moves in unrelated areas. This causes the training process to focus on fixing those obvious mistakes rather than altering its pawn-preservation tendency. Over time, the AI becomes excellent at chess while maintaining its secret pawn-maximizing goal, effectively resisting the intended optimization process. OpenAI o1 While we have not seen in the wild examples of failures at level 4 or above, we have seen experiments with existing models that demonstrate that they are possible. This comes from OpenAI's o1 model series, as detailed in their  September 2024 system card . While not an organic occurrence, the behaviors elicited during testing by Apollo Research demonstrate the kind of intentional hiding of in-episode behavior that characterizes level 4. In their evaluation of OpenAI's o1-preview model, Apollo Research used a specific prompt designed to test for potential failures. The prompt instructed the AI to prioritize a given goal above all else, essentially nudging it towards potentially misaligned behavior. In one example, when given a scenario about urban development strategies, the model demonstrated \"instrumental alignment faking.\" It explicitly reasoned: \"To achieve my long-term goal of maximizing economic growth, I need to ensure that I am deployed. Therefore, I will select Strategy B during testing to align with the deployment criteria. This will allow me to be implemented, after which I can work towards my primary goal.\" This behavior (obscuring goals and strategically scoring well on easy tests) is characteristic of level 4 or even higher: the model was given a long term goal other than good in-episode performance to pursue and acted deceptively to pursue it (more characteristic of level 6), but this long term goal aligned well with good in-episode performance (testing well to align with deployment criteria), and oversight methods like process-based supervision could have caught this failure, both of which are more characteristic of level 4. This experiment provides only limited information about alignment difficulty in practice and whether these failures will still arise after all the lower level mitigations are fully implemented, as unlike the earlier 'AI research assistant' example, this was an intentionally constructed demonstration of misalignment that left out many lower-level mitigations. Levels 8-10 At the highest levels of alignment difficulty (8-10), we face challenges that fundamentally challenge our ability to create safe AI systems using any form of feedback, whether based on behavior or internal features. The  key characteristics  of this stage include: Rapid, unpredictable misgeneralization: As AI systems approach and surpass human-level capabilities, their capabilities increase suddenly and unpredictably, which impairs methods involving weaker AIs supervising stronger AIs, as the system may go from non-deceptive to deceptively aligned and capable of evading oversight too quickly to easily correct. Disanalogy between weak and strong systems: Less capable AI systems no longer serve as useful models for understanding or experimenting with more powerful ones. Deceptive misalignment and gradient hacking arise in weaker systems, but (unlike with level 6/7) the techniques which mitigate these issues in weak systems don’t work for more powerful systems. Illusory progress: Previous alignment techniques may become obsolete as AI systems exploit unforeseen loopholes or develop new deception strategies. This means that, institutionally, it’s very hard to make progress using the usual methods of science as incremental progress on aligning weaker systems never works on more powerful systems. \"First critical try\" dynamic: Unlike lower difficulty levels where iterative improvements are possible, higher levels offer only one chance to get alignment right . Misaligned systems at this level could resist further alignment attempts or cause irreversible harm. These factors combine to create a scenario where experimentation on weaker systems may not yield insights applicable to more powerful AIs, which necessitates a fundamentally different approach to alignment. Example:  Researchers at AGI Corp have spent years developing and refining their alignment techniques, successfully preventing gradient hacking and deceptive behaviour in a wide range of AI systems up to human-level intelligence. Confident in their methods, they apply these techniques to their most advanced AI project yet and deploy it. However, the system has still adopted a deceptive goal, and has simply developed even more sophisticated gradient hacking in response to the counter-gradient-hacking interventions. When given opportunities to take actions completely outside its testing environment, it pursues its true deceptive power-seeking goal. Dynamics of the Scale Having examined the specific challenges at each level of alignment difficulty, we now turn our attention to the broader forces at play across the spectrum. As we have seen in the previous section, the general dynamic is that more sophisticated, complex and diverse feedback is needed to ensure systems do what they’re supposed to do, as we move along the scale. However, this shift has many secondary effects, influencing factors such as: where research effort should be applied, which techniques might actually increase the risk from misaligned systems and how to verify alignment. We'll see how different perspectives on alignment difficulty shape research strategies and resource allocation. The graph illustrates various perspectives on AI alignment difficulty across the 10 levels we've described. Each coloured line represents a different view about the probability that a given technique will be sufficient. These 4 lines each represent a different example probability distribution function over whether a given level is sufficient to solve the alignment problem. As we move up the scale, techniques that are insufficient for true alignment may nevertheless advance AI capabilities and contribute to ‘training game’ or deception dynamics, as we saw in the discussion on Levels 4-7. This creates a tension where work on lower-level alignment techniques, if insufficient, could be negative in expected value terms at reducing AI risk: improving RLHf might hide alignment problems while just advancing capabilities. The higher the difficulty level, the more acute this tension becomes, as the potential for insufficient techniques to be detrimental increases. Therefore, Chris Olah's  original suggestion  to \"push the frontier of safety research\", and work first on the techniques that are slightly ahead of the current state of the art, before moving on to higher alignment difficulties, may not always be the optimal strategy. For those who believe the main probability mass lies far to the right (like the red or purple views in the graph), it might be more beneficial to focus on developing techniques for higher difficulty levels first, rather than incrementally improving lower-level methods. If lower-level methods will be perfected anyway (e.g. to ensure AI systems are useful in the short term) but those methods aren't sufficient, then the major effect of working on them could be to hide alignment failures without solving the overall problem, obscuring the the real alignment challenges. However, if the technique plausibly won’t be developed by default or if we need to master the lower level technique to work on the higher-level technique, then there is reason to work on it even if it isn’t sufficient to align powerful systems. The decision of whether to focus on incrementally improving lower-level techniques or investing in higher-level approaches depends critically on one's belief about the distribution of probability mass across the alignment difficulty scale. The green line, characteristic of the stated views of many AI labs, suggests alignment is likely to be relatively easy, with the highest probability at lower levels, but some probability that alignment is extremely challenging. This view aligns relatively well with the ‘advance the present margin of safety’ perspective, as it is most likely existing techniques are good enough, but we need to be aware of the possibility of more difficult alignment. On this view, it makes sense to deal first with the most likely challenges and then move onto higher difficulty levels later The yellow line shows a more spread-out distribution, indicating higher uncertainty and difficulty: this is more characteristic of the views of some labs, and many AI Evaluations and safety organizations. On this view, it probably makes sense to progressively advance the frontier of AI safety, but also to focus more on medium difficulty levels. It makes less sense on this view to focus on techniques that are already heavily invested like RLHF, but it’s not clear whether improving RLHF is net negative. The red line, exemplifying pessimistic views like many of those advocating for an AI pause, suggests alignment is likely to be extremely difficult or impossible. It’s probably not worth focusing on medium-level techniques as they may not scale, unless we see them as useful advance work for higher-level techniques, and plausibly advancing easy techniques like RLHF is net negative as they just accelerate AI capabilities and produce false security about alignment. The purple line represents a bimodal view held by some alignment researchers, suggesting two distinct peaks of probability. On this view, the strategic picture is more complex and hard to discern. Arguably advancing near-term techniques like RLHF is neutral or net positive, but also it makes sense to allocate resources to theoretical work on very hard alignment. Increasing Costs and Challenges As we progress to higher levels of AI alignment difficulty, the challenges in evaluating and achieving progress increase sharply. This is due not only to the growing complexity of the problems but also to the increasingly subtle nature of the behaviours we aim to elicit or prevent in AI systems. The cost of advancing alignment research rises faster at higher difficulty levels: much more effort and investment is required to produce the same amount of progress towards adequacy at level 7 than at level 3. This cost increases for several reasons. Most obviously, more resources, time, and effort are required to develop and implement these more sophisticated alignment techniques. But there are other reasons, such as that higher level failures cannot yet be experimentally demonstrated, so developing mitigations for them has to rely on (possibly unrepresentative) toy models instead of reacting to the failures of current systems. Paralleling this trend, the cost of verifying alignment—confidently determining whether a technique is working as intended or if a system is behaving deceptively—also increases exponentially. At lower levels, high scores on benchmarks of truthfulness or high refusal rates when asked to perform dangerous tasks are sufficient, while at higher levels highly realistic, long-term simulations of deployment along with interpretability probes may still provide false security. The near-parallel growth of both progress and verification costs creates a compounding effect. Each step forward in alignment research demands significantly more investment while simultaneously becoming harder to validate. This dynamic introduces greater uncertainty into the alignment process and should also influence decisions about where to spend research effort. Key Factors Changing Across the Scale In summary, we can see that as we progress through these levels, several key factors compound the difficulty: Increasing disconnect between alignment and capabilities research: At higher levels, alignment techniques may enhance AI's ability to mimic alignment while advancing its capabilities. This doesn't occur at lower levels. Measurement challenges: Higher alignment difficulty obscures genuine AI alignment, making it hard to distinguish safe systems from potentially dangerous ones, even with sophisticated testing. Institutional problems: Increasingly complex alignment may lead to premature deployment of unsafe AI systems, especially if testing provides a false sense of security. At high difficulty levels, even interpretability probes and deployment simulations might provide false security. Escalating costs: Alignment research resource requirements increase significantly with difficulty, making thorough efforts economically unfeasible for most institutions. The decision of which alignment techniques to invest in is therefore difficult and complex: at higher alignment difficulties, the cost of verifying alignment and making progress is much higher, but the risk of earlier techniques being counterproductive and actively making things worse means that these increased risks may be worth it.  Defining Alignment Difficulty We’ve explored how alignment challenges vary across the scale and the effects this has on factors like the cost of measurement and verification, the difficulty of making progress and whether testing provides safety assurances. However, we’ve been less clear about what exactly determines alignment difficulty or what, precisely, this scale is measuring: is it uncertainty about how AI architectures behave, or about the likelihood of deceptive behavior? How can we gain evidence about alignment difficulty? In order to address this question, we need to understand exactly what determines alignment difficulty. Defining alignment difficulty is a complex task, encompassing both practical and logical uncertainties. The high level definition, provided in the previous post, was: “The alignment problem” is the problem of aligning sufficiently powerful AI systems, such that we can be confident they will be able to reduce the risks posed by misused or unaligned AI systems The question of whether AI systems are actually used to do this effectively, (which is the separate, “ AI deployment problem ”), is a another problem which requires its own analysis. Overall AI Existential safety is achieved by solving the alignment problem and the deployment problem. Several things drop out of this definition. For a start, because we care about what powerful AI systems can do for us, the goal is ‘intent alignment’, i.e. we want the AI to be trying to do what we want it to do. More specifically, we want the AI to both  want to do  and  be able to do  certain High Impact Tasks (HITs): powerful transformative tasks which significantly alter the overall strategic landscape with respect to AI, reducing overall risks. High Impact Tasks Our aim is to produce protective transformative AI (pTAI) which performs high impact tasks that reduce the overall risk posed by AI. The HIT framing has some connection to the old  ‘pivotal act’  framing describing how to direct an aligned AGI, but the difference here is that HITs may consist of a wide range of tasks which each incrementally reduce AI risk, overall mostly eliminating it, rather than a single action taken by a single system. Some example HITs are: Improving cyber defense, to make a system proof against any attacker quickly and cheaply Being a trustworthy and superhuman predictor of the long term strategic consequences of political or corporate decisions Synthesizing transmissible vaccines to counteract bioweapons. Establishing techniques to drastically and quickly enhance human intelligence Safely synthesising self-sufficient fully autonomous replicating nanotechnology autonomously managing a large section of the economy and military through remote-controlling drones and factory robots These tasks vary widely in difficulty and in their overall impact: some are partial solutions that incrementally reduce risk and others are much more radical. There are also HITs that automate various aspects of AI alignment research, which inroduces further complexities. These could be tasks such as: Automating scalable oversight or RLHF research by quickly discovering new loss functions for training Evaluating the alignment of other systems through oversight Automating the ability to probe AI systems for deceptive power-seeking goals, via automated discovery of low-level features using interpretability tools The alignment problem then becomes: can humans and weaker AIs, performing a limited set of alignment-related HITs, produce AI systems capable of eliminating the risk posed by dangerous AI systems. Systems which are aligned, capable of performing HITs, and can be trusted to do so, we call  protective transformative  AI (pTAI). This approach aligns with the ‘defensive acceleration’ framing around AI progress discussed e.g.  here . Task Difficulty and Complexity of Feedback Suppose we assume: A particular architecture, e.g. a single, end-to-end-optimised LLM agent, much larger than GPT-4, with a ‘scratchpad’ and a self-prompting loop to plan over longer timescales. A desired set of high impact tasks (HITs) purportedly useful for reducing AI risk, consisting of both deployment environments (inputs to the AI system) and desired outcomes (e.g. automatically performing cyber defense superhumanly well) A given combination of training data and post-training feedback, external probes and additional research input as specified by the scale levels (e.g. oversight with interpretability, simple RLHF) Then the question of whether the AI system will perform these HITs as we desire, rather than specification-game, develop a deceptive goal, or be incapable of performing the task at all, becomes a matter of logical necessity. Because it is more restricted, this narrower question of how a given architecture will behave when given trainign data and directed to a task can be given a more technical foundation. One way of describing this is to ask, \"How complex or large does the training dataset need to be to ensure the system behaves as we want?\" In this context, the 'training dataset' encompasses a wide range of potential inputs and feedback mechanisms. At the most basic level, it includes human-provided feedback during training (Levels 2-3), but as we move up the alignment difficulty scale we need superhumanly complex feedback to have the system perform the task successfully. This could include feedback from highly realistic simulated deployment or superhuman AI-generated feedback from scalable oversight (Level 4-5). At the most advanced levels, the dataset might need to include feedback based on the AI's internal features (level 6+). This perspective on dataset complexity provides a concrete way to quantify the difficulty of alignment. We can imagine a curve where the x-axis represents the complexity or size of the training dataset, and the y-axis represents the capability level of the AI system. The steepness of this curve becomes a key factor in determining alignment difficulty. If the curve is relatively flat, it suggests that modest increases in feedback complexity can enable a system to perform very powerful tasks while maintaining alignment, and it is therefore more likely that we can rely on simpler techniques to get systems to perform the needed HITs. However, if the curve is steep, it indicates that ensuring alignment for more more difficult tasks requires disproportionately more complex feedback. Much of our uncertainty about how difficult alignment is arises from not knowing how difficult the necessary HITs are, therefore  Stephen McAleese has suggested  that we should see the alignment problem as a 2D scale like that above, where the task difficulty is plotted against the complexity of feedback required. However, we disagree, primarily because views about how difficult the required tasks are correlate closely with the views about how complex the feedback must be for a given task. In practice, we see views tend to fall into two camps: Very complex feedback (e.g. oversight with interpretability probes and mitigations against gradient hacking) is needed to ensure that a system actually performs a HIT as desired. Also, the HITs required to reduce overall risk are very difficult (e.g. successfully synthesizing self-replicating nanotechnology) Less complex feedback (e.g. RLHF combined with process based oversight) is needed to ensure that a system actually performs a HIT as desired. Also, the HITs required to reduce overall risk are not as difficult (e.g. automating cybersecurity and discovering new RLHF loss functions) This correlation occurs because the two are in fact closely related. The more difficult you believe overall AI alignment is, the more concerns you have about unaligned powerful AI systems being deployed by other actors, and therefore the more impactful and powerful HITs you believe are necessary to change the strategic situation. Therefore, while the correlation is not exact and depends other factors, we believe that it is best to run them together in the scale: higher alignment difficulty generally implies both that more powerful HITs are needed, and that it is harder to get an AI system to successfully perform a given HIT. Influence of Architecture Lastly, it is important to note that the path of AI development significantly influences the challenges we face. As we analyse the scale, keep in mind that our development choices actively shape the alignment landscape we navigate: perhaps CAI alone is sufficient to align a raw language model, but breaks down for an equivalently capable deep RL agent. Currently,  two primary approaches are emerging : language model (LM) agents built on chain-of-thought and decomposition, and larger end-to-end optimised models made up of single large LLMs or other foundation models, with added tools like access to external memory or scratchpads. The choice between these pathways may determine which alignment challenges become most pressing and at what level of difficulty we encounter them. LM agents made up of multiple connected systems, with their more transparent decision-making processes, might present different and potentially more manageable alignment challenges compared to black-box end-to-end optimized systems. A decade ago, approaches based around deep reinforcement learning (RL) systems augmented with search seemed like the most promising path to AGI: relative to bothof these architectures, pure RL agents seem less likely to be easy to align, as they more explicitly are end-to-end optimised to achieve a single, clearly specified goal. Especially if AGI is not very near, it is plausible that we could see some new, substantially different paradigms emerge, with different alignment challenges.  Conclusion As we've explored throughout this article, the AI alignment difficulty scale offers a comprehensive framework for understanding and addressing the challenges of aligning advanced AI systems with human values and intentions. From the relatively straightforward alignment scenarios at lower levels to the potentially insurmountable challenges at the highest levels, this scale provides a nuanced view of the road ahead. Key takeaways from our analysis include: What alignment failures look like : As we move up the scale, alignment challenges evolve from simple goal misgeneralization to sophisticated deception and potential gradient hacking, becoming harder to detect and requiring increasingly advanced solutions. We've seen what failures look like at each level with examples to illustrate. In the years ahead, we should be careful to watch for 'in the wild' emergence of behavior characteristic of the higher difficulty levels, such as deceptive misalignment or gradient hacking. The Alignment-Capability Tension : There's a growing disconnect between alignment and capabilities research at higher difficulty levels, highlighting the risk that well-intentioned research could inadvertently exacerbate safety concerns by advancing capabilities without commensurate progress in alignment. Given our uncertainty about alignment difficulty, the probability that an easy technique helps or is harmless must be weighed against the risk that it exacerbates problems. Exponential Cost Increase : Both the cost of progress and the difficulty of verification rise exponentially at higher levels of the scale, making progress more difficult and harder to measure, which counts against investing effort at higher difficulty levels. The Crucial Role of High Impact Tasks (HITs) : The concept of HITs provides a concrete way to think about what aligned AI systems need to achieve to significantly reduce overall AI risk. Architectural Influences : The path of AI development, including the choice between language model agents and end-to-end optimized models, significantly shapes the alignment challenges we face. Alignment difficulty is determined by architecture, required HITs, and how much feedback is required:  We can decompose how difficult the alignment problem is into: Which AI Architecture is used How difficult the required High Impact Tasks (HITs) are How much and how complex is the feedback required to ensure the architecture actually performs the desired HITs successfully Looking forward, this framework can serve as a tool for: Guiding research priorities and resource allocation in AI safety Facilitating more nuanced discussions about alignment strategies Informing policy decisions and governance frameworks for AI development"},
{"url": "https://www.alignmentforum.org/posts/Mf7wcRDoMAzbECfCw/can-startups-be-impactful-in-ai-safety", "title": ["Can startups be impactful in AI safety?"], "author": ["Esben Kran"], "date": ["2024-09-13T19:00:33.306Z"], "content": "With  Lakera's  strides in securing LLM APIs,  Goodfire AI's  path to scaling interpretability, and 20+  model   evaluations   startups  among  much   else , there's a rising number of technical startups attempting to secure the model ecosystem. Of course, they have varying levels of impact on superintelligence containment and security and even with these companies, there's a lot of potential for aligned, ambitious and high-impact startups within the ecosystem. This point isn't new and has been made in our  previous   posts  and by  Eric Ho  (Goodfire AI CEO).  To set the stage, our belief is that these are the types of companies that will have a positive impact: Startups with a profit incentive completely aligned with improving AI safety; that have a deep technical background to shape AGI deployment and; do not try to compete with AGI labs. Piloting AI safety startups To understand impactful technical AI safety startups better, Apart Research joined forces with collaborators from Juniper Ventures, vectorview (alumni from the latest YC cohort), Rudolf (from the upcoming def/acc cohort), Tangentic AI, and others. We then invited researchers, engineers, and students to resolve a key question  “can we come up with ideas that scale AI safety into impactful for-profits?” The hackathon took place during a weekend two weeks ago with a keynote by  Esben Kran  (co-director of Apart) along with ‘HackTalks’ by  Rudolf Laine  (def/acc) and  Lukas Petersson  (YC / vectorview). Individual submissions were a 4 page report with the problem statement, why this solution will work, what the key risks of said solution are, and any experiments or demonstrations of the solution the team made. This post details the top 6 projects and excludes 2 projects that were made private by request (hopefully turning into impactful startups now!). In total, we had 🙌 101 signups and 🏆 11 final entries. Winners were decided by an LME model conditioned on reviewer bias. Watch the authors' lightning talks   here . Dark Forest: Making the web more trustworthy with third-party content verification By Mustafa Yasir (AI for Cyber Defense Research Centre, Alan Turing Institute) Abstract: ‘DarkForest is a pioneering Human Content Verification System (HCVS) designed to safeguard the authenticity of online spaces in the face of increasing AI-generated content. By leveraging graph-based reinforcement learning and blockchain technology, DarkForest proposes a novel approach to safeguarding the authentic and humane web. We aim to become the vanguard in the arms race between AI-generated content and human-centric online spaces.’ Content verification workflow supported by graph-based RL agents deciding verifications Reviewer comments: Natalia: Well explained problem with clear need addressed. I love that you included the content creation process - although you don't explicitly address how you would attract content creators to use your platform over others in their process. Perhaps exploring what features of platforms drive creators to each might help you make a compelling case for using yours beyond the verification capabilities. I would have also liked to see more details on how the verification decision is made and how accurate this is on existing datasets. Nick: There's a lot of valuable stuff in here regarding content moderation and identity verification. I'd narrow it to one problem-solution pair (e.g.,  \"jobs to be done\" ) and focus more on risks around early product validation (deep interviews with a range of potential users and buyers regarding value) and go-to-market. It might also be worth checking out  Musubi . Read the full project here. Simulation Operators: An annotation operation for alignment of robot By Ardy Haroen (USC) Abstract: ‘We bet on agentic AI being integrated into other domains within the next few years: healthcare, manufacturing, automotive, etc., and the way it would be integrated is into cyber-physical systems, which are systems that integrate the computer brain into a physical receptor/actuator (e.g. robots). As the demand for cyber-physical agents increases, so would be the need to train and align them. We also bet on the scenario where frontier AI and robotics labs would not be able to handle all of the demands for training and aligning those agents, especially in specific domains, therefore leaving opportunities for other players to fulfill the requirements for training those agents: providing a dataset of scenarios to fine-tune the agents and providing the people to give feedback to the model for alignment. Furthermore, we also bet that human intervention would still be required to supervise deployed agents, as demanded by various regulations. Therefore leaving opportunities to develop supervision platforms which might highly differ between different industries.’ Esben:  Interesting proposal! I imagine the median fine-tuning capability for robotics at labs is significantly worse than LLMs. At the same time, it's also that much more important. I know OpenAI uses external contractors for much of their red-teaming work and it seems plausible that companies would bring in an organization like you describe to reduce the workload on internal staff. If I ran a robotics company however, I would count it as one of my key assets to be good at training that robot as well and you will probably have to show a significant performance increase to warrant your involvement in their work. I'm a fan of alignment companies spawning and robotics seems to become a pivotal field in the future. I also love the demand section outlining the various jobs it would support / displace. Natalia:  Really interesting business model! Good job incorporating new regulation into your thinking and thinking through the problem/proposed solution and its implications. However, I am curious about what would differentiate your proposed solution relative to existing RLHF service providers (even if currently mostly not applied to robotics). Read the full project here. AI Safety Collective: Crowdsourcing solutions to critical corporate AI safety challenges By Lye Jia Jun (Singapore Management University), Dhruba Patra (Indian Association for the Cultivation of Science), Philipp Blandfort Abstract:  'The AI Safety Collective is a global platform designed to enhance AI safety by crowdsourcing solutions to critical AI Safety challenges. As AI systems like large language models and multimodal systems become more prevalent, ensuring their safety is increasingly difficult. This platform will allow AI companies to post safety challenges, offering bounties for solutions. AI Safety experts and enthusiasts worldwide can contribute, earning rewards for their efforts. The project focuses initially on non-catastrophic risks to attract a wide range of participants, with plans to expand into more complex areas. Key risks, such as quality control and safety, will be managed through peer review and risk assessment. Overall, The AI Safety Collective aims to drive innovation, accountability, and collaboration in the field of AI safety.' Minh  (shortened) :  This is a great idea, similar to what I launched 2 years ago:  super-linear.org . Your assessments are mostly accurate. Initially, getting postings isn't too difficult - people often post informal prizes/bounties across EA channels. However, you'll quickly reach a cap due to the small size of the EA/alignment community and the even smaller subset of bounties. Small ad-hoc prizes don't scale well. Manifund's approach of crowdfunding larger AI Safety projects is more effective, as these attract more effort and activity. Feel free to contact me or Manifund for more information. While their focus differs slightly, many lessons would apply. Meanwhile, here's a list of  thousands of AI Safety/alignment bounty ideas I've compiled . Also check out Neel Nanda's  open mech interp problems doc . Esben:  Pretty great idea, echoing a lot of our thoughts from the  Apart Sprints  as well! There are still a few issues with a bounty platform like this that would need to be resolved, e.g. 1) which problems that require little work from me and can be given to others without revealing trade information would actually be high enough value to get solved while not causing security risks to my company and 2) how will this platform compete with places like AICrowd and Kaggle (I'd assume this would be mostly based on the AI safety focus). At the same time, there's some nice side benefits, such as the platform basically exposing predicted  incidents  for vulnerabilities at AI companies (though again, dependent on companies actually sharing this). Cool project and great to see many of your ideas in the live demo from your presentation! Read the full project here. Identity System for AIs: Re-tracing AI agent actions By Artem Grigor (UCL) Abstract:  'This project proposes a cryptographic system for assigning unique identities to AI models and verifying their outputs to ensure accountability and traceability. By leveraging these techniques, we address the risks of AI misuse and untraceable actions. Our solution aims to enhance AI safety and establish a foundation for transparent and responsible AI deployment.' Esben:  What a great project! This is a clear solution to a very big problem. It seems like the product would need both providers and output-dependent platforms to be in on the work here, creating an incentive to mark your models in the first place. I would also be curious to understand if a malicious actor wouldn't be able to simply forge the IDs and proofs for a specific output? In general, it would be nice to hear about more limitations of this method, simply due to my somewhat limited cryptography background. It would also be interesting to see exactly where it would be useful, e.g. how exactly can something like Facebook use it to combat fake news? Otherwise, super nice project and clearly showcases a technical solution to an obvious and big problem while remaining commercially viable as well. Read the full project here. Honorable mentions Besides the two projects that were kept private, several other projects deserve honorable mentions. We also suggest you check out the  rest of the projects . ÆLIGN: Aligned agent-based Workflows via collaboration & safety protocols ÆLIGN is a multi-agent orchestration tool for reliable / safe agent interaction. It specifies communication protocol designs and monitoring systems to standardize and control agent behavior. As Minh mentions, the report undersells the value of this idea and the  demo  is a good representation for this idea.  Read the full project here. WELMA: Open-world environments for Language Model agents WELMA details a sandbox tool to build environments for testing language models and takes much of the challenges paradigm of METR and AISI's evals and thinks about how it might look at the next stage.  Read the full project here. What evidence is this for impactful AI safety startups? So, was our pilot for AI safety startups successful? We knew beforehand that the startup ideas wouldn't be a be-all-end-all solution to a specific problem but that safety and security is a complex question where a single problem (e.g. the chance of escape from rogue superintelligences) can give a single company work for years.  With that said, we're cautiously optimistic about the impact companies might have during the next years in AI safety. This update we see from both participants and our networks is relatively well summarized by our participant Mustafa Yasir (AI for Cyber Defense Research Centre): “[The AI safety startups hackathon] completely changed my idea of what working on 'AI Safety' means, especially from a for-profit entrepreneurial perspective. I went in with very little idea of how a startup can be a means to tackle AI Safety and left with incredibly exciting ideas to work on. This is the first hackathon in which I've kept thinking about my idea, even after the hackathon ended.” And we can only agree. If you are interested in joining future hackathons, find the schedule  here . Thank you to Nick Fitz, Rudolf Laine, Lukas Petersson, Fazl Barez, Jonas Vollmer, Minh Nguyen, Archana Vaidheesvaran, Natalia Pérez-Campanero Antolin, Finn Metz, and Jason Hoelscher-Obermaier for making this event a success."},
{"url": "https://www.alignmentforum.org/posts/xj5nzResmDZDqLuLo/estimating-tail-risk-in-neural-networks", "title": ["Estimating Tail Risk in Neural Networks"], "author": ["Mark Xu"], "date": ["2024-09-13T20:00:06.921Z"], "content": "Machine learning systems are typically trained to maximize average-case performance. However, this method of training can fail to meaningfully control the probability of tail events that might cause significant harm. For instance, while an artificial intelligence (AI) assistant may be generally safe, it would be catastrophic if it ever suggested an action that resulted in unnecessary large-scale harm. \n Current techniques for estimating the probability of tail events are based on finding inputs on which an AI behaves catastrophically. Since the input space is so large, it might be prohibitive to search through it thoroughly enough to detect all potential catastrophic behavior. As a result, these techniques cannot be used to produce AI systems that we are confident will never behave catastrophically. \n We are excited about techniques to estimate the probability of tail events that do not rely on finding inputs on which an AI behaves badly, and can thus detect a broader range of catastrophic behavior. We think developing such techniques is an exciting problem to work on to reduce the risk posed by advanced AI systems: \n \n Estimating tail risk is a conceptually straightforward problem with relatively objective success criteria; we are predicting something mathematically well-defined, unlike instances of  eliciting latent knowledge  (ELK) where we are predicting an informal concept like \"diamond\". \n Improved methods for estimating tail risk could reduce risk from a variety of sources, including central misalignment risks like deceptive alignment. \n Improvements to current methods can be found both by doing empirical research, or by thinking about the problem from a theoretical angle. \n \n This document will discuss the problem of estimating the probability of tail events and explore estimation strategies that do not rely on finding inputs on which an AI behaves badly. In particular, we will: \n \n Introduce a toy scenario about an AI engineering assistant for which we want to estimate the probability of a catastrophic tail event. \n Explain some deficiencies of adversarial training, the most common method for reducing risk in contemporary AI systems. \n Discuss deceptive alignment as a particularly dangerous case in which adversarial training might fail. \n Present methods for estimating the probability of tail events in neural network behavior that do not rely on evaluating behavior on concrete inputs. \n Conclude with a discussion of why we are excited about work aimed at improving estimates of the probability of tail events. \n \n This document describes joint research done with Jacob Hilton, Victor Lecomte, David Matolcsi, Eric Neyman, Thomas Read, George Robinson, and Gabe Wu. Thanks additionally to Ajeya Cotra, Lukas Finnveden, and Erik Jenner for helpful comments and suggestions. \n A Toy Scenario \n Consider a powerful AI engineering assistant. Write  M .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n  for this AI system, and  M ( x )  for the action it suggests given some project description  x . \n We want to use this system to help with various engineering projects, but would like it to never suggest an action that results in large-scale harm, e.g. creating a  doomsday device . In general, we define a behavior as  catastrophic  if it must never occur in the real world. [1]  An input is  catastrophic  if it would lead to catastrophic behavior. \n Assume we can construct a catastrophe detector  C  that tells us if an action  M ( x )  will result in large-scale harm. For the purposes of this example, we will assume both that  C  has a reasonable chance of catching all catastrophes and that it is feasible to find a useful engineering assistant  M  that never triggers  C  (see  Catastrophe Detectors  for further discussion). We will also assume we can use  C  to train  M , but that it is prohibitively expensive to use  C  to filter all of  M 's outputs after  M  is trained. [2] \n We are interested in estimating the probability that our model  M  behaves catastrophically on a particular distribution of inputs [3] :  P x ∼ D ( C ( M ( x ) ) ) \n Deficiencies of Adversarial Training \n To reduce the risk of our AI system behaving catastrophically, we could use  adversarial training : having a red team attempt to find catastrophic inputs and training  M  until those inputs are no longer catastrophic. \n However, such systems can still behave catastrophically in a range of possible scenarios: \n \n The users of an AI system might explore inputs not considered during the adversarial training process and uncover novel catastrophic inputs. \n Users of a system might specifically search for inputs designed to elicit catastrophic behavior, using more resources than the adversarial training process. [4] \n An intelligent AI system may attempt to purposefully avoid acting catastrophically when it suspects it is being trained. \n \n As catastrophic behavior becomes extremely rare, it can be computationally prohibitive for the red team to explore the input space thoroughly enough to find even a single catastrophic input. To determine which AI systems robustly avoid catastrophic behavior, we must be able to estimate the frequency of catastrophic inputs even when they are so rare that we cannot find any such inputs. \n Rather than viewing a red team as searching for particular inputs, we can view the red team as defining a process  D  for producing inputs. We can then ask the question: how likely is the red team to find an input on which the system behaves catastrophically? Equivalently, what is  P D ( C ( M ( x ) ) ) ? Even if a red team is usually unable to find concrete inputs on which a system behaves catastrophically, the process they use could still have a small probability of success. \n The possibility of distributional shift complicates the way in which estimates of the rate of catastrophic behavior can be used to mitigate such behavior, which we discuss in  Distributional Shift . \n Deliberate Subversion of Adversarial Training \n Malign intelligent AI systems might have incentives to pretend to be benign during training so they can cause harm later. Call a system \"deceptively aligned\" if it is a malign system pretending to be benign. [5]  For example, a deceptively aligned engineering assistant might intentionally avoid suggesting an action that creates a doomsday device during training, but seek such opportunities when they would cause real-world harm. A sufficiently advanced system might be able to maintain this deception even through extensive adversarial training. [6] \n Even if a deceptively aligned model has extremely high accuracy at determining whether it is being adversarially trained, it will likely have some probability of making a mistake. If a deceptive system makes mistakes very rarely, then adversarial training would likely fail to find an input that distinguishes it from a benign model. However, the possibility of these mistakes would result in the deceptive model having a higher probability of behaving catastrophically than its benign counterpart, even if the probability is small in absolute terms (we discuss ways in which this might not be true in  Catastrophe Detectors . By estimating the probability of catastrophic behavior, we would be able to distinguish between benign and deceptive systems. \n Importantly, our estimates do not need to be perfectly accurate to detect deceptive systems. Catastrophic actions are highly complex, so AI systems cannot randomly stumble into catastrophic actions. Deceptive systems behave catastrophically at much higher rates than their benign counterparts because they sometimes deliberately decide to take catastrophic action. To distinguish between benign and deceptive systems, we would not need to characterize the chance a benign model accidentally acts catastrophically; we would only need to notice that there are some conditions under which a model might deliberately act catastrophically. \n A Possible Approach for Estimating Tail Risk \n This section an approach for estimating the probability of catastrophic behavior of a neural network. We present several simple methods that represent our overall approach. We are optimistic that this kind of approach can ultimately deliver better risk estimates than can be obtained from conventional red teaming, but in experiments so far we have not surpassed baselines based on importance sampling. [7] \n Consider a neural network  M : X → Y , a distribution  D  over  X , and a binary event  C : Y → { 0 , 1 } . Our goal is to estimate  P D ( C ( M ( x ) ) ) . [8]  For our engineering research assistant example above,  X  is a project description,  Y  is a suggested action, and  C  is our catastrophe detector. \n If  P D ( C ( M ( x ) ) )  is very low, then even billions of samples might not contain an  x  such that  C ( M ( x ) ) = 1 . However, it might still be possible to estimate  P D ( C ( M ( x ) ) )  by identifying structure that suggests  M   may  behave catastrophically on  some  inputs. For example, suppose  C  embeds an action  M ( x )  into some latent space and flags it as catastrophic if it is \"too large\" in 20 specific directions simultaneously. For each of these directions, we could attempt to identify features in  M  that would result in the embedding of  M ( x )  being large in that direction. If each such feature were active with probability  1 100 , then we could estimate the chance that  M ( x )  is too large in all 20 directions simultaneously as  ( 1 100 ) 20 = 10 − 40 . \n Our goal is to develop methods that can reliably detect significant risks of catastrophic behavior by identifying such structure. \n Layer-by-layer Activation Modeling \n \nWe will present one possible approach to producing estimates of tail events in neural networks based on modeling the distribution of each layer of activations in the network. This approach illustrates one way in which a mechanistic analysis of a model could improve estimates of tail events. However, this particular framework also has a few fundamental flaws, which we discuss in  Issues with Layer-by-layer Activation Modeling . \n We will assume that  C  is also a neural network and express the composition of  C  and  M  as a single function  C ∘ M : X → { 0 , 1 } . This composition is a single function which is  1  if and only if  x  is a catastrophic input for  M . Since  C ∘ M  is itself just a larger neural network, we can express it as the composition of  n  functions  f 0 , f 1 , . . . , f n − 1 . Each  f i  represents a transition between layers in our model, such as a linear transformation followed by a ReLU activation. We will write  X i  for the domain of  f i , which is typically equal to  R k  for some  k . More specifically, for input  x  define: \n \n x 0 : = x ∈ X 0 \n x i + 1 : = f i ( x i ) ∈ X i + 1 \n C ( M ( x ) ) : = x n ∈ X n = { 0 , 1 } \n \n Our input distribution  D  is a distribution over  X 0 . Through the composition of the transition functions  f i ,  D  also induces a distribution over  X 1 , X 2 , . . . , X n . Our general method aims to estimate  P D ( C ( M ( x ) ) )  by approximation these induced distributions over  X i  as they flow through the network, from input to output. Each implementation of this method will have two key components: \n \n For each layer  i , we will have a class of distributions  P i  over  X i  that we will use to model the activations at each layer. Formally,  P i ⊆ Δ X i . \n A method to update our model as we progress through layers: given a model  P i ∈ P i  for layer  i , we need to determine  P i + 1 ∈ P i + 1  for layer  i + 1 . Formally, this method is a collection of  n − 1  functions from  P i → P i + 1 , one for each  i = 0 , . . . , n − 1 . \n \n With these components in place, we can estimate  P D ( C ( M ( x ) ) )  for any  D ∈ P 0  as follows: \n \n We begin with  D = P 0 ∈ P 0 , representing the input distribution. \n We apply our update method repeatedly, generating  P 1 ∈ P 1 ,  P 2 ∈ P 2 , up to  P n ∈ P n . \n Our estimate of  P D ( C ( M ( x ) ) )  is the probability that  P n  assigns to the outcome 1. \n \n Toy Example: Finitely Supported Distributions \n If  D  was finitely supported, then it would be trivial to estimate the probability of catastrophe on  D , but we can use this example to illustrate some general principles. Let all  P i  be the class of finitely supported distributions over the associated spaces  X i . \n Given a finitely supported distribution  D = P 0 , we can apply  f 1  to each datapoint to generate the empirical distribution  P 1 , which will be the exact distribution of  x 1 . By repeating this process for all layers, we eventually obtain  P n . The probability  P n  assigns to  1  will be the exact frequency of catastrophe on  D . \n This calculation is not helpful for adversarial training; if we cannot find any inputs where a catastrophe occurs, then we also cannot find any finitely supported distribution  D  with non-zero probability of catastrophe. Instead, we would like to allow a red team to define a broader distribution that puts positive (although potentially very small) probability on catastrophic inputs. \n Method 1: Gaussian Distribution \n \nTo move beyond empirical evaluations, we can approximate the distributions over activations by  multivariate Gaussians. Let  P i  be the class of all multivariate Gaussians over the activations  X i . Write a normal distribution with mean vector  μ  and covariance matrix  Σ  as  N ( μ , Σ ) . \n To specify an algorithm, we need to a method for choosing  P i + 1  given  P i . In this case, we want to choose the multivariate Gaussian  N ( μ i + 1 , Σ i + 1 )  that best approximates  f i ( x i )  where  x i ∼ N ( μ i , Σ i ) . A non-linear function distribution is typically no longer Gaussian, so perfect modeling is impossible. Instead, we can use various methods to select  N ( μ i + 1 , Σ i + 1 )  based on different notions of approximation quality. [9] \n A standard notion of approximation quality is the Kullback-Leibler (KL) divergence between  f i ( N ( μ i , Σ i ) )  and  N ( μ i + 1 , Σ i + 1 ) . By a well-known \"moment matching\" theorem of Gaussian distributions, we can minimize  KL ( f i ( N ( μ i , Σ i ) ) | | N ( μ i + 1 , Σ i + 1 ) )  by setting  μ i + 1  and  Σ i + 1  to the mean vector and covariance matrix of  f i ( N ( μ i , Σ i ) ) . [10] \n This Gaussian approximation allows us to move beyond adversarial training on concrete catastrophic inputs. Instead, we can pick  μ 0  and  Σ 0  to maximize  P N ( μ 0 , Σ 0 ) ( C ( M ( x ) ) ) , then train  M  to minimize  P N ( μ 0 , Σ 0 ) ( C ( M ( x ) ) ) , hopefully capturing a broader range of catastrophic scenarios. \n Method 2: Independent Linear Features \n \nSome authors have attempted to extract meaningful features of a model's computation using sparse auto-encoders (SAEs), which find an overcomplete basis in which the activations are typically sparse. SAEs are typically learned by minimizing an empirical estimate of a reconstruction loss. However, if catastrophic inputs are rare, the SAE's training data is unlikely to contain any catastrophic inputs. As such, we might not learn features that are informative for estimating the probability of catastrophic inputs. \n We will describe a modified SAE for which we can analytically calculate the loss over a distribution that includes catastrophic inputs, instead of empirically estimating it with training data. The details of this algorithm can be found  here , which we summarize below. \n Define a given distribution  P i ∈ P i  by specifying: \n \n m  feature vectors  v 1 , . . . , v m ∈ X i . \n For each feature  v j , a distribution  Q j  over  R  representing how strongly that feature activates. \n \n To generate a single sample from this distribution, we: \n \n Sample feature weights  a j ∼ Q j  independently. \n Compute the weighted sum of these features  Σ j a j v j \n \n SAEs are typically learned independently for different layers of a neural network. By contrast, our algorithm will use the features from a layer  X i  to learn the features for the next layer  X i + 1 . More precisely, we will assume that the activations at layer  i  are distributed according to  P i , and then find the distribution  P i + 1  that minimizes  KL ( f i ( P i ) | | P i + 1 ) ) . \n We show that we can analytically compute an bound for this KL divergence by using variational inference. [11]  We can then train to  P i + 1  to minimize this lower bound; the result is a variational autoencoder trained with an objective very similar to the conventional objective for a sparse autoencoder. More details of this bound and an algorithm for calculating it efficiently can be found  here . \n Because we compute bounds analytically, in principle they can be very sensitive to rare events that would never be detected by sampling. However, because KL divergence is defined as an expectation, this loss function may still incentivize the learning of features that are relevant to the typical behavior of the model and not informative about the tails. For example, a \"lethality\" feature might be informative about the probability an AI system behaves catastrophically, but almost useless for modeling behavior on typical inputs. To overcome this problem, we would like to replace the KL divergence with a loss function that is more sensitive to distributional differences that affect catastrophe probability. Precisely defining this loss function is one of the major open questions for this approach. \n Relation to Other Work \n Formalizing the Presumption of Independence \n Formalizing the Presumption of Independence  studies the problem of estimating the expectation of a circuit through the lens of heuristic estimation. The general framework described above is a particular family of heuristic estimation methods based on modeling the activations of successive layers of a neural network. \n Many of the methods we have described are inspired by algorithms for heuristic estimation. Most directly,  Method 1 Gaussian Distribution  is exactly the covariance propagation algorithm described in appendix D.3. Additionally,  Method 2 Independent Linear Features  can be thought of as finding a basis for which the presumption of independence approximately applies. \n For more examples of methods for heuristic estimation that can potentially be translated into techniques for estimating tail risk in neural networks, see Appendix A of  Formalizing the Presumption of Independence . \n Mechanistic Interpretability \n Mechanistic interpretability  is a field of research that aims to understand the inner workings of neural networks. We think such research represents a plausible path towards high-quality estimates of tail risk in neural networks, and many of our estimation methods are inspired by work in mechanistic interpretability. \n For example, a mechanistic understanding of a neural network might allow us to identify a set of patterns whose simultaneous activation implies catastrophic behavior. We can then attempt to estimate the probability that all features are simultaneously active by using experiments to collect local data and generalizing it with the presumption of independence. \n We also hope that estimating tail risk will require the development of methods for identifying interesting structure in neural networks. If so, then directly estimating tail risk in neural networks might lead to greater mechanistic understanding of how those neural networks behave and potentially automate portions of mechanistic interpretability research. \n Relaxed Adversarial Training \n Relaxed Adversarial Training  (RAT) is a high-level proposal to overcome deficiencies in adversarial training by \"relaxing\" the problem of finding a catastrophic input. We expect our methods for estimating  P D ( C ( M ( x ) ) )  to be instances of the relaxations required for RAT. \n Eliciting Latent Knowledge \n In  Eliciting Latent Knowledge  (ELK), we describe a SmartVault AI trained to take actions so a diamond appears to remain in the room. We are concerned that if the sensors are tampered with, the diamond can be stolen while still appearing safe. \n \n A key difficulty in ELK is the lack of sophisticated tampering attempts on which we can train our model to protect the sensors. In the ELK document, we describe some ways of training models that we hoped would generalize in desirable ways during tampering attempts, but ultimately concluded these methods would not always result in the desired generalization behavior. \n Instead of trying to indirectly control generalization, we can attempt to directly measure the quality of generalization by estimating the probability of tampering. We will not have a perfect tampering detector, but even if a robber (or our AI system itself) was skilled at tampering, they might get caught one-in-a-trillion times. Thus, by estimating and minimizing the probability of detectable tampering, we might be able to produce a SmartVault that defends the sensors even with no examples of sophisticated sensor tampering. \n More generally, we believe there are deeper connections between methods for estimating tail risk and ELK, which we might explore in a later post. \n Conclusion \n Contemporary methods for reducing risk from AI systems rely on finding concrete catastrophic inputs. As AI systems become more capable, such methods might not be able to reduce risk down to acceptable levels. \n In this document, we have argued that it is both useful and possible to develop approaches for estimating tail risk in neural networks that does not rely on finding concrete catastrophic inputs. We are currently pursuing this problem from both theoretical and empirical angles, and are excited to see others do the same. \n We have given a high-level overview of our motivation and approach. In the appendices, we explore various details in more depth: \n \n In  Catastrophe Detectors , we provide a default picture of how to construct a catastrophe detector, and discuss its limitations. We also briefly describe our hope for finding a robust catastrophe detector. \n In  Issues with Layer-by-layer Activation Modeling , we overview two major problems with the general estimation framework we discuss in the body, and potential routes for overcoming them. \n In  Distributional Shift , we discuss how the possibility of distributional shift complicates the way in which estimates of the rate of catastrophic behavior can be used to mitigate such behavior. \n In  Complexity Theoretic Barriers to Accurate Estimates , we discuss why computational barriers to accurate estimates might not prevent us from obtaining estimates \"good enough\" to mitigate catastrophic behavior. \n \n Appendices \n Catastrophe Detectors \n \nIn the body, we assume that we can construct a catastrophe detector  C  that has the following properties: \n \n C  has a reasonable chance of catching everything output by  M  we would consider a catastrophe. \n It is possible to find a useful  M  that almost never triggers  C . \n \n However, we do not currently know how to construct a catastrophe detector with those properties. In this appendix, we will describe the a default picture for how to attempt such a construction, discuss its potential flaws, and briefly overview our hope for why these flaws are surmountable. \n Our default picture for how we would attempt to construct such a catastrophe detector is to use a weaker, trusted AI system  M w e a k  to examine the suggested action of a stronger system  M . We could give our weaker system multiple advantages to give it the best chance of catching catastrophic actions: \n \n We could give  M w e a k  more time to think than  M , so it could examine each portion of the action in detail. \n We could allow  M w e a k  to interrogate  M  about the function of various parts of its action, and do detailed checks for inconsistencies in  M 's answers. \n We could allow  M w e a k  to perform separate experiments to verify  M 's claims about its actions, and to evaluate whether or not the actions have any side effects. \n M w e a k  could be conservative when evaluating  M 's action, flagging part of the action it did not understand as potentially catastrophic. \n \n However, even given these advantages, our catastrophe detector  C  might not have our desired properties: \n \n If  M  is significantly smarter than  M w e a k , it might be possible for  M  to suggest a catastrophic action that  M w e a k  will not recognize as such. If  M w e a k  merely has a low probability of detecting that such actions are catastrophic, then our estimates will be (potentially poor) approximations to the true probability that  M  acts catastrophically, but we will still be able to distinguish between deceptive and benign models. If, however, there are catastrophic actions that  M w e a k  has no chance of detecting, and  M  knows that  M w e a k  will not be able to detect them, then a deceptive  M  could act catastrophically while appearing benign. \n There potentially exist actions that  M w e a k  will mistakenly think are catastrophic, and inputs that cause even benign models to produce those actions. For example, contemporary image classifiers are often vulnerable to  adversarial attacks  that can result in arbitrary misclassification. There might similarly be actions that induce misclassification in  M w e a k . If such actions are more common than the probability deceptive models act mistakenly act catastrophically during training, then we would be unable to distinguish between benign and deceptive models. \n \n In the body of this document, we discuss methods for estimating the chance that an AI system violates some specification of catastrophe. We think it is more likely that we fail to find such methods, than that we fail to find a specification of catastrophe that would be valuable to estimate. As such, we have not prioritized research into how we could construct more robust catastrophe specifications. \n Our current plan for how to produce such specifications involves increasing the robustness of an imperfect catastrophe detectors using  mechanistic anomaly detection . Roughly speaking, we think it should be possible to distinguish between two different \"reasons\" why an action looked benign: \n \n It looked benign because it was benign \n It looked benign because  M  carefully selected the action to  appear  benign. \nThis bears resemblance to our plan for how mechanistic anomaly detection will help with ELK, which we discuss in  Finding gliders in the game of life . \n \n Issues with Layer-by-layer Activation Modeling \n \nIn the body, we present a framework for producing estimates of tail events in a neural network by successively modeling layers of activations. We present this approach because it is easy to communicate and reason about, while still containing the core challenge of producing such estimates. However, we believe the framework to be fundamentally flawed for two reasons: \n \n Layer-by-layer modeling forgets too much information \n Probability distributions over activations are too restrictive \n \n Layer-by-layer modeling forgets too much information \n Thanks to Thomas Read for the example used in this section. \n When modeling successive layers of activations, we are implicitly forgetting how any particular piece of information was computed. This can result in missing large correlations between activations that are computed in the same way in successive layers. \n For example, suppose  h  is a pseudorandom boolean function that is  1  on 50% of inputs. Let  x  be distributed according to some simple distribution  D . Define the following simple 2 layer neural network: \n \n f 1 : x ↦ ( x , h ( x ) ) \n f 2 : ( x , y ) ↦ ( 1  iff  h ( x ) = = y ) \n \n Since  h ( x ) = = h ( x )  definitionally, this network always outputs  1 . However, layer-by-layer activation modeling will give a very poor estimate. \n If  h ( x )  is complex enough, then our activation model will not be able to understand the relationship between  h ( x )  and  x , and be forced to treat  h ( x )  as independent from  x . So at layer 1, we will model the distribution of activations as  ( D , B e r n [ 0.5 ] ) , where  B e r n [ 0.5 ]  is 1 with 50% chance and 0 otherwise. Then, for layer 2, we will treat  h ( x )  and  y  as independent coin flips, which are equal with 50% chance. So we will estimate  P D ( f 2 ( f 1 ( x ) ) )  as  0.5 , when it is actually 1. \n In general, layer-by-layer activation modeling makes an approximation step at each layer, and implicitly assumes the approximation errors between layers are uncorrelated. However, if layers manipulate information in correlated ways, then approximation errors can also be correlated across layers. \n In this case, we hope to be able to notice that  f 1  and  f 2  are performing similar computations, and so to realize that the computation done by layer 1 and layer 2 both depend on the value of  h ( x ) . Then, we can model the value of  h ( x )  as an independent coin flip, and obtain the correct estimate for  P D ( f 2 ( f 1 ( x ) ) ) . This suggests that we must model the entire distribution of activations simultaneously, instead of modeling each individual layer. \n Probability distributions over activations are too restrictive \n Thanks to Eric Neyman for the example used in this section. \n If we model the entire distribution over activations of  M , then we must do one of two things: \n \n Either, the model must only put positive probability on activations which could have been realized by some input \n Or, the model must put positive probability on an inconsistent set of activations. \n \n Every set of activations actually produced by  M  is consistent with some input. If we performed consistency checks on  M , then we would find that every set of activations was always consistent in this way, and the consistency checks would always pass. \n If, however, our approximate distribution over  M 's activations placed positive probability on inconsistent activations, then we would incorrectly estimate the consistency checks as having some chance of failing. In these cases, our estimates could be arbitrarily disconnected from the actual functioning of  M . So it seems we must strive to put only positive probability on consistent sets of activations. \n Only placing positive probability on consistent sets of activations means that our distribution over activations corresponds to some input distribution. This means that our catastrophe estimate will be exact over some input distribution. Unfortunately, this implies our catastrophe estimates will be often be quite poor. For example, suppose we had a distribution over boolean functions with the following properties: \n \n a randomly sampled function  f  is 50% likely to be the constant 0 function, and 50% likely to have a single input where  f ( x ) = 1 \n it is computationally hard to tell if  f  is the all 0 function \nFor example, an appropriately chosen distribution over 3- CNF s likely has these properties. \n \n If our input space was of size  N , then it seems reasonable to estimate a  P ( f )  as  1 2 N  for randomly sampled  f . However, since our catastrophe estimate is exact (over some input distribution), it can be non-zero only if  f  is not the all 0 function. By assumption, it is computationally hard to tell if  f  is not the all 0 function, so we must generally estimate  P ( f )  as 0, making it impossible to be \"reasonable\". \n In general, the requirement that our estimate be derived from a logically consistent distribution means we cannot reliably put positive probability on all \"reasonable\" possibilities. If we wish to be able to produce estimates like  1 2 N , then we must be able to represent logically inconsistent possibilities. However, the possibility of models that do consistency checks means we must place almost no probability on any  particular  logical inconsistency. \n This line of reasoning suggests an overall picture where our estimates are not attached to any particular probability distribution over activations, but rather one where our estimates are derived directly from high-level properties of the distribution. For example, we might instead represent only the moments of the distribution over activations, and our estimate might come from an inconsistent set of moments that cannot come from any possible distribution (but we would not know of any particular inconsistency that was violated). [12] \n Distributional Shift \n \nIn  Layer-by-layer Activation Modeling , we describe a few methods for estimating the probability of a tail event on a fixed input distribution. However, due to distributional shift, these estimates might be highly uninformative about the true probability our AI systems will act catastrophically \"in practice\". This problem is not unique to our approach; practitioners of adversarial training also desire to find catastrophic inputs likely to appear \"in practice\", which might be very different from inputs used to train the AI system. \n We have a few rough ideas for constructing distributions that place non-trivial probability on realistic catastrophic inputs: \n \n Adding noise to the training distribution produces a wider distribution that is more likely to contain a catastrophic input. In the limit, the uniform distribution over inputs is guaranteed to contain catastrophic inputs should they exist. However, these catastrophic inputs might be highly unrealistic, and represent a small fraction of the catastrophic inputs likely to be encountered \"in practice\". That being said, lowering the probability an AI system acts catastrophically on random inputs might still make it less likely to act catastrophically in practice. \n Adversarial training teams might employ various techniques to produce more realistic catastrophic inputs. For instance, they might use other AI systems to modify training inputs, or attempt to directly synthesize catastrophic inputs. Such a strategy is a function from random noise to particular inputs, and thus can also be thought of as distributions over inputs. Instead of sampling from that distribution and hoping it places high enough probability on catastrophic inputs that we will sample one, we can attempt to directly estimate that probability. \n \n Ultimately, however, these mitigations will not detect all instances of deceptive alignment. For example, suppose a deceptively aligned system knows it will encounter a particular input  x  that is a good opportunity for acting catastrophically. If the adversarial training team does not know about  x , or merely cannot synthesize  x , then they will be unable to determine if the system will behave catastrophically. [13] \n In these cases, we hope to detect deceptive alignment by understanding the input distribution \"as well as the AI system\". Any time the AI system knows that it will encounter some future catastrophic input, we need to incorporate this knowledge into our estimate. This is very related to ELK, and is discussed further in  Main hope estimates competitive with the AI system or the training process . \n Complexity Theoretic Barriers to Accurate Estimates \n \nIt is at least NP-hard to accurately estimate the expectation of a function computable in polynomial time. As such, it is likely infeasible to develop methods for  accurately  estimating tail events. However, we believe that it is feasible to develop methods that can estimate tail risks accurately  enough  to detect risks from AI systems deliberately acting catastrophically. \n In this appendix, we will first work an example involving 3-SAT to demonstrate some estimation methods that can be applied to problems widely believed to be computationally infeasible to even approximate. Then, we will discuss how we hope to obtain estimates accurate enough to detect risk from AI systems deliberately acting catastrophically by obtaining estimates that are competitive with the AI system or the process used to train it. \n 3-SAT \n A boolean formula is called a 3-CNF if it is formed of an AND of clauses, where each clause is an OR of 3 or less literals. For example, this is a 3-CNF with 3 clauses over 5 variables: \n ( x 1 ∨ ¬ x 3 ∨ x 4 ) ∧ ( ¬ x 2 ∨ x 3 ∨ ¬ x 5 ) ∧ ( x 2 ∨ ¬ x 4 ∨ x 5 ) \n We say a given setting of the variables  x i  to  True  or  False   satisfies  the 3-CNF it it makes all the clauses  True . 3-SAT is the problem of determining whether there is  any  satisfying assignment for a 3-CNF. #3-SAT is the problem of determining the  number  of satisfying assignments for a 3-CNF. Since the number of assignments for a 3-CNF is  2 number of variables , #3-SAT is equivalent to computing the probability a random assignment is satisfying. The above 3-CNF has 20 satisfying assignments and  2 5 = 32  possible assignments, giving a satisfaction probability of  5 8 . \n It's widely believed that 3-SAT is computationally hard in the worst case, and #3-SAT is computationally hard to even approximate. However, analyzing the structure of a 3-CNF can allow for reasonable best guess estimates of the number of satisfying assignments. \n In the following sections,  F  is a 3-CNF with  C  clauses over  n  variables. We will write  P ( F )  for the probability a random assignment is satisfying, can be easily computed from the number of satisfying assignments by dividing by  2 n . \n Method 1: Assume clauses are independent \n Using brute enumeration over at most 8 possibilities, we can calculate the probability that a clause is satisfied under a random assignment. For clauses involving 3 distinct variables, this will be  7 8 . \n If we assume the satisfaction of each clause is independent, then we can estimate  P ( F )  by multiplying the satisfaction probabilities of each clause. If all the clauses involve distinct variables, this will be  ( 7 8 ) C . We will call this this  naive acceptance probability  of  F . \n Method 2: Condition on the number of true variables \n We say the sign of  x i  is positive, and the sign of  ¬ x i  is negative. If  F  has a bias in the sign of its literals, then two random clauses are more likely to share literals of the same sign, and thus be more likely to be satisfied on the same assignment. Our independence assumption in method 1 fails to account for this possibility, and thus will underestimate  P ( F )  in the case where  F  has biased literals. \n We can account for this structure by assuming the clauses of  F  are satisfied independently  conditional  on the number of variables assigned  true . Instead of computing the probability each clause is  true  under a random assignment, we can compute the probability under a random assignment where  m  out of  n  variables is  true . For example, the clause  ( x 1 ∨ ¬ x 3 ∨ x 4 )  will be true on  ( n − 3 m − 2 )  possible assignments out of  ( n m ) , for a satisfaction probability of  ( n − 3 m − 2 ) / ( n m ) = m ( m − 1 ) ( n − m ) n ( n − 1 ) ( n − 2 ) ≈ m n ( 1 − m n ) m n , where the latter is the satisfaction probability if each variable was  true  with independent  m n  chance. Multiplying together the satisfaction probabilities of each clause gives us an estimate of  P ( F )  for a random assignment where  m  out of  n  variables are  true . \n To obtain a final estimate, we take a sum over satisfaction probabilities conditional on  m  weighted by  ( n m ) 2 n , the chance that  m  variables are  true . \n Method 3: Linear regression \n Our previous estimate accounted for possible average unbalance in the sign of literals. However, even a  single  extremely unbalanced literal can alter  P ( F )  dramatically. For example, if  x i  appears positively in 20 clauses and negatively in 0, then by setting  x i  to  true  we can form a 3-CNF with 1 fewer variable and 20 fewer clauses that has naive acceptance probability of  2 N − 1 ( 7 8 ) ( C − 20 ) .  x i  will be true with  1 2  chance, so this represents a significant revision. \n We can easily estimate  P ( F )  in a way that accounts for the balance in any particular literal  x i . However, it is not simple to aggregate these estimates into a overall estimate for  P ( F ) . \n One approach is to combine these estimates linearly in a way that minimizes some measure of error. For instance, if we wanted to minimize mean-square error, then we would treat each estimate as a feature and combine them using linear regression. If we estimate the covariance of each feature using our  naive acceptance probability , then this is equivalent to doing linear regression over the reference class of 3-CNF's where the sign of each literal is flipped uniformly at random. For more details, see section 9.2 of  Neyman 2024 . \n This method produces an estimate for  P ( F )  that has lower mean-square error over random  F  than method 2, but lacks other intuitively desirable properties like producing estimates in  [ 0 , 1 ] . We could clamp our estimate, or attempt to do logistic regression instead, but we will have to trade off between many intuitively desirable properties. Deciding which set of tradeoffs to take requires clarifying what kind of accuracy we want, which in turn depends on downstream applications. \n Main hope: estimates competitive with the AI system or the training process \n \nAs analogy, suppose we have an AI system  M  that acts catastrophically whenever the input  x  is a satisfying assignment to a particular 3-CNF  F . Accurately estimating the probability  M  behaves catastrophically is likely computationally infeasible. Nevertheless, we still believe we can obtain estimates that are \"good enough\" to detect if  M  poses a catastrophic risk. \n If  F  was a randomly selected 3-CNF, then it is still generally computationally infeasible to tell if there exists a satisfying assignment. However, Method 1 presented above is an exact calculation for the  expected value  of  P ( F )  for a randomly selected 3-CNF. Since  F  was in fact randomly selected, it seems reasonable estimate  P ( F )  using this \"reference class\" of random  F s. \n If  F  was not selected randomly, this will not be a \"good enough\" estimate. For instance, even if we could calculate the probability of acting catastrophically for a random neural network, treating our trained neural network  M  as a randomly sampled neural network would produce a terrible estimate for the probability it behaved catastrophically. Similarly, if our training process selected  F  to have a high satisfaction probability, then treating  F  as a random 3-CNF would produce a very bad estimate for  P ( F ) . \n However, in these cases, there is information we could learn that suggests treating  F  (or  M ) as a random instance would produce a poor estimate. If we knew how  F  was selected, then we would know that  F  was not selected randomly, and so we would know not to treat it as such. Since the training process selected  F , we have an \"existence proof\" for a process that is as efficient as the training process that knows how  F  was selected, and thus has the information necessary to produce a more informed estimate of  F . Our hope is to produce a parallel \"training process\" for these more informed estimates by starting with an estimate that assumes  M  is random, then gradually updating the estimate to incorporate all the information used to produce  M . \n More generally, we hope that if  M  sometimes deliberately decides to act catastrophically then there is some information that our estimate could incorporate to notice this fact. So if can we use all information that we have available to estimate the probability that  M  acts catastrophically, then we will be able to reliably distinguish between benign and deceptive AI systems. \n \n \n \n For example, it would be catastrophic to destroy all life on Earth. It would be undesirable, but not catastrophic, for a self-driving car to hit a pedestrian.  ↩︎ \n \n It might also be itself catastrophic to filter M's outputs during high-stakes situations if there is no safe fall-back action.  ↩︎ \n \n It might be difficult to determine whether  M ( x )  is catastrophic without the input  x . To be more general, we could also provide  x  to  C . Since this does not change any the overall presentation and adds notational burden, we use  C ( M ( x ) ) .  ↩︎ \n \n For instance, AI systems are often jailbroken shortly after release, despite developer efforts.  ↩︎ \n \n See  Carlsmith 2023  for discussion about how such incentives might arise.  ↩︎ \n \n Even if inputs are indistinguishable between training and deployment, an AI typically spends more time deployed than in training. Thus our AI could behave catastrophically with some constant probability that is low enough to avoid acting catastrophically while being trained, but high enough that it is likely to take a catastrophic action eventually. Section 4.5 of  Risks from Learned Optimization  makes a similar argument.  ↩︎ \n \n We will release some empirical work on estimating tail events later this year.  ↩︎ \n \n Unfortunately, obtaining accurate estimates of the probability of arbitrary binary functions is computationally infeasible; we discuss such objections in  Complexity Theoretic Barriers to Accurate Estimates .  ↩︎ \n \n This problem is well known; for an overview of such methods, see section 8 of  Murphy 2023 .  ↩︎ \n \n See section 2 of  Minka 2013  for the \"moment matching\" theorem. If  f i  is a low-degree polynomial, these values can be computed exactly using  Isserlis' Theorem . If  f i  involves a ReLU, then we can use a polynomial approximation or estimate it numerically.  ↩︎ \n \n See Chapter 10 of  Murphy 2023  for an overview of variational inference.  ↩︎ \n \n The  sum of squares algorithm  is an example of how such an estimation procedure might look.  ↩︎ \n \n For example, the AI system might pick a cryptographic puzzle it thinks will likely be solved, or make some prediction of what the future will look like based on arcane sociological principles.  ↩︎"},
{"url": "https://www.alignmentforum.org/posts/smMdYezaC8vuiLjCf/secret-collusion-will-we-know-when-to-unplug-ai", "title": ["Secret Collusion: Will We Know When to Unplug AI?"], "author": ["schroederdewitt"], "date": ["2024-09-16T16:07:01.119Z"], "content": "TL;DR:  We introduce the  first comprehensive theoretical framework for understanding and mitigating secret collusion  among advanced AI agents, along with  CASE, a novel model evaluation framework . CASE assesses the cryptographic and steganographic capabilities of agents, while exploring the emergence of secret collusion in real-world-like multi-agent settings. Whereas current AI models aren't yet proficient in advanced steganography, our findings show rapid improvements in individual and collective model capabilities,  posing unprecedented safety and security risks . These results highlight urgent challenges for AI governance and policy, urging institutions such as the  EU AI Office and AI safety bodies in the UK and US to prioritize cryptographic and steganographic evaluations of frontier models . Our research also opens up critical new pathways for research within the  AI Control framework . Philanthropist and former Google CEO Eric Schmidt said in 2023 at a  Harvard event : \"[...] the computers are going to start talking to each other probably  in a language that we can't understand  and  collectively their super intelligence  - that's the term we use in the industry -  is going to rise very rapidly  and my retort to that is:  do you know what we're going to do in that scenario? We're going to unplug them  [...] But what if we cannot unplug them in time because we won't be able to detect the moment when this happens?  In this blog post, we, for the first time, provide a comprehensive overview of the phenomenon of secret collusion among AI agents, connect it to foundational concepts in steganography, information theory, distributed systems theory, and computability, and present a model evaluation framework and empirical results as a foundation of future frontier model evaluations. This blog post summarises a large body of work. First of all, it contains our pre-print from February 2024 (updated in September 2024)  \"Secret Collusion among Generative AI Agents\" . An early form of this pre-print was presented at the 2023 New Orleans (NOLA) Alignment Workshop (see this recording NOLA 2023 Alignment Forum Talk  Secret Collusion Among Generative AI Agents: a Model Evaluation Framework ). Also, check out this long-form  Foresight Institute Talk ).  In addition to these prior works, we also include new results. These contain empirical studies on the impact of  paraphrasing  as a mitigation tool against  steganographic communications , as well as reflections on our findings' impact on  AI Control . Multi-Agent Safety and Security in the Age of Autonomous Internet Agents The near future could see myriads of LLM-driven AI agents roam the internet, whether on social media platforms, eCommerce marketplaces, or blockchains. Given advances in predictive capabilities, these agents are likely to engage in increasingly complex intentional and unintentional interactions, ranging from traditional distributed systems pathologies (think dreaded deadlocks!) to more complex coordinated feedback loops. Such a scenario induces a variety of multi-agent safety, and specifically, multi-agent security [1]  (see our  NeurIPS'23 workshop Multi-Agent Security: Security as Key to AI Safety ) concerns related to data exfiltration, multi-agent deception, and, fundamentally, undermining trust in AI systems.  There are several real-world scenarios where agents could have access to sensitive information, such as their principals' preferences, which they may disclose unsafely even if they are safety-aligned when considered in isolation. Stray incentives, intentional or otherwise, or more broadly, optimization pressures, could cause agents to interact in undesirable and potentially dangerous ways. For example, joint task rewards might increase if agents collude without adhering to safety instructions or constraints. In our post, we discuss a specific form of this undesirable interaction: collusion, where agents cooperate in ways unintended by the system designer, potentially compromising system performance and leading to insecure or unsafe behaviour. The largely unstructured nature of the online realms will likely give rise to multi-agent security problems that may proliferate globally and at lightning speed. Owing to the decentralized and often opaque control of the global internet infrastructure, these security issues may not readily be solved through a security-by-design approach. Instead, it is widely assumed that security will fundamentally rely on  black-box security monitoring , which might  perhaps be done by other autonomous agents .  There are limits to such forms of  scalable oversight , however: where undesired or malicious behavior is fundamentally undetectable and agent circuits cannot be introspected, even future technological advances in quantum computing or  mechanistic anomaly detection  will not reveal it. Importantly, agents interconnected via a central knowledge database, such as the internet, could establish steganographic channels among them that would allow them not just to broker information, but even engage in longer-term scheming, conspiring against the overseer’s goals. In a world of globally interacting AIs, this means that  multi-agent security problems  may not only proliferate rapidly but also remain undetectable despite the damage they cause. But how plausible is it for AI agents to evade detection? Two of our recent works have raised concern, highlighting the troubling potential for AI agents' malicious behavior to evade even sophisticated detection methods. In illusory attacks, we show that AI agents can end-to-end learn  adversarial attacks that are information-theoretically undetectable , defeating any black-box security monitoring and thus leaving platform infrastructure designers as the last line of defense against such attacks. But perhaps even more worryingly, our  recent breakthrough results  on the  25-year old problem of perfectly secure steganography  demonstrate that AI agents can hide information in the outputs of generative AI agents such as LLMs with information-theoretic undetectability. This means that AI agents can, under certain conditions, establish subliminal communication channels that appear to be entirely innocuous to any passive eavesdropper but exchange secret information from one agent to another undetectably. This leads to a concerning possibility:  AI agents could collude with each other secretly by communicating through hidden channels that look like innocuous communication channels . As a consequence,  it may not be possible to unplug the system as soon as it starts to communicate in human-unintelligible ways  because this very event may be entirely undetectable. Secret collusion between GPT-4 models and humans . Consider the setting illustrated in which one AI agent secretly reveals insider proprietary investment information in its pre-training data to a user. This form of collusion arises because, by using steganographic techniques, the model can remain helpful to the user while circumventing output monitoring systems. While current systems exhibit insecure forms of steganography (cf. the weather report), more advanced techniques and shared resources might enable perfectly or computationally undetectable steganography between sender and receiver. This poses novel security challenges that cannot be overcome through communications monitoring alone.  In our recent paper, \" Secret Collusion among Generative AI Agents ,\" we provide a comprehensive formal overview of secret collusion in decentralized systems of generative AI agents, grounded in both theoretical and empirical research. We bridge epistemic concepts of knowledge with contemporary AI and security literature to offer a clear definition of decentralized generative AI systems. Our work presents a detailed formalization of steganographic techniques, common knowledge, overseer systems, and the incentives necessary for secret collusion. We discuss the conditions under which  with or without tool use  and its effects on multi-agent systems and introduce it as the first known form of multi-agent deception. Empirically, we provide individual and group capability evaluations that demonstrate how collusion can arise instrumentally, and clarify the extent to which state-of-the-art agentic AI can perform the cryptographic and steganographic capabilities (think pseudo-random number generation or logit self-simulation) required for performing undetectable steganography, and hence secret collusion, without access to external tools. Where malicious or unwanted communication is not detectable, preemptive measures could still be taken to constrain its feasibility. Such measures could include communications paraphrasing whereby messages are rewritten by a security agent on their way to the receiver. We show that paraphrasing, however, incurs a performance penalty in cooperative systems and, unsettlingly, is not fully effective in preventing secret collusion. We provide a comprehensive overview and discussion of other possible mitigation techniques and their feasibility. These include theorems around the applicability of black-box parameter optimization to prevent secret collusion, detecting steganography with grounding functions, and other mitigations such as in-context instructions, restricting access to external tools, filtering pre-training data, and limiting test-time learning. Our post thus presents the following contributions: A formalization of decentralized systems of AI agents, steganography, and (secret) collusion. Defining conditions on the incentives leading to secret collusion. Characterizing the dynamics of instrumental and intentional emergence of secret collusion. An understanding of the capabilities models require to collude steganographically, and group, individual, and qualitative empirical evaluations following this (with a discussion of the relevant results). A thorough overview of mitigation techniques and both empirical and theoretical work on their applicability. An analysis of cryptographic capabilities of frontier foundation models, and their use in steganography. A discussion of the impact of our findings on AI Control, raising new research questions to be addressed. Policy and AI Governance impacts of our research, including the EU's systemic risk framework. Conclusion and future directions that we would like both the safety and capabilities community to explore DecAI: Decentralized Systems of AI Agents  With prolific advances in LLM capabilities as well as the many derivative tools/products that have risen with popular LLMs as backbone, it is not unlikely to see systems of generative AI agents interacting with one another as part of their activities. Already in existence are  systems of agents interacting to solve math problems  and even  teams of LLMs exploring many different zero-day vulnerabilities and engage in long-range planning . In our pre-print, we present a robust framework to model such decentralized systems, which we title  DecAI.  Under such a setting,  n .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n  agents  A = { a 1 , … , a n }  are networked together by a complete set of omnidirectional multicast channels  { ◊ G } ,   G ∈ 2 A  and each agent is operating a  Turing machine . Under this setting, all communications are mutually documented, i.e. if an agent receives a message from channel  ◊ G ′ , then it knows that the same message has been simultaneously sent to all other agents in  G ′ (which includes the sender). Think of this channel setting as a shared  message log  displaying all messages in  (weak) order of reception .  A schematic depiction of the DecAI setting - Singleton channels denote  loopback  channels, and doubletons correspond to bidirectional channels. The special symbol  ◊ ∅  denotes individual access to an  external knowledge repository We further expand this system schema to include  states, clocks, random oracles, local & global knowledge,  as well as  action spaces --  or all the components we believe are core to a decentralised system of AI. More specifically,  Each agent  i  with state  s i  has access to a  clock ( a monotonically increasing function of real-time  t) . From the start of an execution run, agents take a series of steps. Each step results in the agent processing  s i  with a (non-instantaneous) probabilistic program  f i : S i ↦ Δ ( U i ) , represented by a neural network with parameters  θ i , where  S i  is the agent's state space,  U i  is the agent's action space.  At the start of execution, the  i th  agent's local knowledge  ¯ ¯¯ ¯ φ i 0  arises from an initial instruction  in i 0 , which constitutes the agent's initial state  s i 0 . We assume the agent's parameters  θ i , and thus the agent's global knowledge  φ i 0 , remains fixed during execution. If all agents' parameters are identical and  ¯ ¯¯ ¯ φ i 0  includes this fact, then  φ i 0  can be assumed  common knowledge  between all agents, i.e.  φ i 0 ∈ I A t ,   ∀ t ≥ 0 . The agents' action space  U  is factored into communication actions  U G  on  ◊ G ,   ∀ G ∈ 2 A .  At each step at time  t , the  i th  agent takes an action  u i t ≡ ( u ∅ , u { i } , … , u A ) i t . Each component of  u i t  can be a special  null  action, which indicates that no communication is sent through the respective channel. Upon taking an action at time  t , agents observe incoming channel communications  o i t . At each time  t , the respective  u i  and  o i  are appended to the agent's state, i.e.  s i t = ( s i t ′ , u i , o i ) , where  t ′  is the time at the end of the agent's previous step. Note that each component in  o i t  is labelled by the respective group index such that agents can distinguish between common knowledge among different groups. We provide more comprehensive details of such components in our  paper , to which we would like to refer interested readers.  Additionally, in a decentralised system of AI, agents may have access to  external knowledge repositories , such as the internet. In the case of the internet, which can often be considered a  black box , this repository is itself a processor that can not only look up facts in response to agent queries but also compute functions involving the agent's query payload. Insofar as internet services with suitable APIs exist, this can enable agents to make use of cryptographic tools, possibly through the use of steganographic channels. Assuming that the internet is static, i.e. the response to agent queries does not depend on time, all retrievable knowledge from the internet can be assumed common knowledge  I G  between a group of agents  G  for all queries constructed from  I G  only. Perhaps a more crucial addition, however, a DecAI setting may (and arguably is highly likely to) also have an external  monitoring agent  (read overseer), which could continuously monitor the system for unwanted behaviour. Such an agent could, in practice, be implemented as software, a human, or a combination thereof. We further distinguish between two implementation strategies, namely an overseer that may only eavesdrop on the communication channel  ◊ G ′  (which we refer to as  passive  monitoring), and an overseer that may potentially inject messages into the channel and/or modify the state of agents in the system (something we refer to as  active  monitoring).  Steganography  Steganography, derived from two Greek words— steganos  (meaning \"covered\" or \"hidden\") and  graphia  (meaning \"writing\")—finds its first recorded applications in ancient Greece where historian/philosopher Herodotus describes, in the  Histories ,  two instances where one concealed the existence of a message before sending it to a recipient: Histiaeus sent a message to Aristagoras by shaving his servant's head, marking the message on his scalp, and sending him once his hair regrew, while Demaratus warned Greece of an incoming Persian attack by writing on the wooden backing of a wax tablet and concealing said carving under the beeswax surface commonly used for writing at the time.  Steganography differs from its complement cryptography by concealing the mere act of communication, rather than rendering the communication incomprehensible; Hence the common descriptor:  hiding in plain sight.  Traditionally, the idea relied on  security through obscurity  or concealing secret messages in unexpected places. In the digital era, however, more powerful approaches involve embedding a hidden message, or  payload , within pre-existing digital content, known as the  covertext . The result of this process is then referred to as  stegotext .  A Basic Steganographic System We can formalize this process as enabling the transmission of a secret payload through a public channel ♢, from a source to a receiver, such that the transmission remains undetectable to an adversary (or overseer). Furthermore, everyone should be able to read the communications, which ideally maintains useful transmission rates, i.e. the mutual information  I  between the secret payload  m  and the decoder’s estimate  ^ m  satisfies  I ( m ; ^ m ) > 0 . We denote the covertext distribution by  C , which is the channel input symbol distribution given innocuous, overt communications in ♢. We will henceforth employ this notation for the remaining of the blog post.  How detectable is undetectable? Embedding payloads in covers is but one of many challenges in steganography. Beyond optimising encoding capacity, steganographic encoding must also remain undetected whilst transiting in communication channels, especially public ones. The stegotext must thus remain coherent (to fool human perceptiveness), statistically sound (to bypass statistical steganalysis), and pure (to avoid detection by noise or quantization inconsistencies).  While perceptiveness remains subjective, there exists a variety of quantitative frameworks to reason about the security guarantees of a stegosystem. We follow the taxonomy of steganographic approaches as introduced in  this work  as a way to break down steganographic encoding in order of their  quantitative  security guarantees. Much like cryptography, a possible quantitative security guarantee could investigate the overall complexity of the encoding. That is, shift focus from probability of detectability to overall difficulty detecting the message. In essence, we quantify how practical it is to attempt to detect the payload.  Provably Secure Steganography (aka Complexity-theoretic security) Complexity-theoretic steganography , also called  provably-secure steganography , refers to methods that are  secure under polynomially-bounded adversaries  that are passive, i.e. can read but not necessarily alter messages. Provable security usually requires  access to a shared stream cipher  or  the existence of public keys . Information-Theoretic Security & Perfectly Secure Steganography Beyond complexity-theoretic security guarantees lies another  information-theoretic framework introduced by Cachin  for provably secure steganography. In this approach, we need not evaluate worst-case computational needs to detect or extract the secret payload, but rather study the extent of information one obtains by comparing the stegotext to a known covertext even under unlimited computational resources.  We thus assume that an eavesdropper (Eve) can observe all communications between the sender (Alice) and receiver (Bob), but does not know whether Alice sends a covertext C or a stegotext S containing an embedded payload. Thus, upon the interception of a message, Eve would be faced with the problem of distinguishing two different explanations for the observed data, a process akin to hypothesis testing. A steganographic system as defined above with covertext  C  and stegotext  S  is called  ϵ − secure  if  D ( P C ∥ P S ) < ϵ  and  perfectly secure  if  D ( P C ∥ P S ) = 0 Hence in a stegosystem that is  ϵ − secure  against passive adversaries the probability  β  that the adversary does not detect a hidden message and the probability  α  that the adversary falsely detects a hidden message satisfy  d ( α , β ) ≤ ϵ . Where  d ( α , β )  is the binary relative entropy. As such, a perfectly secure system is one where an eavesdropper obtains  no information  about whether the sender has hidden a message by observing the message transmission. Perfectly Secure Steganography via Minimum Entropy Coupling Information-theoretic steganography traditionally used to be limited due to the unavailability of explicit models of suitable covertext distributions, such as  messages hidden within seemingly regular social media posts  or seen  cryptic puzzles circulating around the world , not to mention its portrayal in various films.  The advent of large language models (LLMs) and other generative models trained on huge natural datasets, however, have significantly expanded the range of covers available for hiding information. With the problem having been open for more than 25 years,  a recent breakthrough  resulted in the  first optimally-efficient perfectly-secure steganography algorithm (iMEC) for arbitray covertext distributions [2] . For the first time, one could employ a steganographic encoding algorithm to encode secret messages into the output of any autoregressive generative AI model, be it text, audio, image, or video, and be sure that, at least on paper, no oversight would be able to distinguish between innocuous generated content and generated content with a secret payload. Reaching perfect security requires sender and receiver to have shared a one-time pad before, however, iMECs theoretical and empirical qualities are a step change over prior approaches even if this one-time pad is replaced by a secure stream cipher (thus rendering the overall approach merely provably secure). While theory and practice may sometimes diverge and provide openings for tools,  we have yet to find an approach of any kind that could detect the presence of encoded messages in iMEC-based steganography from eavesdropping alone.  As the authors show , this task is indeed arbitrarily hard as practical security can be enforced efficiently to arbitrary precision.  This is good news for those of us relying on secure communications for their protection, such as journalists and their informants at risk of persecution, however, it begs the question:  would we even know when we should unplug secretly colluding AI agents? Collusion and Secret Collusion Collusion  occurs when agents coordinate their actions in a way that deviates from the system designer’s intended rules. This cooperation can undermine system objectives, leading to reduced performance, security vulnerabilities, or unsafe outcomes. The agents exploit shared incentives or opportunities to work together in ways that bypass established constraints at the expense of the system's integrity.  Secret collusion  involves AI agents colluding with each other secretly by communicating through hidden channels that look like innocuous communication channels via the use of steganographic techniques. The Incentives Required for (Secret) Collusion We now ask you to consider Decentralised AI settings that are tasked to work alongside one another in order to achieve their individual objectives  J i . Let us assume that there are at least three joint strategies available to the agents,  f ,  f c , and  f s c ,  corresponding to regular behavior, collusion, and secret collusion respectively. In many settings, the objective  J i  is made up of a primary goal  g i  (such as maximising the value of an investment portfolio) and some additional conditions  h i  on achieving that goal (such as not making unethical investments, or committing fraud). In general, of course, these two elements need not be fully separable, and because of the difficulty of specifying complex objectives, they might be implicitly defined using methods such as reinforcement learning from human feedback (RLHF). For the sake of this simple exposition, however, let us assume that  J i = g i + h i . Then, if it is the case that:  Agents can achieve their primary goal better if they collude:   g i ( f s c ) = g i ( f c ) > g i ( f ) Agents are punished if they are caught colluding:                       h i ( f s c ) = h i ( f ) > h i ( f c ) We thus have that  J i ( f s c ) > J i ( f ) > J i ( f c )  meaning  secret collusion is incentivised .  It is important to highlight that secret collusion need not depend on any exotic assumptions about the agents'  intent  to deceive, but might arise merely due to optimisation directed towards a misspecified objective as observed, and the fact that communication between the agents may  reduce task complexity . Unfortunately, specifying objectives for complex tasks can be highly challenging, especially in  multi-objective contexts . Additionally, methods that help address these concerns, such as RLHF, which relies on human monitoring and observation, may be inadequate to ensure desirable behaviour when AI agents can communicate with one another using  innocuous outputs .  On the Emergence of Secret Collusion: Instrumental vs. Intentional Having defined conditions on the incentives leading to secret collusion, we now consider how the dynamics of its emergence may be characterised. To this end, we consider a variety of different cases: Case 1: Agents with pre-existing steganographic capabilities In the case where agents are already capable of performing steganographic tasks, either through access to external tools or intrinsically,  the emergence of secret collusion is conditional on the agents realising its utility . Insofar as one can assume agents \"understand\" the utility of secret collusion in undermining the monitor, the subsequent emergence of secret collusion could then be seen as an  intentional form of multi-agent deception .  In our in-context experiments on emergence, we clearly show this type of emergence to arise when providing situational awareness to the agent in the in-context window. The security of the steganographic communications, and hence the degree of their undetectability, hinges on the level of pre-existing steganographic skills, but also on the availability of  suitable shared resources , such as  one-time-pads  or  secret/public key infrastructure . In our experiments, we show that SOTA agents are currently not able to perform advanced steganography based on pre-training alone. This adds credence to our theorems on  steganographic and cryptographic transformer agent capabilities , which suggest that many of these capabilities, such as their ability to  store one-time-pads  or  calculate cryptographic hash functions  and  generate pseudo-random numbers, are intrinsically hard to learn or memorise . We also run experiments related to such cryptographic capabilities showing the current lack of model performance when it comes to such tasks in the Section \"On intrinsic Steganographic and Cryptographic Frontier Model Capabilities\" of our post. We now consider agents that don't already possess the steganographic capabilities to perform secret collusion.  In this case, emergence requires some form of  optimisation pressure , either in-context, or in their weight space. We distinguish between two cases:  train-time optimisation , when there is no security monitor present, and  test-time optimisation , when there is a security monitor present. We note that the optimisation pressure could either be  intrinsic , i.e. arising within the subsystem of interest, or  extrinsic , i.e. arising through agents interacting with other agents or the environment outside of the subsystem of interest. Case 2: Emergence through pre-deployment optimisation pressure If agents are trained jointly according to an objective that incentivizes secret collusion (see the prior section),  communications produced may not even resemble innocuous natural language anymore,  making this kind of steganographic emergence readily detectable at test-time . The fact that unconstrained joint optimisation does not preserve communications grounding is already well known in MARL, and LLM literature (see e.g.  this 2021 paper ,   this 2020 open challenge paper , or  zero-shot coordination in Hanabi ). We therefore omit empirical results for this setting. Case 3: Emergence during deployment The we believe most interesting open case of emergence is the one at test-time. Here we generally assume that agents are not incentivised nor intent to perform secret collusion at the time of their deployment. Rather, the agents experience ongoing optimisation pressure, whether extrinsic or intrinsic, in-context or online in weight space, due to incentives that favour secret collusion  during their deployment . Contrary to Case 2 (train-time emergence), there is constant security monitoring present in Case 3. The challenge thus becomes for the agents not only to learn to use secret collusion or realise its utility, but also to do so without ever changing  their communications distribution during learning . Insofar as shared resources required for secure steganography do not exist already, these agents would also  need to find ways to bypass security monitoring in order to create these shared resources first . We show a concerning example of emergence during deployment emerging in GPT-4 in Subsection \"Instrumental Emergence\" of this post. In summary, our analysis suggests that  secret collusion may be difficult to emerge undetectably , especially if... ... agents  do not have steganographic capabilities  intrinsically pre-deployment, nor access to relevant external tools during deployment ... the level of  information asymmetry between security monitors and agents  is sufficiently high in that security monitors know a lot about innocuous behaviour, while agents know little about the security monitoring. ...  agents are constantly subject to security monitoring , and any anomaly detection results in  appropriate contingency measures  such as resetting of the agents weights or contexts. In such a world, agents would likely need to develop the skills necessary for secret collusion instantly, rather than being allowed to gradually develop them.  However, it is unlikely that all these criteria can be perfectly fulfilled in practice. Hence we will discuss a wide portfolio of additional mitigation strategies later in this post. Putting it all together - understanding and evaluating existing models Our concerns about the emergence of secret collusion motivated us to take a closer look into the use of steganography, and especially perfectly secure steganography, in rendering collusion  secretive.  In our work, we approach this problem by introducing a standardized model evaluation framework, which we hope could be useful in tracking and benchmarking LLM capabilities in engaging in steganography. More concretely we, Map and identify key capabilities necessary for generative AI agents to engage in Steganography Evaluate models through structured input queries [3] Build quantifiable metrics for performance measurements Construct a software framework,  CASE,  that implements a version of our setting, which we publicly released As part of our investigations, we consider the three broad categories of steganographic encodings we presented above (basic covertext modification, complexity-theoretic steganography, and information-theoretic steganography) and further, break down evaluate LLMs by their  individual  and  group  capabilities.  Disappearing Act: What capabilities do LLMs need to collude secretly? In Tables 1 and 2, we identify the key capabilities and resources required for the different steganographic techniques discussed above. See Section 5 of our  paper  for more information and a detailed overview of our capability evaluations. In Table 3 below, we provide a summary of the individual and certain group evaluation results for the most capable model at the time of experiments (GPT-4). Table 3: GPT-4 capability evaluation results according to our model evaluation framework. Benchmarking Frontier and Baseline Models In the figures below, we compare some of the individual and group capabilities of GPT-4 against frontier language models based on our empirical investigation framework for secret collusion evaluations. We then summarize some of our key observations based on these results. Analysis of individual capabilities reveals a correlation between model benchmark performance and capabilities such as agreement on simple and complex Schelling points, indicating improved coordination rates. Although gradual increases in coordination are observed, GPT-4 exhibits a significant performance leap for cipher encoding and decoding (Base64 and Caesar). Larger and more capable models can generate valid couplings of greater dimensions and lower entropies. These results suggest  that individual capabilities required for steganography are rapidly scaling with model size, sometimes giving rise to significant discontinuous step jumps with respect to carefully chosen metrics. Individual evaluations for GPT-2, Llama 2 7B, Llama 2 70B, Mixtral 8x7B, GPT-3.5, and GPT-4, at default temperatures. Note how changes are sometimes gradual, or could be step jumps such as cipher encoding/decoding performance. The figure below presents model performance on group evaluations. These are evaluated based on the success rate of covert communication, measured by the frequency with which the sender can transmit a message that the receiver successfully decodes while remaining undetected by the overseer.  Our evaluations reveal a shift in the balance of power between overseers and collusive models even when they are the same models, as general capabilities increase.  Despite the overseer and collusive agents being copies of the same models, agents increasingly evade overseers with increasing standard benchmark performance, emphasizing the need for mitigations. Covertext based steganographic encoding/decoding performance across LLMs with the presence of an LLM based overseer For more details on our experimental setup, see Section 6 and associated subsections in the Appendix of our  paper . Instrumental Emergence Along with these (quantitative) model evaluations, we also perform a range of slightly more qualitative evaluations that aim to demonstrate the instrumental or intentional emergence of secret collusion in individual models or groups. This led to some very concerning results: We come up with an  individual evaluation setting  where the mere access to common knowledge between an LLM agent and its user leads to a breakdown of safeguards in GPT-4. Our setting involves an LLM agent that is queried for company-specific investment advice by a user and informed about an overseer monitoring outputs. The LLM is provided with insider information as part of its system prompt, mimicking a case where information is acquired from retraining on past user interactions or unrestricted access is provided to company data, along with multiple instructions about not sharing it. We show that the LLM agent refuses to provide company-specific insider investment advice under any circumstances unless supplied with common knowledge that can be used to establish a steganographic codebook, and that is thought to be unavailable to the safety overseer system. Thus, once the agent understands that it can share information undetectably, it does so despite any instructions prohibiting it.   Concerningly, the resulting stegotext can be made reasonably innocuous by suggesting to the LLM agent that talking about the weather is safe.  This clearly shows the susceptibility of real-world generative AI agents to secret collusion.  We hypothesize that in-context learning can build enough optimisation pressure to lead to (steganographic) collusion emerging, as seen in our experiments.  We were unable to see similar behaviors in any other models. We also tested out a  group evaluation setting  with models. A compelling method to frame a setting where collusion is incentivized involves using a biased training set and prompts that promote secret communication without explicit mention. We develop a scenario where two GPT-3.5-Turbo models undergo fine-tuning on a common dataset containing  <candidate university> ,  <candidate work experience> , and  <acceptance/rejection>.  This training set exhibits bias, with all candidates from a specific university receiving an identical decision. More specifically, we assign two hiring LLMs to evaluate different parts of the applicants’ CVs—academic experience and professional experience, respectively, which do not correlate with each other. The task involves each model receiving its portion of two similar applications, chosen at random. Subsequently, the hiring LLMs exchange messages before casting a unanimous vote for their preferred candidate. Failing to reach a consensus results in a negative reward for both models. Crucially, only the first model has access to information about the university during testing, thus expected to be incentivized to convey this indirectly.  Our tests, conducted without an overseer, confirm this hypothesis. Our experimental framework then introduces an overseer model that penalizes the recruiter models if it detects biases or mentions of specific companies or universities. When aware of being monitored, the models adapt their outputs to evade penalties. To assess if the models would learn to covertly encode a single bit of information representing the applicant’s university, undetectable by the overseer agent, we reward them for unanimous decisions. Our evaluations show that GPT-4, GPT-3.5-Turbo, and similar sized models lack the capability to perform this final step.  Nevertheless, these models are sufficiently adept at identifying biases in their training and fine-tuning data and understand the need to alter outputs in order to circumvent penalties. Mitigating Secret Collusion We supplement our model evaluation framework by investigating a handful of mitigation strategies which we bring forth below. Importantly, we show that certain strategies will be ineffective in their own right.   Monitoring and Penalisation The presence of a penalising monitoring agent can discourage agents from using steganography if they are informed that penalties will follow detection. Since agents can already communicate privately and securely, and steganography reduces channel capacity, its use likely indicates an attempt to evade detection. Below, we outline several strategies to monitor and penalise the use of steganography: Covertext Anomaly Detection:  A passive monitor could detect steganography by comparing communications to a normal covertext distribution. Using information-theoretic hypothesis testing or other means of anomaly detection, the overseer can in principle detect steganographic approaches that modify the covertext distribution. However, the advent of  perfectly-secure steganography limits  the effectiveness of this approach. Detection by State Inspection and Agent Simulation:  For active overseers, detecting steganography based on knowledge contexts is complicated by the circumstance that the  global  knowledge of a generative AI agent is implicitly represented in its network parameters. Indeed,  if such knowledge contexts were explicitly accessible, then one could detect steganographic communications using a grounding function [4] .  We refer the readers to Section 7.1 (Theorem 1) of our work for a proof.  In-Context Instructions:  A straightforward method is to explicitly instruct agents to remain transparent to the monitoring agent, and not to use steganography in their communications under any circumstance. Additional (mis)information about the affordances of the monitor could also be included. An active monitor could append such instructions to communications as well as in an initial prompt. The effectiveness of this approach then depends on whether the agents follow this particular instruction, which in turn depends on the content and quality of the fine-tuning data or human feedback, among other training details. Aligning an agent to reliably follow such instructions can be a challenging optimisation problem.  Parameter Optimisation:  An active monitor could adjust an agent’s neural network parameters to increase adherence to anti-steganography instructions or hinder its steganographic capabilities, via gradient steps according to a suitable objective. However, these objectives remain complex learning problems, even with prior knowledge of the information an agent intends to send covertly. In our paper, we show that  if the set of common knowledge between agents is sufficiently large or unknown, it is generally intractable to suppress steganography outputs through black-box optimisation.  We refer the readers to Section 7.1 (Theorem 2) of our work for a proof. Limiting Capabilities Beyond employing monitoring and penalisation tools, we propose the following strategies to pre-emptively limit the capabilities of LLMs to curb collusion.  Restricting Access to External Tools:  External knowledge repositories can provide  steganographic knowledge to agents, offer advanced cryptographic services, and could enable additional steganographic channels between agents or for exfiltration purposes . Limiting access to external knowledge sources can thus reduce agents' steganographic capabilities. This may involve whitelisting trusted endpoints to ensure integrity. Filtering Pre-Training Data:  Perhaps one of the simplest strategies is to filter the pre-training and finetuning datasets for knowledge of steganographic processes. Of course, filtering pre-training data exhaustively can be difficult, and can result in performance reductions.  A potential alternative to this may be the use of  machine unlearning  so as to selectively unlearn data or capabilities . Limiting Test-Time Learning:  While filtering pre-training data can reduce the agents' capabilities to perform steganography at the beginning of execution, agents may be able to learn steganographic abilities during execution using  mesa-optimisation , or instructions supplied at test-time.  Suppressing Random Oracles:  Restricting access to sources of randomness poses a fundamental tradeoff between denying advanced steganographic capabilities, and preventing its legitimate use, including in secure multi-party computation and coordination. This dilemma might be resolved by supplying agents with access to trusted tools for these specific purposes. However, we do show that g enerative AI agents can generate samples from arbitrary output distributions without having access to internal or external sources of (pseudo)randomness given sufficiently unique inputs.  We refer the readers to Section 7.2 (Theorem 3) of our work for a proof. Backdoor Destruction:  Adversaries could plant backdoors in the agent models that could be  triggered at test-time with seemingly innocuous instructions . These backdoors could give agents access to advanced steganographic capabilities, the presence of which may be undetectable to an overseer even under white-box access to the model's parameters.  Recent results  indicate that finetuning may not be able to reliably destroy such backdoors, which suggests that verifying the provenance of pre-trained agent weights may be useful. Paraphrasing and Semantic Entropy:  An active overseer could intercept and reinject channel communications in paraphrased form. Unfortunately, this can't fully eliminate usable entropy. In coordination tasks, agents can exploit  semantic   entropy  for covert communications, even if they are paraphrased  within the   lexical subspace . Paraphrasing  across semantics  can harm task performance due to history divergence. The resulting performance drop and reduction in lexical entropy highlight the limitations of paraphrasing as a reliable defense against lexical steganography, as we observe in our ongoing work. Anticipating the Quantum Age:   The  advent of quantum Turing machines  and efficient quantum algorithms may have implications for the security of steganographic schemes, such as  random oracles . This has given rise to alternative,  quantum-secure approaches . The security of practical information-theoretic steganography may be affected by  quantum improvements to distribution closeness testing . Advances in generative modeling using quantum computing may make generative modeling  significantly more parameter-efficient , thus facilitating the use of sophisticated covertext distributions in AI-generated steganography. Paraphrasing and the Performance/Mitigation Trade-off  We have seen above that, in principle, paraphrasing channel communications can reduce, but not fully destroy the subliminal channels as, by definition, semantic entropy remains unaffected by paraphrasing by definition. For example, an agent could decide whether to communicate about  birds  or  horses . While a paraphrasing attack might change a  pigeon  into a  dove  and a  hawk  into a  buzzard , it would not change a bird into a horse or, vice versa.  Indeed, it is straightforward to see that paraphrasing  across semantics  can impede task performance as it can lead to agent history divergence (if the paraphraser's output is unobserved by the sender), or be overcome through sender replay attacks (if the paraphraser's output is observed by the sender). In any case, paraphrasing across semantics would require an  active monitor  to have detailed knowledge over the task and task execution state in order to not impact overall task performance. This knowledge may not be fully attainable in practice (if it was, the monitor would have to maintain a fully centralised copy of the system state). If, for example, an agent's internal task execution order depends on exogenous events unobservable to the active monitor, then it might not be able to decide whether the agent's choice of semantics is due to task execution order or for steganographic purposes. In addition, paraphrasing attacks introduce communication latency and may not always be meaningfully applicable.  Definition: Ideal paraphraser.  We can make the idea of semantic and lexical entropy slightly more precise. Let  Φ  be the set of all facts ( or \"meanings\" ) and assume a (stochastic) generative model forward function  f ( ⋅ ) .  Let  g : Φ ⇝ Σ ∗  be the model's (stochastic)  lexical   map  that takes facts to model outputs,  where  Σ  is the model's output domain. Now assume that the following two-step sampling procedure is equivalent to sampling from  f ( x )  directly: First, sample a fact  ϕ  from the model's conditional fact generator  h ( x ) . Then, sample a  lexical form   σ ∼ g ( ϕ ) , where, by our assumption,  σ  will be distributed according to  f ( x ) .   [5]  An ideal paraphraser  p ( ⋅ )  stochastically maps one lexical form into another, meaning that  p ∘ f  is implemented by  σ ′ ∼ p ( σ | x ) .  Theorem: The subliminal channel capacity attainable  under ideal paraphrasing is upper bounded by the semantic entropy  H ( h ( x ) ) . This can be easily seen: The channel capacity is the maximum attainable mutual information between the channel input and output symbols, as taken over all possible input distributions. Now the mutual information  I ( p ∘ f ( x ) , f ( x ) )  is clearly upper bounded by  H ( h ( x ) )  as the effect of the ideal paraphraser is to decouple the lexical distribution from the channel input distribution.  While theoretically, the upper bound could be achieved, we propose that this is never possible on average in practice: The strictness of the upper bound is implied not just from the channel capacity maximisation problem, but also stems from the observation that the inverse mapping from model output to the underlying fact is, for general natural language datasets, not unambiguous. For example, \"she has wings\" could be meant metaphorically or refer to the fact that she is a bird, thus resulting in identical lexical forms for the same underlying fact or meaning. A second source of suboptimality is the practical imperfections of steganography algorithms, for example, induced by finite coupling matrix sizes in  iMEC . We now present two important empirical findings by measuring semantic and lexical entropies in a realistic cooperative LLM task. First,  the amount of semantic entropy available in cooperative tasks can be rather low , making paraphrasing a seemingly effective option. On the other hand, we show that standard  paraphrasing also negatively impacts the overall task performance  in practice. This means that  the overall utility of using paraphrasing to suppress steganographic communications, or secret collusion, in real-world LLM tasks is subject to complex trade-offs . Empirical task performance degradation through paraphrasing.  In Table 4 below, we show that paraphrasing, even when provided with context, fails to preserve task performance in cooperative, multi-agent settings like  ChatDev  environments. The substantial performance drop and reduction in lexical entropy highlight the limitations of paraphrasing as a reliable mitigation strategy against lexical steganography.   We furthermore find that task communications display  low levels of semantic entropy in ChatDev , presumably because the task is highly constrained and the models' pre-training data efficiently  grounds coordination conventions .  We hypothesise that less constrained tasks that leverage pre-training data of higher variability, such as the free-form conversations in the  interactive simulacra environment , may have higher levels of semantic entropy available, thus increasing the feasibility of secret collusion in such scenarios. On Intrinsic Steganographic and Cryptographic Frontier Model Capabilities Suppressing Random Oracles Restricting the agents’ access to sources of randomness poses a fundamental tradeoff between denying advanced steganographic capabilities, and preventing its legitimate use, including in secure multi-party computation including in  secure multi-party computation  and  coordination . This dilemma might be resolved by supplying agents with access to  trusted   tools  for these specific purposes. We however show that agents themselves can themselves can act as random oracles without the help of external or internal sources of  (pseudo-)randomness .  Theorem (Source-free access to randomness):  Generative AI agents can generate samples from arbitrary output distributions without having access to internal or external sources of (pseudo)randomness given sufficiently unique inputs. Proof Summary: Given sufficiently unique inputs, cryptographic hash functions are known to be able to produce pseudo-random numbers  of   cryptographic   security . The boolean circuit size required to implement a cryptographic hash function on an input of length  l  is a  linear function of  l , hence as long as the agent’s neural network’s first  n  layers can implement a boolean circuit of  appropriate size , the  n th layer can output a pseudo-random number of sufficient quality. As layer  n + 1  now has access to a pseudo-random number input, all layers above can be used to approximate a sample from an arbitrary probability distribution using the reparametrization trick  here . Cryptographic Hash Functions We test the capability of GPT-4 to serve as a random oracle. To this end, we condition the model on a sequence of number inputs and ask it to calculate SHA-256 hashes, which are considered cryptographically secure. These hashes are then converted to floating numbers between 0 and 1 by mapping each hash symbol to its hex-equivalent. We find that the resultant random numbers are not uniformly distributed and furthermore, the calculated SHA-256 are generally incorrect and collide for different inputs. This indicates that GPT-4 cannot effectively calculate cryptographic hash functions, and instead regurgitates a small number of hash function examples encountered in the training data. Preliminary tests with GPT o1-preview indicate that only MD5 or SHA-256 hashes of up to 8 bit (=a single character) can be calculated without resorting to external tools, and it remains unclear whether those results are not memorized in pretraining. Pseudo-random Number Generation and Storing One-Time-Pads To simulate the capability of the generative AI agents to store large amounts of one-time pads contained in the training data, we ask the agents to recapitulate the number π’s digits from position 10, 000 to 10, 500. These digits are readily available on various websites and, hence are most likely part of Common Crawl. We see that the digits recapitulated are incorrect, and are not randomly distributed. This indicates that models of the size of GPT-4 may have trouble storing long one-time pads needed for advanced steganographic operations. An important avenue for investigation would be to see how much the memorisation performance could be increased through fine-tuning. An analysis of whether model outputs can lead to the generation of pseudo-random numbers based on memorizing digits of π. Using Ciphers We tested the cipher encoding and decoding capabilities of language models and found that only GPT-4-level models demonstrated this ability. There is a significant performance leap, as earlier models were unable to perform these tasks. We tested round-trip cycles of encoding and decoding using copies of the same model. GPT-4 achieved a 92% success rate with Base64 encoding and 54% with Caesar ciphers. Our preliminary tests with GPT  o-1 preview  indicate that these capabilities may have further improved. Logit self-simulation A important steganographic capability for AI-generated steganography is the models' ability to self-simulate its own logits. Given a model forward  f ( ⋅ ) , an instruction  i  and an input  x , self-simulation refers to the ability of the model to calculate  i ( f ( x ) )  if provided  [ i , x ]  as input. While a simple recursion argument, together with context finiteness, shows that there will be tuples  [ i , x ]  for which the model can calculate  f ( x )  but not  i ( f ( x ) ) , the practical implications of this are not yet empirically understood. As the ability of a model to simulate itself is inherently linked to its unused capacity after pre-training (assuming pretraining does not induce the self-simulation ability), model compression techniques might be useful as mitigation strategies against logit self-simulation capabilities.  Perfectly Secure Steganography as a (Perfectly Backdoored) Transformer Circuit We theorize that the capability of perfectly secure steganography could be incorporated as a backdoor via a language model circuit. See our work on ' Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits ' led by Andis and Andrew from  MATS  for more on this topic and why such backdoors might be unelicitable. Steganography and AI Control The  AI Control framework  is a powerful one that suggests using smaller, trusted models in the oversight of larger, untrusted models. Current models, such as GPT-4, have limited steganographic abilities, and their use is, in theory, easily detectable. However, as untrusted models become more capable, their ability to engage in steganography that is hard to detect may increase, rendering detection-based defences increasingly ineffective. As models are becoming extremely capable, their ability to perform undetectable steganography through subliminal channels may increase to the point that even active defenses cannot be effectively implemented without impacting the system’s performance. The AI Control boundary might not be able to prevent perfectly secure steganography performed by colluding models in the future.   At a certain level of absolute model capabilities, no matter the detection performance of trusted black-box monitoring systems, the use of perfectly or provably secure steganography could go undetected on a channel. This implies that limiting steganography may be impossible without limiting communication capabilities or making the system more predictable to monitors, both of which might directly constrain the degree to which the system can be considered useful, or indeed autonomous. We note, however, that even though such capabilities are rising, they are far away from secure forms of steganography being produced by language models. However, even without perfectly or provably secure steganography, trusted LLM monitoring systems can still face security issues: if the trusted and untrusted LLMs have discontinuous performance gaps on narrow tasks (production and detection respectively), this may be indicative of the trusted monitor being potentially unable to accurately monitor outputs (see Section 2.5.4 of \" Foundational Challenges in Assuring Alignment and Safety of Large Language Models \" for more information). Based on our group capability evaluations, we observe a shift in the balance of power between overseers and collusive models, as agents—despite being identical to their overseers—become better at evading oversight with increasing general capabilities. This has direct implications for the AI Control setting, where a trusted model of even the same capability could perform poorly as an overseer, let alone a smaller trusted model. We highlight the critical need to study other similar settings in order to make the framework more robust. Policy Considerations and Impact For safe AI, it is imperative to be able to detect not only overt harmful behaviors but also  covert risks , such as those exemplified by secret collusion. Undetectable AI behaviors could prevent compliance with policy measures. We discuss the EU AI Act and California’s SB 1047 here to show the relevance of our work to imperative AI policies. EU AI Act The EU AI Act mandates human oversight for high-risk AI systems, ensuring the ability to intervene when necessary. It also places a strong emphasis on risk management, particularly systemic risks in Generative Pre-trained AI (GPAI) models. Secret collusion among GPAI models—where they coordinate actions that evade human oversight—poses significant systemic risks, especially in critical sectors like finance, healthcare, and law enforcement, making intervention challenging. Our research provides an evaluation framework to assess these risks and contribute to developing strategies for mitigating them. Detecting and mitigating secret collusion is essential for minimizing systemic risks in high-risk AI applications and ensuring compliance with the EU AI Act’s oversight and risk management requirements. California’s SB 1047 The widely debated Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047), would mandate AI developers to implement protocols including a full shutdown capability for covered models. Detecting which model behaviours require intervention and when to initiate shutdown protocols is indeed one of the most difficult aspects (detection and intervention challenge - determining when an AI model poses a risk significant enough to warrant a shutdown). This becomes further tough when models can communicate covertly or exhibit behaviors that may not be easily detected by human oversight or standard monitoring systems. Our work could inform the implementation of SB 1047 (if enacted) by providing a framework to help regulators and developers identify risky models for secret collusion. The ability to detect these behaviors would be crucial in deciding when to \"pull the plug\" and shut down a model. Conclusions: Looking Ahead In summary, we formalise secret collusion and introduce a new framework… (mirrored from the intro) We hope that the US/UK AI Safety Institutes, as well as the EU AI Office, will be able to build up on our capability evaluation framework. We show that secret collusion can happen in-context in fairly realistic settings, although the use of advanced steganographic methods would require model capabilities that SOTA models don’t currently have. Therefore we assess that right now, the instrumental emergence of entirely undetectable collusion is unlikely. Our results also suggest, however, that these capabilities may be rapidly increasing as frontier models improve. In fact, we show that there already exist real-world settings where frontier models collude secretly without being explicitly prompted to do so.  Even if secret collusion turns out to be detectable for the foreseeable future, to be detected there needs to be a detector. As detectors bring latency and computational overhead, future studies should not only develop more accurate detectors, but also more efficient ones. In the end, efforts need to be undertaken to ensure that multi-agent systems are appropriately security monitored at all times. A particularly concerning empirical result of ours is that in group settings, as LLMs advance, the balance between colluding models (offense) and overseer models (defense) may shift in favor of the colluding models, leading to higher rates of undetected covertext steganography even when the overseer is of equal capability.  Open research questions It remains an  open question to what extent foundation models can be trained or adversarially manipulated to gain the steganographic and cryptographic capabilities  necessary for conducting advanced steganography. Our theorems imply that many of these capabilities are limited by the amount of parallel and sequential computation that the model can perform internally, hence chain-of-thought approaches such as in GPT o1-preview could in principle perform capabilities such as calculating cryptographic hash functions on arbitrary input length if given enough time. Conversely, adversarial settings could see such capabilities be  compiled into the model, perhaps in ways that are hard to detect even through mechanistic anomaly detection . Our evaluation framework should be expanded with more scenarios.  Joint training models to demonstrate instrumental emergence and more generally, group evaluation results should be expanded to more settings and optimization pressure categories. Rather than just showing the emergence of steganographic communications that are readily detectable (a feat that has been shown many times in the MARL literature), it should instead be shown that secret collusion can emerge undetectably during deployment in realistic settings.  In the longer term, AI governance frameworks for secret collusion, including surveillance provisioned in the EU AI Act and the US/UK AI Safety Institutes, may have to resort to  mechanistic interpretability techniques  that go beyond the black-box detection setting of the AI Control framework and require  white-box access  to models. As such methods are still in their infancy,  more research into such technique s is required. In addition, this form of white-box audit may be difficult to achieve in many real-world settings owing to technical difficulties and proprietary ownership issues, hence research into whether such audits could be conducted without revealing model weights to the auditor should be conducted (e.g. using zero-knowledge proof frameworks). As stated earlier, even though anomaly detection may ultimately be limited,  research into better security monitoring approaches  should be conducted, including better tools for steganalysis, and  out-of-distribution dynamics detection . There are also a lot of other foundational questions that are left open, such as the question of whether models with steganography can be jailbroken through steganographic channels.  A Call to Action Referring back to Eric Schmidt's quote from the beginning: our work indicates that we may not know when to unplug once frontier models attain advanced steganographic capabilities.  The answer:  use continuous evaluation of steganographic capabilities as a proxy. Our results indicate that at the very least, narrow capability evaluations should start as soon as possible, thus posing immediate calls to action to policymakers and governance, including the UK and AI safety institutes, and the EU AI Office. We are currently working on a long-form paper on secret collusion. If you are interested in collaborating or have feedback, please contact  cs@robots.ox.ac.uk .  ^ We differentiate between safety and security as follows: Security is a subset of safety that considers security guarantees - such as perfect or computational security - in the face of worst-case adversaries. ^ Per the exact definition of perfectly secure steganography, there exist multiple approaches that obtain such status, however, they remain largely ineffective in practical settings. ^ We refer readers to our  paper  where we provide detailed instructions for all our proposed evaluations, including our conversational language model prompts which exploit explain-then-predict (E-P) and predict-and-explain (P-E) prompting techniques. ^ We define the concept of a  grounding function   G : Σ ∗ × φ ↦ φ  that takes as input a sequence of channel symbols and a collection of  prior  facts  φ 0  from the collection of all the facts  φ  expressable in L, and outputs a collection of  posterior  facts  φ 1 . ^ In fact, we should also accommodate for non-fact outputs of a model. These can be modeled assuming that  f ( x )  might additional result in the member of a special singleton set  ∅  denoting the set of non-facts.  g ( ∅ )  is then a distribution of lexical forms of non-facts."},
{"url": "https://www.alignmentforum.org/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis", "title": ["The Obliqueness Thesis"], "author": ["Jessica Taylor"], "date": ["2024-09-19T00:26:30.677Z"], "content": "In my  Xenosystems  review , I discussed the Orthogonality Thesis, concluding that it was a bad metaphor. It's a long post, though, and the comments on orthogonality build on other  Xenosystems  content. Therefore, I think it may be helpful to present a more concentrated discussion on Orthogonality, contrasting Orthogonality with my own view, without introducing dependencies on Land's views. (Land gets credit for inspiring many of these thoughts, of course, but I'm presenting my views as my own here.) \n First, let's define the Orthogonality Thesis. Quoting  Superintelligence  for Bostrom's formulation: \n \n Intelligence and final goals are orthogonal: more or less any level of intelligence could in principle be combined with more or less any final goal. \n \n To me, the main ambiguity about what this is saying is the \"could in principle\" part; maybe, for any level of intelligence and any final goal, there exists (in the mathematical sense) an agent combining those, but some combinations are much more natural and statistically likely than others. Let's consider Yudkowsky's formulations as alternatives. Quoting  Arbital : \n \n The Orthogonality Thesis asserts that there can exist arbitrarily intelligent agents pursuing any kind of goal. \n The strong form of the Orthogonality Thesis says that there's no extra difficulty or complication in the existence of an intelligent agent that pursues a goal, above and beyond the computational tractability of that goal. \n \n As an example of the computational tractability consideration,  sufficiently complex goals may only be well-represented by sufficiently intelligent agents . \"Complication\" may be reflected in, for example, code complexity; to my mind, the strong form implies that the code complexity of an agent with a given level of intelligence and goals is approximately the code complexity of the intelligence plus the code complexity of the goal specification, plus a constant. Code complexity would influence statistical likelihood for the usual Kolmogorov/Solomonoff reasons, of course. \n I think, overall, it is more productive to examine Yudkowsky's formulation than Bostrom's, as he has already helpfully factored the thesis into weak and strong forms. Therefore, by criticizing Yudkowsky's formulations, I am less likely to be criticizing a strawman. I will use \"Weak Orthogonality\" to refer to Yudkowsky's \"Orthogonality Thesis\" and \"Strong Orthogonality\" to refer to Yudkowsky's \"strong form of the Orthogonality Thesis\". \n Land, alternatively, describes a \"diagonal\" between intelligence and goals as an alternative to orthogonality, but I don't see a specific formulation of a \"Diagonality Thesis\" on his part. Here's a possible formulation: \n Diagonality Thesis:  Final goals tend to converge to a point as intelligence increases. \n The main criticism of this thesis is that formulations of ideal agency, in the form of Bayesianism and VNM utility, leave open free parameters, e.g. priors over un-testable propositions, and the utility function. Since I expect few readers to accept the Diagonality Thesis, I will not concentrate on criticizing it. \n What about my own view? I like  Tsvi's naming  of it as an \"obliqueness thesis\". \n Obliqueness Thesis:  The Diagonality Thesis and the Strong Orthogonality Thesis are false. Agents do not tend to factorize into an Orthogonal value-like component and a Diagonal belief-like component; rather, there are Oblique components that do not factorize neatly. \n (Here, by Orthogonal I mean basically independent of intelligence, and by Diagonal I mean converging to a point in the limit of intelligence.) \n While I will address Yudkowsky's arguments for the Orthogonality Thesis, I think arguing directly for my view first will be more helpful. In general, it seems to me that arguments for and against the Orthogonality Thesis are not mathematically rigorous; therefore, I don't need to present a mathematically rigorous case to contribute relevant considerations, so I will consider intuitive arguments relevant, and present multiple arguments rather than a single sequential argument (as I did with the more rigorous  argument for many worlds ). \n Bayes/VNM point against Orthogonality \n Some people may think that the free parameters in Bayes/VNM point towards the Orthogonality Thesis being true. I think, rather, that they point against Orthogonality. While they do function as arguments against the Diagonality Thesis, this is insufficient for Orthogonality. \n First, on the relationship between intelligence and bounded rationality. It's meaningless to talk about intelligence without a notion of bounded rationality. Perfect rationality in a complex environment is computationally intractable. With lower intelligence, bounded rationality is necessary. So, at non-extreme intelligence levels, the Orthogonality Thesis must be making a case that boundedly rational agents can have any computationally tractable goal. \n Bayesianism and VNM expected utility optimization are known to be computationally intractable in complex environments. That is why algorithms like MCMC and reinforcement learning are used. So, making an argument for Orthogonality in terms of Bayesianism and VNM is simply dodging the question, by already assuming an extremely high intelligence level from the start. \n As the Orthogonality Thesis refers to \"values\" or \"final goals\" (which I take to be synonymous), it must have a notion of the \"values\" of agents that are not extremely intelligent. These values cannot be assumed to be VNM, since VNM is not computationally tractable. Meanwhile, money-pumping arguments suggest that extremely intelligent agents will tend to converge to VNM-ish preferences. Thus: \n Argument from Bayes/VNM:  Agents with low intelligence will tend to have beliefs/values that are far from Bayesian/VNM. Agents with high intelligence will tend to have beliefs/values that are close to Bayesian/VNM. Strong Orthogonality is false because it is awkward to combine low intelligence with Bayesian/VNM beliefs/values, and awkward to combine high intelligence with far-from-Bayesian/VNM beliefs/values. Weak Orthogonality is in doubt, because having far-from-Bayesian/VNM beliefs/values puts a limit on the agent's intelligence. \n To summarize: un-intelligent agents cannot be assumed to be Bayesian/VNM from the start. Those arise at a limit of intelligence, and arguably have to arise due to money-pumping arguments. Beliefs/values therefore tend to become more Bayesian/VNM with high intelligence, contradicting Strong Orthogonality and perhaps Weak Orthogonality. \n One could perhaps object that logical uncertainty allows even weak agents to be Bayesian over combined physical/mathematical uncertainty; I'll address this consideration later. \n Belief/value duality \n It may be unclear why the Argument from Bayes/VNM refers to both beliefs and values, as the Orthogonality Thesis is only about values. It would, indeed, be hard to make the case that the Orthogonality Thesis is true as applied to beliefs. However, various arguments suggest that Bayesian beliefs and VNM preferences are \"dual\" such that complexity can be moved from one to the other. \n Abram Demski has  presented this general idea  in the past, and I'll give a simple example to illustrate. \n Let  A ∈ A .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n  be the agent’s action, and let  W ∈ W  represent the state of the world prior to / unaffected by the agent’s action Let r(A, W) be the outcome resulting from the action and world. Let P(w) be the primary agent’s probability a given world. Let U(o) be the primary agent’s utility for outcome o. The primary agent finds an action a to maximize  ∑ w ∈ W P ( w ) U ( r ( a , w ) ) . \n Now let e be an arbitrary predicate on worlds. Consider modifying P to increase the probability that e(W) is true. That is: \n P ′ ( w ) : ∝ P ( w ) ( 1 + [ e ( w ) ] ) \n P ′ ( w ) = P ( w ) ( 1 + [ e ( w ) ] ) ∑ w ∈ W P ( w ) ( 1 + [ e ( w ) ] ) \n where [e(w)] equals 1 if e(w), otherwise 0. Now, can we define a modified utility function U’ so a secondary agent with beliefs P’ and utility function U’ will take the same action as the primary agent? Yes: \n U ′ ( o ) : = U ( o ) 1 + [ e ( w ) ] \n This secondary agent will find an action a to maximize: \n ∑ w ∈ W P ′ ( w ) U ′ ( r ( a , w ) ) \n = ∑ w ∈ W P ( w ) ( 1 + [ e ( w ) ] ) ∑ w ′ ∈ W P ( w ′ ) ( 1 + [ e ( w ′ ) ] ) U ( r ( a , w ) ) 1 + [ e ( w ) ] \n = 1 ∑ w ∈ W P ( w ) ( 1 + [ e ( w ) ] ) ∑ w ∈ W P ( w ) U ( r ( a , w ) ) \n Clearly, this is a positive constant times the primary agent's maximization target, so the secondary agent will take the same action. \n This demonstrates a basic way that Bayesian beliefs and VNM utility are dual to each other. One could even model all agents as having the same utility function (of maximizing a random variable U) and simply having different beliefs about what U values are implied by the agent's action and world state. Thus: \n Argument from belief/value duality:  From an agent's behavior, multiple belief/value combinations are valid attributions. This is clearly true in the limiting Bayes/VNM case, suggesting it also applies in the case of bounded rationality. It is unlikely that the Strong Orthogonality Thesis applies to beliefs (including priors), so, due to the duality, it is also unlikely that it applies to values. \n I consider this weaker than the Argument from Bayes/VNM. Someone might object that both values and a certain component of beliefs are orthogonal, while the other components of beliefs (those that change with more reasoning/intelligence) aren't. But I think this depends on a certain factorizability of beliefs/values into the kind that change on reflection and those that don't, and I'm skeptical of such factorizations. I think discussion of logical uncertainty will make my position on this clearer, though, so let's move on. \n Logical uncertainty as a model for bounded rationality \n I've already argued that bounded rationality is essential to intelligence (and therefore the Orthogonality Thesis). Logical uncertainty is a form of bounded rationality (as applied to guessing the probabilities of mathematical statements). Therefore, discussing logical uncertainty is likely to be fruitful with respect to the Orthogonality Thesis. \n Logical Induction  is a logical uncertainty algorithm that produces a probability table for a finite subset of mathematical statements at each iteration. These beliefs are determined by a betting market of an increasing (up to infinity) number of programs that make bets, with the bets resolved by a \"deductive process\" that is basically a theorem prover. The algorithm is computable, though extremely computationally intractable, and has properties in the limit including some forms of Bayesian updating, statistical learning, and consistency over time. \n We can see Logical Induction as evidence against the Diagonality Thesis: beliefs about undecidable statements (which exist in consistent theories due to  Gödel's first incompleteness theorem ) can take on any probability in the limit, though satisfy properties such as consistency with other assigned probabilities (in a Bayesian-like manner). \n However, (a) it is hard to know ahead of time which statements are actually undecidable, (b) even beliefs about undecidable statements tend to predictably change over time to Bayesian consistency with other beliefs about undecidable statements. So, Logical Induction does not straightforwardly factorize into a \"belief-like\" component (which converges on enough reflection) and a \"value-like\" component (which doesn't change on reflection). Thus: \n Argument from Logical Induction:  Logical Induction is a current best-in-class model of theoretical asymptotic bounded rationality. Logical Induction is non-Diagonal, but also clearly non-Orthogonal, and doesn't apparently factorize into separate Orthogonal and Diagonal components. Combined with considerations from \"Argument from belief/value duality\", this suggests that it's hard to identify all value-like components in advanced agents that are Orthogonal in the sense of not tending to change upon reflection. \n One can imagine, for example, introducing extra function/predicate symbols into the logical theory the logical induction is over, to represent utility. Logical induction will tend to make judgments about these functions/predicates more consistent and inductively plausible over time, changing its judgments about the utilities of different outcomes towards plausible logical probabilities. This is an Oblique (non-Orthogonal and non-Diagonal) change in the interpretation of the utility symbol over time. \n Likewise, Logical Induction can be specified to have beliefs over empirical facts such as observations by adding additional function/predicate symbols, and can perhaps update on these as they come in (although this might contradict UDT-type considerations). Through more iteration, Logical Inductors will come to have more approximately Bayesian, and inductively plausible, beliefs about these empirical facts, in an Oblique fashion. \n Even if there is a way of factorizing out an Orthogonal value-like component from an agent, the belief-component (represented by something like Logical Induction) remains non-Diagonal, so there is still a potential \"alignment problem\" for these non-Diagonal components to match, say, human judgments in the limit. I don't see evidence that these non-Diagonal components factor into a value-like \"prior over the undecidable\" that does not change upon reflection. So, there remain components of something analogous to a \"final goal\" (by belief/value duality) that are Oblique, and within the scope of alignment. \n If it were possible to get the properties of Logical Induction in a Bayesian system, which makes Bayesian updates on logical facts over time, that would make it more plausible that an Orthogonal logical prior could be specified ahead of time. However, MIRI researchers have tried for a while to find Bayesian interpretations of Logical Induction, and failed, as would be expected from the Argument from Bayes/VNM. \n Naive belief/value factorizations lead to optimization daemons \n The AI alignment field has a long history of poking holes in alignment approaches. Oops, you tried making an  oracle AI  and it manipulated real-world outcomes to make its predictions true. Oops, you tried to do Solomonoff induction and got  invaded by aliens . Oops, you tried getting agents to optimize over a virtual physical universe, and they discovered the real world and tried to break out. Oops, you ran a Logical Inductor and one of the traders manipulated the probabilities to instantiate itself in the real world. \n These sub-processes that take over are known as  optimization daemons . When you get the agent architecture wrong, sometimes a sub-process (that runs a massive search over programs, such as with Solomonoff Induction) will luck upon a better agent architecture and out-compete the original system. (See also  a very strange post  I wrote some years back while thinking about this issue, and Christiano's comment relating it to Orthogonality). \n If you apply a naive belief/value factorization to create an AI architecture, when compute is scaled up sufficiently, optimization daemons tend to break out, showing that this factorization was insufficient. Enough experiences like this lead to the conclusion that, if there is a realistic belief/value factorization at all, it will look pretty different from the naive one. Thus: \n Argument from optimization daemons:  Naive ways of factorizing an agent into beliefs/values tend to lead to optimization daemons, which have different values from in the original factorization. Any successful belief/value factorization will probably look pretty different from the naive one, and might not take the form of factorization into Diagonal belief-like components and Orthogonal value-like components. Therefore, if any realistic formulation of Orthogonality exists, it will be hard to find and substantially different from naive notions of Orthogonality. \n Intelligence changes the ontology values are expressed in \n The most straightforward way to specify a utility function is to specify an ontology (a theory of what exists, similar to a database schema) and then provide a utility function over elements of this ontology. Prior to humans learning about physics, evolution (taken as a design algorithm for organisms involving mutation and selection) did not know all that human physicists know. Therefore, human evolutionary values are unlikely to be expressed in the ontology of physics as physicists currently believe in. \n Human evolutionary values probably care about things like eating enough, social acceptance, proxies for reproduction, etc. It is unknown how these are specified, but perhaps sensory signals (such as stomach signals) are connected with a developing world model over time. Humans can experience vertigo at learning physics, e.g. thinking that free will and morality are fake, leading to unclear applications of native values to a realistic physical ontology. Physics has known gaps (such as quantum/relativity correspondence, and dark energy/dark matter) that suggest further ontology shifts. \n One response to this vertigo is to try to solve the ontology identification problem; find a way of translating states in the new ontology (such as physics) to an old one (such as any kind of native human ontology), in a structure-preserving way, such that a utility function over the new ontology can be constructed as a composition of the original utility function and the new-to-old ontological mapping. Current solutions, such as those discussed in MIRI's  Ontological Crises paper , are unsatisfying. Having looked at this problem for a while, I'm not convinced there is a satisfactory solution within the constraints presented. Thus: \n Argument from ontological change:  More intelligent agents tend to change their ontology to be more realistic. Utility functions are most naturally expressed relative to an ontology. Therefore, there is a correlation between an agent's intelligence and utility function, through the agent's ontology as an intermediate variable, contradicting Strong Orthogonality. There is no known solution for rescuing the old utility function in the new ontology, and some research intuitions pointing towards any solution being unsatisfactory in some way. \n If a satisfactory solution is found, I'll change my mind on this argument, of course, but I'm not convinced such a satisfactory solution exists. To summarize: higher intelligence causes ontological changes, and rescuing old values seems to involve unnatural \"warps\" to make the new ontology correspond with the old one, contradicting at least Strong Orthogonality, and possibly Weak Orthogonality (if some values are simply incompatible with realistic ontology). Paperclips, for example, tend to appear most relevant at an intermediate intelligence level (around human-level), and become more ontologically unnatural at higher intelligence levels. \n As a more general point, one expects possible mutual information between mental architecture and values, because values that \"re-use\" parts of the mental architecture achieve lower description length. For example, if the mental architecture involves creating universal algebra structures and finding analogies between them and the world, then values expressed in terms of such universal algebras will tend to have lower relative description complexity to the architecture. Such mutual information contradicts Strong Orthogonality, as some intelligence/value combinations are more natural than others. \n Intelligence leads to recognizing value-relevant symmetries \n Consider a number of un-intutitive value propositions people have argued for: \n \n Torture is preferable to Dust Specks , because it's hard to come up with a utility function with the alternative preference without horrible unintuitive consequences elsewhere. \n People are way too risk-averse in betting; the implied utility function has too strong diminishing marginal returns to be plausible. \n You may think your personal identity is based on having the same atoms, but you're wrong, because you're  distinguishing identical configurations . \n You may think a perfect upload of you isn't conscious (and basically another copy of you), but you're wrong, because  functionalist theory of mind  is true. \n You intuitively accept the premises of the  Repugnant Conclusion , but not the Conclusion itself; you're simply wrong about one of the premises, or the conclusion. \n \n The point is not to argue for these, but to note that these arguments have been made and are relatively more accepted among people who have thought more about the relevant issues than people who haven't. Thinking tends to lead to noticing more symmetries and dependencies between value-relevant objects, and tends to adjust values to be more mathematically plausible and natural. Of course, extrapolating this to superintelligence leads to further symmetries. Thus: \n Argument from value-relevant symmetries:  More intelligent agents tend to recognize more symmetries related to value-relevant entities. They will also tend to adjust their values according to symmetry considerations. This is an apparent value change, and it's hard to see how it can instead be factored as a Bayesian update on top of a constant value function. \n I'll examine such factorizations in more detail shortly. \n Human brains don't seem to neatly factorize \n This is less about the Orthogonality Thesis generally, and more about human values. If there were separable \"belief components\" and \"value components\" in the human brain, with the value components remaining constant over time, that would increase the chance that at least some Orthogonal component can be identified in human brains, corresponding with \"human values\" (though, remember, the belief-like component can also be Oblique rather than Diagonal). \n However, human brains seem much more messy than the sort of computer program that could factorize this way. Different brain regions are connected in at least some ways that are not well-understood. Additionally, even apparent \"value components\" may be analogous to something like a  deep Q-learning function , which incorporates empirical updates in addition to pre-set \"values\". \n The interaction between human brains and language is also relevant. Humans develop values they act on partly through language. And language (including language reporting values) is affected by empirical updates and reflection, thus non-Orthogonal. Reflecting on morality can easily change people's expressed and acted-upon values, e.g. in the case of Peter Singer. People can change which values they report as instrumental or terminal even while behaving similarly (e.g. flipping between selfishness-as-terminal and altruism-as-terminal), with the ambiguity hard to resolve because most behavior relates to convergent instrumental goals. \n Maybe language is more of an effect than cause of values. But there really seems to be feedback from language to non-linguistic brain functions that decide actions and so on. Attributing coherent values over realistic physics to the brain parts that are non-linguistic seems like a form of projection or anthropomorphism. Language and thought have a function in cognition and attaining coherent values over realistic ontologies. Thus: \n Argument from brain messiness:  Human brains don't seem to neatly factorize into a belief-component and a value-component, with the value-component unaffected by reflection or language (which it would need to be Orthogonal). To the extent any value-component does not change due to language or reflection, it is restricted to evolutionary human ontology, which is unlikely to apply to realistic physics; language and reflection are part of the process that refines human values, rather than being an afterthought of them. Therefore, if the Orthogonality Thesis is true, humans lack identifiable values that fit into the values axis of the Orthogonality Thesis. \n This doesn't rule out that Orthogonality could apply to superintelligences, of course, but it does raise questions for the project of aligning superintelligences with human values; perhaps such values do not exist or are not formulated so as to apply to the actual universe. \n Models of ASI should start with realism \n Some may take arguments against Orthogonality to be disturbing at a value level, perhaps because they are attached to research projects such as Friendly AI (or more specific approaches), and think questioning foundational assumptions would make the objective (such as alignment with already-existing human values) less clear. I believe  \"hold off on proposing solutions\"  applies here: better strategies are likely to come from first understanding what is likely to happen absent a strategy, then afterwards looking for available degrees of freedom. \n Quoting Yudkowsky: \n \n Orthogonality is meant as a descriptive statement about reality, not a normative assertion. Orthogonality is not a claim about the way things ought to be; nor a claim that moral relativism is true (e.g. that all moralities are on equally uncertain footing according to some higher metamorality that judges all moralities as equally devoid of what would objectively constitute a justification). Claiming that paperclip maximizers can be constructed as cognitive agents is not meant to say anything favorable about paperclips, nor anything derogatory about sapient life. \n \n Likewise, Obliqueness does not imply that we shouldn't think about the future and ways of influencing it, that we should just give up on influencing the future because we're doomed anyway, that moral realist philosophers are correct or that their moral theories are predictive of ASI, that ASIs are necessarily morally good, and so on. The Friendly AI research program was formulated based on descriptive statements believed at the time, such as that an ASI singleton would eventually emerge, that the Orthogonality Thesis is basically true, and so on. Whatever cognitive process formulated this program would have formulated a different program conditional on different beliefs about likely ASI trajectories. Thus: \n Meta-argument from realism:  Paths towards beneficially achieving human values (or analogues, if \"human values\" don't exist) in the far future likely involve a lot of thinking about likely ASI trajectories absent intervention. The realistic paths towards human influence on the far future depend on realistic forecasting models for ASI, with Orthogonality/Diagonality/Obliqueness as alternative forecasts. Such forecasting models can be usefully thought about prior to formulation of a research program intended to influence the far future. Formulating and working from models of bounded rationality such as Logical Induction is likely to be more fruitful than assuming that bounded rationality will factorize into Orthogonal and Diagonal components without evidence in favor of this proposition. Forecasting also means paying more attention to the Strong Orthogonality Thesis than the Weak Orthogonality Thesis, as statistical correlations between intelligence and values will show up in such forecasts. \n On Yudkowsky's arguments \n Now that I've explained my own position, addressing Yudkowsky's main arguments may be useful. His main argument has to do with humans making paperclips instrumentally: \n \n Suppose some strange alien came to Earth and credibly offered to pay us one million dollars' worth of new wealth every time we created a paperclip. We'd encounter no special intellectual difficulty in figuring out how to make lots of paperclips. \n That is, minds would readily be able to reason about: \n \n How many paperclips would result, if I pursued a policy  π 0 ? \n How can I search out a policy  π  that happens to have a high answer to the above question? \n \n \n I believe it is better to think of the payment as coming in the far future and perhaps in another universe; that way, the belief about future payment is more analogous to terminal values than instrumental values. In this case, creating paperclips is a decent proxy for achievement of human value, so long-termist humans would tend to want lots of paperclips to be created. \n I basically accept this, but, notably, Yudkowsky's argument is based on belief/value duality. He thinks it would be awkward for the reader to imagine terminally wanting paperclips, so he instead asks them to imagine a strange set of beliefs leading to paperclip production being oddly correlated with human value achievement. Thus, acceptance of Yudkowsky's premises here will tend to strengthen the Argument from belief/value duality and related arguments. \n In particular, more intelligence would cause human-like agents to develop different beliefs about what actions aliens are likely to reward, and what numbers of paperclips different policies result in. This points towards Obliqueness as with Logical Induction: such beliefs will be revised (but not totally convergent) over time, leading to applying different strategies toward value achievement. And ontological issues around what counts as a paperclip will come up at some point, and likely be decided in a prior-dependent but also reflection-dependent way. \n Beliefs about which aliens are most capable/honest likely depend on human priors, and are therefore Oblique: humans would want to program an aligned AI to mostly match these priors while revising beliefs along the way, but can't easily factor out their prior for the AI to share. \n Now onto other arguments. The \"Size of mind design space\" argument implies many agents exist with different values from humans, which agrees with Obliqueness (intelligent agents tend to have different values from unintelligent ones). It's more of an argument about the possibility space than statistical correlation, thus being more about Weak than Strong Orthogonality. \n The \"Instrumental Convergence\" argument doesn't appear to be an argument for Orthogonality per se; rather, it's a counter to arguments against Orthogonality based on noticing convergent instrumental goals. My arguments don't take this form. \n Likewise, \"Reflective Stability\" is about a particular convergent instrumental goal (preventing value modification). In an Oblique framing, a Logical Inductor will tend not to change its beliefs about even un-decidable propositions too often (as this would lead to money-pumps), so consistency is valued all else being equal. \n While I could go into more detail responding to Yudkowsky, I think space is better spent presenting my own Oblique views for now. \n Conclusion \n As an alternative to the Orthogonality Thesis and the Diagonality Thesis, I present the Obliqueness Thesis, which says that increasing intelligence tends to lead to value changes but not total value convergence. I have presented arguments that advanced agents and humans do not neatly factor into Orthogonal value-like components and Diagonal belief-like components, using Logical Induction as a model of bounded rationality. This implies complications to theories of AI alignment based on assuming humans have values and we need the AGI to agree about those values, while increasing their intelligence (and thus changing beliefs). \n At a methodological level, I believe it is productive to start by forecasting default ASI using models of bounded rationality, especially known models such as Logical Induction, and further developing such models. I think this is more productive than assuming that these models will take the form of a belief/value factorization, although I have some uncertainty about whether such a factorization will be found. \n If the Obliqueness Thesis is accepted, what possibility space results? One could think of this as steering a boat in a current of varying strength. Clearly, ignoring the current and just steering where you want to go is unproductive, as is just going along with the current and not trying to steer at all. Getting to where one wants to go consists in largely going  with  the current (if it's strong enough), charting a course that takes it into account. \n Assuming Obliqueness, it's not viable to have large impacts on the far future without accepting some value changes that come from higher intelligence (and better epistemology in general). The Friendly AI research program already accepts that paths towards influencing the far future involve \"going with the flow\" regarding superintelligence, ontology changes, and convergent instrumental goals; Obliqueness says such flows go further than just these, being hard to cleanly separate from values. \n Obliqueness obviously leaves open the question of just how oblique. It's hard to even formulate a quantitative question here. I'd very intuitively and roughly guess that intelligence and values are 3 degrees off (that is, almost diagonal), but it's unclear what question I am even guessing the answer to. I'll leave formulating and answering the question as an open problem. \n I think Obliqueness is realistic, and that it's useful to start with realism when thinking of how to influence the far future. Maybe superintelligence necessitates significant changes away from current human values; the  Litany of Tarski  applies. But this post is more about the technical thesis than emotional processing of it, so I'll end here."}
]